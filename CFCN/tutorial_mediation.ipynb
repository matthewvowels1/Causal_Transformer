{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ca21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from model import DAGAutoencoder\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import reorder_dag, get_full_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2fd00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'M', 'Y'] 0.3999999999999992\n"
     ]
    }
   ],
   "source": [
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 10000\n",
    "batch_size = 50\n",
    "max_iters =  30000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.1\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "neurons_per_layer = [3,6,3]\n",
    "\n",
    "def generate_data_mediation(N):\n",
    "    DAGnx = nx.DiGraph()\n",
    "    \n",
    "    Ux = np.random.randn(N)\n",
    "    X =  Ux\n",
    "\n",
    "    Um = np.random.randn(N)\n",
    "    M =  0.5 * X + Um\n",
    "\n",
    "    Uy = np.random.randn(N)\n",
    "    Y =  0.8 * M + Uy\n",
    "\n",
    "    M0 = 0.5 * 0 + Um \n",
    "    M1 = 0.5 * 1 + Um\n",
    "\n",
    "    Y0 = 0.8 * M0 + Uy \n",
    "    Y1 = 0.8 * M1 +  Uy \n",
    "\n",
    "    all_data_dict = {'X': X, 'M': M, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'M': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'M'), ('M', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "    ordered_var_types = dict(sorted(var_types.items(), key=lambda item: causal_ordering[item[0]]))\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, ordered_var_types, Y0, Y1\n",
    "\n",
    "_, _, _, _, _, Y0, Y1 = generate_data_mediation(N=1000000)\n",
    "ATE = (Y1 - Y0).mean()  # ATE based off a large sample\n",
    "all_data, DAG, var_names, causal_ordering, var_types, Y0, Y1 = generate_data_mediation(N=sample_size)\n",
    "print(var_names, ATE)\n",
    "\n",
    "input_dim = all_data.shape[1]\n",
    "\n",
    "# prepend the input size to neurons_per_layer if not included in neurons_per_layer\n",
    "# append the intput size to neurons_per_layer (output) if not included in neurons_per_layer\n",
    "neurons_per_layer = [6,12,6]\n",
    "neurons_per_layer.insert(0, input_dim)\n",
    "neurons_per_layer.append(input_dim)\n",
    "\n",
    "utils.assert_neuron_layers(layers=neurons_per_layer, input_size=input_dim)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "\n",
    "initial_adj_matrix = nx.to_numpy_array(DAG)\n",
    "\n",
    "initial_masks = [torch.from_numpy(mask).float().to(torch.float64) for mask in\n",
    "                 utils.expand_adjacency_matrix(neurons_per_layer[1:], initial_adj_matrix)]\n",
    "\n",
    "\n",
    "model = DAGAutoencoder(neurons_per_layer=neurons_per_layer, dag=DAG, causal_ordering=causal_ordering, var_types=var_types, dropout_rate=dropout_rate).to(device)\n",
    "model.initialize_masks(initial_masks)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed0fbc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 30000: train_loss 3.0399, val loss 3.2258\n",
      "step 100 of 30000: train_loss 2.9986, val loss 3.1372\n",
      "step 200 of 30000: train_loss 2.9429, val loss 2.9852\n",
      "step 300 of 30000: train_loss 2.4101, val loss 2.4992\n",
      "step 400 of 30000: train_loss 2.2368, val loss 2.2721\n",
      "step 500 of 30000: train_loss 2.1131, val loss 2.0585\n",
      "step 600 of 30000: train_loss 2.0324, val loss 2.0194\n",
      "step 700 of 30000: train_loss 2.0381, val loss 2.1007\n",
      "step 800 of 30000: train_loss 2.1115, val loss 2.0330\n",
      "step 900 of 30000: train_loss 2.0591, val loss 2.0903\n",
      "step 1000 of 30000: train_loss 2.0327, val loss 2.0160\n",
      "step 1100 of 30000: train_loss 2.0122, val loss 2.0948\n",
      "step 1200 of 30000: train_loss 2.0431, val loss 2.0102\n",
      "step 1300 of 30000: train_loss 2.0317, val loss 2.0281\n",
      "step 1400 of 30000: train_loss 2.0018, val loss 2.0589\n",
      "step 1500 of 30000: train_loss 2.0215, val loss 1.9978\n",
      "step 1600 of 30000: train_loss 2.0242, val loss 2.0173\n",
      "step 1700 of 30000: train_loss 2.0465, val loss 1.9945\n",
      "step 1800 of 30000: train_loss 2.0525, val loss 2.0536\n",
      "step 1900 of 30000: train_loss 1.9735, val loss 2.0373\n",
      "step 2000 of 30000: train_loss 2.0632, val loss 2.0572\n",
      "step 2100 of 30000: train_loss 2.0504, val loss 2.0347\n",
      "step 2200 of 30000: train_loss 2.0239, val loss 2.0344\n",
      "step 2300 of 30000: train_loss 2.0930, val loss 2.0146\n",
      "step 2400 of 30000: train_loss 2.0334, val loss 2.0196\n",
      "step 2500 of 30000: train_loss 2.0737, val loss 2.0356\n",
      "step 2600 of 30000: train_loss 2.0406, val loss 2.0026\n",
      "step 2700 of 30000: train_loss 2.0363, val loss 2.0482\n",
      "step 2800 of 30000: train_loss 2.0176, val loss 2.0629\n",
      "step 2900 of 30000: train_loss 2.1009, val loss 2.0204\n",
      "step 3000 of 30000: train_loss 2.0713, val loss 1.9980\n",
      "step 3100 of 30000: train_loss 2.0349, val loss 1.9969\n",
      "step 3200 of 30000: train_loss 2.0079, val loss 2.0229\n",
      "step 3300 of 30000: train_loss 2.0332, val loss 2.0072\n",
      "step 3400 of 30000: train_loss 2.0489, val loss 1.9974\n",
      "step 3500 of 30000: train_loss 2.0435, val loss 2.0340\n",
      "step 3600 of 30000: train_loss 2.0249, val loss 2.0537\n",
      "step 3700 of 30000: train_loss 2.0399, val loss 2.0114\n",
      "step 3800 of 30000: train_loss 1.9904, val loss 2.0122\n",
      "step 3900 of 30000: train_loss 2.0176, val loss 1.9978\n",
      "step 4000 of 30000: train_loss 2.0212, val loss 2.0535\n",
      "step 4100 of 30000: train_loss 2.0291, val loss 2.0618\n",
      "step 4200 of 30000: train_loss 2.0626, val loss 2.0394\n",
      "step 4300 of 30000: train_loss 2.0210, val loss 1.9891\n",
      "step 4400 of 30000: train_loss 2.0580, val loss 2.0445\n",
      "step 4500 of 30000: train_loss 2.0111, val loss 2.0143\n",
      "step 4600 of 30000: train_loss 2.0824, val loss 2.0656\n",
      "step 4700 of 30000: train_loss 2.0632, val loss 2.0370\n",
      "step 4800 of 30000: train_loss 2.0241, val loss 2.0203\n",
      "step 4900 of 30000: train_loss 2.1002, val loss 1.9620\n",
      "step 5000 of 30000: train_loss 2.0500, val loss 2.0650\n",
      "step 5100 of 30000: train_loss 2.0190, val loss 2.0309\n",
      "step 5200 of 30000: train_loss 2.0589, val loss 2.0121\n",
      "step 5300 of 30000: train_loss 2.0026, val loss 2.0244\n",
      "step 5400 of 30000: train_loss 1.9592, val loss 2.0464\n",
      "step 5500 of 30000: train_loss 2.0093, val loss 2.0220\n",
      "step 5600 of 30000: train_loss 1.9989, val loss 2.0202\n",
      "step 5700 of 30000: train_loss 2.0401, val loss 2.0460\n",
      "step 5800 of 30000: train_loss 1.9920, val loss 2.0255\n",
      "step 5900 of 30000: train_loss 2.0301, val loss 1.9953\n",
      "step 6000 of 30000: train_loss 2.0543, val loss 2.0467\n",
      "step 6100 of 30000: train_loss 2.0222, val loss 2.0725\n",
      "step 6200 of 30000: train_loss 1.9683, val loss 2.0503\n",
      "step 6300 of 30000: train_loss 2.0106, val loss 2.0319\n",
      "step 6400 of 30000: train_loss 1.9998, val loss 1.9982\n",
      "step 6500 of 30000: train_loss 2.0324, val loss 2.0242\n",
      "step 6600 of 30000: train_loss 2.0277, val loss 2.0493\n",
      "step 6700 of 30000: train_loss 2.0445, val loss 1.9999\n",
      "step 6800 of 30000: train_loss 2.0132, val loss 2.0446\n",
      "step 6900 of 30000: train_loss 2.0260, val loss 2.0311\n",
      "step 7000 of 30000: train_loss 2.0347, val loss 2.0285\n",
      "step 7100 of 30000: train_loss 2.0367, val loss 2.0139\n",
      "step 7200 of 30000: train_loss 1.9957, val loss 1.9875\n",
      "step 7300 of 30000: train_loss 2.0279, val loss 2.0197\n",
      "step 7400 of 30000: train_loss 2.0043, val loss 2.0052\n",
      "step 7500 of 30000: train_loss 2.0311, val loss 1.9861\n",
      "step 7600 of 30000: train_loss 1.9896, val loss 2.0161\n",
      "step 7700 of 30000: train_loss 2.0527, val loss 2.0212\n",
      "step 7800 of 30000: train_loss 2.0326, val loss 2.0136\n",
      "step 7900 of 30000: train_loss 2.0465, val loss 1.9551\n",
      "step 8000 of 30000: train_loss 2.0439, val loss 2.0659\n",
      "step 8100 of 30000: train_loss 2.0716, val loss 2.0610\n",
      "step 8200 of 30000: train_loss 1.9684, val loss 2.0319\n",
      "step 8300 of 30000: train_loss 2.0115, val loss 2.0576\n",
      "step 8400 of 30000: train_loss 2.0268, val loss 2.0733\n",
      "step 8500 of 30000: train_loss 2.0460, val loss 1.9929\n",
      "step 8600 of 30000: train_loss 2.0029, val loss 2.0055\n",
      "step 8700 of 30000: train_loss 1.9947, val loss 2.0662\n",
      "step 8800 of 30000: train_loss 2.0739, val loss 2.0206\n",
      "step 8900 of 30000: train_loss 2.0012, val loss 1.9808\n",
      "step 9000 of 30000: train_loss 2.0443, val loss 2.0147\n",
      "step 9100 of 30000: train_loss 2.0452, val loss 2.0082\n",
      "step 9200 of 30000: train_loss 2.0439, val loss 2.0284\n",
      "step 9300 of 30000: train_loss 1.9915, val loss 1.9891\n",
      "step 9400 of 30000: train_loss 1.9957, val loss 2.0083\n",
      "step 9500 of 30000: train_loss 2.0147, val loss 2.0147\n",
      "step 9600 of 30000: train_loss 2.0405, val loss 2.0334\n",
      "step 9700 of 30000: train_loss 2.0289, val loss 2.0238\n",
      "step 9800 of 30000: train_loss 2.0145, val loss 1.9858\n",
      "step 9900 of 30000: train_loss 2.0057, val loss 2.0131\n",
      "step 10000 of 30000: train_loss 2.0696, val loss 2.0364\n",
      "step 10100 of 30000: train_loss 2.0404, val loss 2.0293\n",
      "step 10200 of 30000: train_loss 1.9962, val loss 2.0482\n",
      "step 10300 of 30000: train_loss 2.0193, val loss 2.0660\n",
      "step 10400 of 30000: train_loss 1.9948, val loss 2.0492\n",
      "step 10500 of 30000: train_loss 1.9894, val loss 1.9725\n",
      "step 10600 of 30000: train_loss 2.0301, val loss 1.9964\n",
      "step 10700 of 30000: train_loss 2.1036, val loss 2.0268\n",
      "step 10800 of 30000: train_loss 2.0180, val loss 2.0062\n",
      "step 10900 of 30000: train_loss 2.0349, val loss 2.0278\n",
      "step 11000 of 30000: train_loss 2.0243, val loss 2.0265\n",
      "step 11100 of 30000: train_loss 2.0105, val loss 2.0643\n",
      "step 11200 of 30000: train_loss 2.0261, val loss 2.0523\n",
      "step 11300 of 30000: train_loss 1.9570, val loss 1.9975\n",
      "step 11400 of 30000: train_loss 2.0341, val loss 1.9856\n",
      "step 11500 of 30000: train_loss 1.9814, val loss 2.0778\n",
      "step 11600 of 30000: train_loss 2.0206, val loss 2.0303\n",
      "step 11700 of 30000: train_loss 2.0082, val loss 2.0320\n",
      "step 11800 of 30000: train_loss 2.0070, val loss 2.0072\n",
      "step 11900 of 30000: train_loss 2.0674, val loss 2.0445\n",
      "step 12000 of 30000: train_loss 2.0383, val loss 2.0197\n",
      "step 12100 of 30000: train_loss 2.0053, val loss 2.0223\n",
      "step 12200 of 30000: train_loss 2.0417, val loss 2.0048\n",
      "step 12300 of 30000: train_loss 2.0712, val loss 2.0449\n",
      "step 12400 of 30000: train_loss 2.0494, val loss 2.0647\n",
      "step 12500 of 30000: train_loss 1.9793, val loss 2.0960\n",
      "step 12600 of 30000: train_loss 1.9868, val loss 2.0143\n",
      "step 12700 of 30000: train_loss 1.9932, val loss 2.0087\n",
      "step 12800 of 30000: train_loss 2.0434, val loss 2.0391\n",
      "step 12900 of 30000: train_loss 2.0595, val loss 2.0192\n",
      "step 13000 of 30000: train_loss 1.9765, val loss 2.0273\n",
      "step 13100 of 30000: train_loss 2.0437, val loss 2.0516\n",
      "step 13200 of 30000: train_loss 2.0639, val loss 2.0375\n",
      "step 13300 of 30000: train_loss 2.0336, val loss 2.0390\n",
      "step 13400 of 30000: train_loss 2.0391, val loss 2.0123\n",
      "step 13500 of 30000: train_loss 2.0032, val loss 1.9920\n",
      "step 13600 of 30000: train_loss 1.9488, val loss 1.9835\n",
      "step 13700 of 30000: train_loss 1.9707, val loss 2.0246\n",
      "step 13800 of 30000: train_loss 2.0277, val loss 1.9749\n",
      "step 13900 of 30000: train_loss 2.0540, val loss 1.9979\n",
      "step 14000 of 30000: train_loss 2.0488, val loss 2.0709\n",
      "step 14100 of 30000: train_loss 2.0156, val loss 2.0108\n",
      "step 14200 of 30000: train_loss 2.0550, val loss 2.0394\n",
      "step 14300 of 30000: train_loss 1.9734, val loss 2.0790\n",
      "step 14400 of 30000: train_loss 1.9757, val loss 2.0432\n",
      "step 14500 of 30000: train_loss 2.0279, val loss 2.0224\n",
      "step 14600 of 30000: train_loss 1.9687, val loss 1.9991\n",
      "step 14700 of 30000: train_loss 2.0024, val loss 1.9709\n",
      "step 14800 of 30000: train_loss 2.0663, val loss 2.0391\n",
      "step 14900 of 30000: train_loss 2.0010, val loss 2.0113\n",
      "step 15000 of 30000: train_loss 2.0217, val loss 1.9817\n",
      "step 15100 of 30000: train_loss 2.0412, val loss 2.0442\n",
      "step 15200 of 30000: train_loss 2.0616, val loss 1.9998\n",
      "step 15300 of 30000: train_loss 1.9861, val loss 2.0322\n",
      "step 15400 of 30000: train_loss 2.0240, val loss 2.0107\n",
      "step 15500 of 30000: train_loss 2.0274, val loss 2.0205\n",
      "step 15600 of 30000: train_loss 1.9603, val loss 2.0502\n",
      "step 15700 of 30000: train_loss 2.0621, val loss 2.0559\n",
      "step 15800 of 30000: train_loss 2.0263, val loss 2.0348\n",
      "step 15900 of 30000: train_loss 2.0259, val loss 2.0304\n",
      "step 16000 of 30000: train_loss 1.9987, val loss 1.9949\n",
      "step 16100 of 30000: train_loss 2.0340, val loss 2.0141\n",
      "step 16200 of 30000: train_loss 2.0098, val loss 2.0740\n",
      "step 16300 of 30000: train_loss 1.9981, val loss 1.9935\n",
      "step 16400 of 30000: train_loss 2.0843, val loss 2.0356\n",
      "step 16500 of 30000: train_loss 2.0045, val loss 2.0183\n",
      "step 16600 of 30000: train_loss 2.0009, val loss 2.0072\n",
      "step 16700 of 30000: train_loss 2.0715, val loss 2.0158\n",
      "step 16800 of 30000: train_loss 2.0059, val loss 2.0811\n",
      "step 16900 of 30000: train_loss 2.0194, val loss 2.0095\n",
      "step 17000 of 30000: train_loss 1.9835, val loss 2.0065\n",
      "step 17100 of 30000: train_loss 2.0213, val loss 2.0481\n",
      "step 17200 of 30000: train_loss 1.9823, val loss 1.9489\n",
      "step 17300 of 30000: train_loss 1.9886, val loss 2.0364\n",
      "step 17400 of 30000: train_loss 2.0264, val loss 2.0397\n",
      "step 17500 of 30000: train_loss 2.0317, val loss 2.0310\n",
      "step 17600 of 30000: train_loss 2.0168, val loss 2.0207\n",
      "step 17700 of 30000: train_loss 2.0194, val loss 2.0721\n",
      "step 17800 of 30000: train_loss 2.0194, val loss 2.0843\n",
      "step 17900 of 30000: train_loss 2.0103, val loss 2.0301\n",
      "step 18000 of 30000: train_loss 2.0011, val loss 2.0200\n",
      "step 18100 of 30000: train_loss 1.9835, val loss 2.0217\n",
      "step 18200 of 30000: train_loss 2.0066, val loss 2.0573\n",
      "step 18300 of 30000: train_loss 2.0420, val loss 2.0307\n",
      "step 18400 of 30000: train_loss 1.9731, val loss 2.0063\n",
      "step 18500 of 30000: train_loss 2.0424, val loss 1.9642\n",
      "step 18600 of 30000: train_loss 2.0028, val loss 2.0844\n",
      "step 18700 of 30000: train_loss 2.0116, val loss 2.0321\n",
      "step 18800 of 30000: train_loss 2.0653, val loss 2.0331\n",
      "step 18900 of 30000: train_loss 1.9772, val loss 2.0269\n",
      "step 19000 of 30000: train_loss 2.0677, val loss 2.0045\n",
      "step 19100 of 30000: train_loss 1.9955, val loss 2.0008\n",
      "step 19200 of 30000: train_loss 2.0022, val loss 2.0157\n",
      "step 19300 of 30000: train_loss 2.0568, val loss 2.0402\n",
      "step 19400 of 30000: train_loss 2.0020, val loss 1.9892\n",
      "step 19500 of 30000: train_loss 2.0461, val loss 2.0193\n",
      "step 19600 of 30000: train_loss 2.0204, val loss 2.0494\n",
      "step 19700 of 30000: train_loss 2.0540, val loss 2.0067\n",
      "step 19800 of 30000: train_loss 2.0507, val loss 1.9887\n",
      "step 19900 of 30000: train_loss 2.0207, val loss 1.9891\n",
      "step 20000 of 30000: train_loss 2.0615, val loss 2.0332\n",
      "step 20100 of 30000: train_loss 2.0141, val loss 2.0502\n",
      "step 20200 of 30000: train_loss 2.0458, val loss 1.9984\n",
      "step 20300 of 30000: train_loss 2.0151, val loss 2.0489\n",
      "step 20400 of 30000: train_loss 1.9963, val loss 2.0261\n",
      "step 20500 of 30000: train_loss 2.0378, val loss 2.0414\n",
      "step 20600 of 30000: train_loss 2.0500, val loss 2.0529\n",
      "step 20700 of 30000: train_loss 2.0310, val loss 1.9762\n",
      "step 20800 of 30000: train_loss 2.0023, val loss 1.9896\n",
      "step 20900 of 30000: train_loss 2.0275, val loss 2.0494\n",
      "step 21000 of 30000: train_loss 2.0751, val loss 2.0341\n",
      "step 21100 of 30000: train_loss 2.0382, val loss 2.0324\n",
      "step 21200 of 30000: train_loss 2.0326, val loss 2.0813\n",
      "step 21300 of 30000: train_loss 2.0724, val loss 2.0220\n",
      "step 21400 of 30000: train_loss 2.0367, val loss 2.0277\n",
      "step 21500 of 30000: train_loss 2.0241, val loss 2.0535\n",
      "step 21600 of 30000: train_loss 2.0261, val loss 2.0136\n",
      "step 21700 of 30000: train_loss 2.0667, val loss 2.0522\n",
      "step 21800 of 30000: train_loss 1.9824, val loss 2.0092\n",
      "step 21900 of 30000: train_loss 2.0228, val loss 2.0548\n",
      "step 22000 of 30000: train_loss 2.0447, val loss 1.9908\n",
      "step 22100 of 30000: train_loss 1.9988, val loss 1.9771\n",
      "step 22200 of 30000: train_loss 2.0532, val loss 2.0465\n",
      "step 22300 of 30000: train_loss 2.0053, val loss 1.9795\n",
      "step 22400 of 30000: train_loss 2.0467, val loss 1.9966\n",
      "step 22500 of 30000: train_loss 2.0794, val loss 2.0344\n",
      "step 22600 of 30000: train_loss 2.0559, val loss 2.0432\n",
      "step 22700 of 30000: train_loss 2.0356, val loss 2.0339\n",
      "step 22800 of 30000: train_loss 2.0040, val loss 2.0244\n",
      "step 22900 of 30000: train_loss 2.0868, val loss 2.0484\n",
      "step 23000 of 30000: train_loss 2.0149, val loss 2.0108\n",
      "step 23100 of 30000: train_loss 1.9754, val loss 2.0311\n",
      "step 23200 of 30000: train_loss 2.0215, val loss 1.9999\n",
      "step 23300 of 30000: train_loss 2.0540, val loss 2.0407\n",
      "step 23400 of 30000: train_loss 2.0284, val loss 2.0584\n",
      "step 23500 of 30000: train_loss 2.0151, val loss 2.0365\n",
      "step 23600 of 30000: train_loss 2.0068, val loss 2.0484\n",
      "step 23700 of 30000: train_loss 1.9836, val loss 2.0680\n",
      "step 23800 of 30000: train_loss 2.0232, val loss 1.9943\n",
      "step 23900 of 30000: train_loss 1.9635, val loss 2.0427\n",
      "step 24000 of 30000: train_loss 2.0020, val loss 2.0280\n",
      "step 24100 of 30000: train_loss 2.0495, val loss 2.0317\n",
      "step 24200 of 30000: train_loss 2.0404, val loss 2.0295\n",
      "step 24300 of 30000: train_loss 2.0443, val loss 2.0133\n",
      "step 24400 of 30000: train_loss 1.9580, val loss 2.0315\n",
      "step 24500 of 30000: train_loss 1.9953, val loss 2.0227\n",
      "step 24600 of 30000: train_loss 2.0246, val loss 2.0731\n",
      "step 24700 of 30000: train_loss 2.0129, val loss 1.9562\n",
      "step 24800 of 30000: train_loss 1.9934, val loss 2.0091\n",
      "step 24900 of 30000: train_loss 1.9751, val loss 2.0648\n",
      "step 25000 of 30000: train_loss 2.0098, val loss 2.0636\n",
      "step 25100 of 30000: train_loss 1.9812, val loss 2.0188\n",
      "step 25200 of 30000: train_loss 2.0277, val loss 2.0139\n",
      "step 25300 of 30000: train_loss 2.0393, val loss 2.0498\n",
      "step 25400 of 30000: train_loss 2.0525, val loss 2.0543\n",
      "step 25500 of 30000: train_loss 2.0391, val loss 2.0244\n",
      "step 25600 of 30000: train_loss 2.0482, val loss 2.0539\n",
      "step 25700 of 30000: train_loss 2.0315, val loss 2.0066\n",
      "step 25800 of 30000: train_loss 1.9977, val loss 2.0194\n",
      "step 25900 of 30000: train_loss 2.0173, val loss 2.0624\n",
      "step 26000 of 30000: train_loss 2.0120, val loss 2.0065\n",
      "step 26100 of 30000: train_loss 2.0269, val loss 2.0271\n",
      "step 26200 of 30000: train_loss 2.0252, val loss 1.9970\n",
      "step 26300 of 30000: train_loss 1.9974, val loss 2.0475\n",
      "step 26400 of 30000: train_loss 2.0304, val loss 1.9674\n",
      "step 26500 of 30000: train_loss 2.0062, val loss 2.0068\n",
      "step 26600 of 30000: train_loss 2.0341, val loss 1.9957\n",
      "step 26700 of 30000: train_loss 2.0318, val loss 2.0438\n",
      "step 26800 of 30000: train_loss 2.0378, val loss 1.9500\n",
      "step 26900 of 30000: train_loss 1.9880, val loss 2.0327\n",
      "step 27000 of 30000: train_loss 2.0575, val loss 1.9851\n",
      "step 27100 of 30000: train_loss 1.9921, val loss 2.0700\n",
      "step 27200 of 30000: train_loss 2.0708, val loss 2.0016\n",
      "step 27300 of 30000: train_loss 2.0558, val loss 2.0664\n",
      "step 27400 of 30000: train_loss 2.0264, val loss 1.9582\n",
      "step 27500 of 30000: train_loss 2.0169, val loss 2.0505\n",
      "step 27600 of 30000: train_loss 2.0142, val loss 2.0321\n",
      "step 27700 of 30000: train_loss 2.0081, val loss 2.0283\n",
      "step 27800 of 30000: train_loss 2.0654, val loss 2.0266\n",
      "step 27900 of 30000: train_loss 2.0215, val loss 2.0715\n",
      "step 28000 of 30000: train_loss 2.0213, val loss 2.0003\n",
      "step 28100 of 30000: train_loss 2.0097, val loss 2.0126\n",
      "step 28200 of 30000: train_loss 2.0592, val loss 1.9909\n",
      "step 28300 of 30000: train_loss 1.9984, val loss 2.0478\n",
      "step 28400 of 30000: train_loss 2.0103, val loss 2.0282\n",
      "step 28500 of 30000: train_loss 1.9855, val loss 2.0107\n",
      "step 28600 of 30000: train_loss 1.9960, val loss 2.0424\n",
      "step 28700 of 30000: train_loss 2.0300, val loss 2.0111\n",
      "step 28800 of 30000: train_loss 2.0092, val loss 1.9937\n",
      "step 28900 of 30000: train_loss 2.0328, val loss 2.0249\n",
      "step 29000 of 30000: train_loss 2.0080, val loss 2.0101\n",
      "step 29100 of 30000: train_loss 1.9954, val loss 2.0389\n",
      "step 29200 of 30000: train_loss 2.0597, val loss 2.0355\n",
      "step 29300 of 30000: train_loss 2.0164, val loss 2.0317\n",
      "step 29400 of 30000: train_loss 2.0432, val loss 2.0128\n",
      "step 29500 of 30000: train_loss 1.9754, val loss 2.0013\n",
      "step 29600 of 30000: train_loss 1.9848, val loss 2.0311\n",
      "step 29700 of 30000: train_loss 2.0179, val loss 2.0729\n",
      "step 29800 of 30000: train_loss 2.0188, val loss 2.0640\n",
      "step 29900 of 30000: train_loss 1.9885, val loss 1.9939\n"
     ]
    }
   ],
   "source": [
    " def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539b528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE: 0.5 Estimated: 0.47507479041814804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(all_data, columns=var_names)\n",
    "data_dict = df.to_dict(orient='list')\n",
    "cause_var = 'X'\n",
    "effect_var = 'M'\n",
    "effect_index = utils.find_element_in_list(var_names, target_string=effect_var)\n",
    "ci = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "model.eval()\n",
    "intervention_nodes_vals_0 = {'X': 0}\n",
    "intervention_nodes_vals_1 = {'X': 1}\n",
    "D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "\n",
    "\n",
    "\n",
    "est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean()\n",
    "print('True ATE X->M:', 0.5, 'Estimated:', est_ATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f62f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37255560606718063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(all_data, columns=var_names)\n",
    "data_dict = df.to_dict(orient='list')\n",
    "cause_var = 'X'\n",
    "effect_var = 'Y'\n",
    "effect_index = utils.find_element_in_list(var_names, target_string=effect_var)\n",
    "ci = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "model.eval()\n",
    "intervention_nodes_vals_0 = {'X': 0}\n",
    "intervention_nodes_vals_1 = {'X': 1}\n",
    "D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "\n",
    "\n",
    "\n",
    "est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean()\n",
    "print('True ATE X->M:', 0.5, 'Estimated:', est_ATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd2ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c38049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2bdd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f01c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c76e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bb87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b60922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c235f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c0bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4329d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f771797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64dc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d92cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
