{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbb2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from model import DAGAutoencoder\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import reorder_dag, get_full_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e03b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 100000\n",
    "batch_size = 50\n",
    "max_iters =  30000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.1\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "neurons_per_layer = [4,8,4]\n",
    "\n",
    "\n",
    "def generate_data(N):\n",
    "    Ux = np.random.randn(N)\n",
    "    X = Ux\n",
    "    Ub = 0.01 * np.random.randn(N)\n",
    "    B = Ub\n",
    "    Uc = 0.01 * np.random.randn(N)\n",
    "    C = Uc\n",
    "    Uy = 0.01 * np.random.randn(N)\n",
    "    Y = 0.5 * X + 0.5 * C + 0.4* B + Uy\n",
    "\n",
    "    Y0 = 0.5 * C + 0.4* B + Uy\n",
    "    Y1 = 0.5 + 0.5 * C + 0.4* B + Uy\n",
    "\n",
    "    all_data_dict = {'X': X, 'B': B, 'C': C, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'B': 'cont', 'C': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx = nx.DiGraph()\n",
    "    DAGnx.add_edges_from([('X', 'Y'), ('B', 'Y'), ('C', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91c75d",
   "metadata": {},
   "source": [
    "## Exogenous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, Y0, Y1 = generate_data(N=1000000)\n",
    "ATE = (Y1 - Y0).mean()  # ATE based off a large sample\n",
    "all_data, DAG, var_names, causal_ordering, var_types_sorted, Y0, Y1 = generate_data(N=sample_size)\n",
    "print(var_names, ATE)\n",
    "\n",
    "input_dim = all_data.shape[1]\n",
    "\n",
    "# prepend the input size to neurons_per_layer if not included in neurons_per_layer\n",
    "# append the intput size to neurons_per_layer (output) if not included in neurons_per_layer\n",
    "neurons_per_layer.insert(0, input_dim)\n",
    "neurons_per_layer.append(input_dim)\n",
    "utils.assert_neuron_layers(layers=neurons_per_layer, input_size=input_dim)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_adj_matrix = nx.to_numpy_array(DAG)\n",
    "\n",
    "initial_masks = [torch.from_numpy(mask).float().to(torch.float64) for mask in\n",
    "                 utils.expand_adjacency_matrix(neurons_per_layer[1:], initial_adj_matrix)]\n",
    "\n",
    "\n",
    "model = DAGAutoencoder(neurons_per_layer=neurons_per_layer, dag=DAG, causal_ordering=causal_ordering, var_types_sorted=var_types_sorted, dropout_rate=dropout_rate).to(device)\n",
    "model.initialize_masks(initial_masks)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01274ff",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4eb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(all_data, columns=var_names)\n",
    "data_dict = df.to_dict(orient='list')\n",
    "cause_var = 'X'\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "ci = inference.CausalInference(model=model, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "intervention_nodes_vals_0 = {'X': 0}\n",
    "intervention_nodes_vals_1 = {'X': 1}\n",
    "D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "\n",
    "effect_var = 'Y'\n",
    "effect_index = utils.find_element_in_list(var_names, target_string=effect_var)\n",
    "\n",
    "est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean()\n",
    "print(ATE, est_ATE)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f57a7d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T15:16:38.484643Z",
     "start_time": "2024-08-21T15:16:38.482916Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdfe83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db46bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831a432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf6501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b61591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fc009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d27ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
