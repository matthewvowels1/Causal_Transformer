{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6ea373bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, Dict, List, Tuple, Union\n",
    "# 1. introduce dag into attention -> include transpose to help with routing\n",
    "# 2. introduce diagonal after first layer\n",
    "# 3. handle skip connection masking issue\n",
    "        \n",
    "# note that the network has MHA with blocks in parallel, but this is also done sequentially, combining\n",
    "# both network width and network depth.\n",
    "\n",
    "# for the causal transformer, we have to be careful that we include a 'diagonal' pass-thru after the first layer\n",
    "# otherwise, and e.g. in a three variable chain A->B->C, the dependency structure will prevent B from being predicted\n",
    "# from A <after the first layer>, because B is caused by A, not by itself. So the diagonal of ones should be\n",
    "# introduced after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "914cb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datasets.py\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def inv_sigm(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "\n",
    "def reorder_dag(dag):\n",
    "    '''Takes a networkx digraph object and returns a topologically sorted graph.'''\n",
    "\n",
    "    assert nx.is_directed_acyclic_graph(dag), 'Graph needs to be acyclic.'\n",
    "\n",
    "    old_ordering = list(dag.nodes())  # get old ordering of nodes\n",
    "    adj_mat = nx.to_numpy_array(dag)  # get adjacency matrix of old graph\n",
    "\n",
    "    index_old = {v: i for i, v in enumerate(old_ordering)}\n",
    "    topological_ordering = list(nx.topological_sort(dag))  # get ideal topological ordering of nodes\n",
    "\n",
    "    permutation_vector = [index_old[v] for v in topological_ordering]  # get required permutation of old ordering\n",
    "\n",
    "    reordered_adj = adj_mat[np.ix_(permutation_vector, permutation_vector)]  # reorder old adj. mat\n",
    "\n",
    "    dag = nx.from_numpy_array(reordered_adj, create_using=nx.DiGraph)  # overwrite original dag\n",
    "\n",
    "    mapping = dict(zip(dag, topological_ordering))  # assign node names\n",
    "    dag = nx.relabel_nodes(dag, mapping)\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "def get_full_ordering(DAG):\n",
    "    ''' Note that the input DAG MUST be topologically sorted <before> using this function'''\n",
    "    ordering_info = {}\n",
    "    current_level = 0\n",
    "    var_names = list(DAG.nodes)\n",
    "\n",
    "    for i, var_name in enumerate(var_names):\n",
    "\n",
    "        if i == 0:  # if first in list\n",
    "            ordering_info[var_name] = 0\n",
    "\n",
    "        else:\n",
    "            # check if any parents\n",
    "            parent_list = list(DAG.predecessors(var_name))\n",
    "\n",
    "            # if no parents ()\n",
    "            if len(parent_list) == 0:\n",
    "                ordering_info[var_name] = current_level\n",
    "\n",
    "            elif len(parent_list) >= 1:  # if some parents, find most downstream parent and add 1 to ordering\n",
    "                for parent_var in parent_list:\n",
    "                    parent_var_order = ordering_info[parent_var]\n",
    "                    ordering_info[var_name] = parent_var_order + 1\n",
    "\n",
    "    return ordering_info\n",
    "\n",
    "\n",
    "def generate_data(N, seed, dataset, standardize=1):\n",
    "    '''\n",
    "    :param N: required sample size\n",
    "    :param seed: random seed\n",
    "    :param dataset: which dataset to use (only 'general' currently implemented)\n",
    "    :return: data (DxN), a DAG (networkX), a list of variable names, Y0 and Y1 (vectors of counterfactual outcomes), a list of variable types\n",
    "\n",
    "    Note that the data, the DAG, and the variable names are topologically sorted.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed=seed)\n",
    "    DAGnx = nx.DiGraph()\n",
    "    if dataset == 'general':\n",
    "        # confounders\n",
    "        z1 = np.random.binomial(1, 0.5, (N, 1))\n",
    "        z2 = np.random.binomial(1, 0.65, (N, 1))\n",
    "        z3 = np.round(np.random.uniform(0, 4, (N, 1)), 0)\n",
    "        z4 = np.round(np.random.uniform(0, 5, (N, 1)), 0)\n",
    "        uz5 = np.random.randn(N, 1)\n",
    "        z5 = 0.2 * z1 + uz5\n",
    "\n",
    "        # risk vars:\n",
    "        r1 = np.random.randn(N, 1)\n",
    "        r2 = np.random.randn(N, 1)\n",
    "\n",
    "        # instrumental vars:\n",
    "        i1 = np.random.randn(N, 1)\n",
    "        i2 = np.random.randn(N, 1)\n",
    "\n",
    "        # treatment:\n",
    "        ux = np.random.randn(N, 1)\n",
    "        xp = sigm(-5 + 0.05 * z2 + 0.25 * z3 + 0.6 * z4 + 0.4 * z2 * z4 + 0.15 * z5 + 0.1 * i1 + 0.15 * i2 + 0.1 * ux)\n",
    "        X = np.random.binomial(1, xp, (N, 1))\n",
    "\n",
    "        # mediator:\n",
    "        Um = np.random.randn(N, 1)\n",
    "        m1 = 0.8 + 0.15 * Um\n",
    "        m0 = 0.15 * Um\n",
    "\n",
    "\n",
    "        if standardize:\n",
    "            z1 = (z1 - z1.mean() )/ z1.std()\n",
    "            z2 = (z2 - z2.mean()) / z2.std()\n",
    "            z3 = (z3 - z3.mean()) / z3.std()\n",
    "            z4 = (z4 - z4.mean()) / z4.std()\n",
    "            z5 = (z5 - z5.mean()) / z5.std()\n",
    "\n",
    "            r1 = (r1 - r1.mean()) / r1.std()\n",
    "            r2 = (r2 - r2.mean()) / r2.std()\n",
    "\n",
    "            i1 = (i1 - i1.mean()) / i1.std()\n",
    "            i2 = (i2 - i2.mean()) / i2.std()\n",
    "\n",
    "            X = (X - X.mean()) / X.std()\n",
    "\n",
    "            m1 = (m1 - m1.mean()) / m1.std()\n",
    "            m0 = (m0 - m0.mean()) / m0.std()\n",
    "\n",
    "        M = m1 * X + m0 * (1 - X)\n",
    "\n",
    "        if standardize:\n",
    "            M = (M - M.mean() ) / M.std()\n",
    "        # outcomes:\n",
    "        Y1 = np.random.binomial(1, sigm(np.exp(-1 + m1 - 0.1 * z1 + 0.35 * z2 +\n",
    "                                               0.25 * z3 + 0.2 * z4 + 0.15 * z2 * z4 + r1 + r2)),\n",
    "                                (N, 1))\n",
    "        Y0 = np.random.binomial(1,\n",
    "                                sigm(-1 + m0 - 0.1 * z1 + 0.35 * z2 + 0.25 * z3 + 0.2 * z4 + 0.15 * z2 * z4 + r1 + r2),\n",
    "                                (N, 1))\n",
    "        Y = Y1 * X + Y0 * (1 - X)\n",
    "\n",
    "        if standardize:\n",
    "            Y = (Y - Y.mean() ) / Y.std()\n",
    "\n",
    "        # colliders:\n",
    "        C = 0.6 * Y + 0.4 * X + 0.4 * np.random.randn(N, 1)\n",
    "\n",
    "        all_data_dict = {'Z1': z1, 'Z2': z2, 'Z3': z3, 'Z4': z4, 'Z5': z5, 'X': X, 'M': M, 'I1': i1,\n",
    "                         'I2': i2, 'R1': r1, 'R2': r2, 'Y': Y, 'C': C}\n",
    "\n",
    "        # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "        var_types = {'Z1': 'cont', 'Z2': 'cont', 'Z3': 'cont', 'Z4': 'cont', 'Z5': 'cont', 'X': 'bin', 'M': 'cont', 'I1': 'cont',\n",
    "                         'I2': 'cont', 'R1': 'cont', 'R2': 'cont', 'Y': 'bin', 'C': 'cont'}\n",
    "\n",
    "        DAGnx.add_edges_from([('Z1', 'Z5'), ('Z2', 'X'), ('Z3', 'X'), ('Z4', 'X'), ('Z5', 'X'),\n",
    "                              ('Z2', 'Y'), ('Z3', 'Y'), ('Z4', 'Y'), ('Z5', 'Y'),\n",
    "                              ('R1', 'Y'), ('R2', 'Y'), ('M', 'Y'),\n",
    "                              ('I1', 'X'), ('I2', 'X'), ('X', 'M'), ('X', 'Y'), ('X', 'C'),\n",
    "                              ('Y', 'C')])\n",
    "\n",
    "\n",
    "    elif dataset == 'simple_test':\n",
    "        ux1 = np.random.randn(N, 1)\n",
    "        ux2 = np.random.randn(N, 1)\n",
    "        uy = np.random.randn(N, 1)\n",
    "\n",
    "        X = ux1\n",
    "        X2 = ux2\n",
    "        Y = 0.6 * X - 0.5 * X2  # + uy\n",
    "\n",
    "        if standardize:\n",
    "            X = (X - X.mean() )/ X.std()\n",
    "            X2 = (X2 - X2.mean()) / X2.std()\n",
    "            Y = (Y - Y.mean()) / Y.std()\n",
    "\n",
    "        # outcomes:\n",
    "        Y1 = 0.6 - 0.5 * X2  # + uy\n",
    "        Y0 = -0.5 * X2   # + uy\n",
    "\n",
    "        X_1 = np.full((len(Y1)//2, 1), 1)  # TODO: very simple test dataset  (to be removed)\n",
    "        X2_1 = np.full((len(Y1)//2, 1), 2)\n",
    "        Y_1 = np.full((len(Y1)//2, 1), 3)\n",
    "\n",
    "        X_2 = np.full((len(Y1) // 2, 1), 2)  # TODO: very simple test dataset  (to be removed)\n",
    "        X2_2 = np.full((len(Y1) // 2, 1), 4)\n",
    "        Y_2 = np.full((len(Y1) // 2, 1), 6)\n",
    "\n",
    "        X = np.concatenate((X_1, X_2), 0)\n",
    "        X2 = np.concatenate((X2_1, X2_2), 0)\n",
    "        Y = np.concatenate((Y_1, Y_2), 0)\n",
    "\n",
    "        all_data_dict = {'X': X, 'X2': X2, 'Y': Y}\n",
    "\n",
    "        # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "        var_types = {'X': 'cont', 'X2': 'cont', 'Y': 'cont'}\n",
    "\n",
    "        DAGnx.add_edges_from([('X', 'Y'), ('X2', 'Y')])\n",
    "\n",
    "    elif dataset == 'simple_test_v2':\n",
    "        data = torch.zeros(N, 4, 5)\n",
    "\n",
    "        for i in range(4):\n",
    "            data[:, i, :] = (i + 1)\n",
    "            \n",
    "        A = data[:, 0, :]\n",
    "        B = data[:, 1, :]\n",
    "        C = data[:, 2, :]\n",
    "        Y = data[:, 3, :]\n",
    "        \n",
    "        Y0 = Y\n",
    "        Y1 = Y\n",
    "        \n",
    "        \n",
    "        all_data_dict = {'A': A, 'B': B, 'C': C, 'Y': Y}\n",
    "\n",
    "        # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "        var_types = {'A': 'cont', 'B': 'cont', 'C': 'cont', 'Y': 'cont'}\n",
    "\n",
    "#         DAGnx.add_edges_from([('A', 'B'), ('B', 'C'), ('C', 'Y')])\n",
    "        DAGnx.add_edges_from([('A', 'B'), ('C', 'B')])\n",
    "        DAGnx.add_node('C')\n",
    "        DAGnx.add_node('Y')\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "\n",
    "    plt.title('general')\n",
    "    pos = graphviz_layout(DAGnx, prog='dot')\n",
    "    nx.draw_networkx(DAGnx, pos, with_labels=True, arrows=True)\n",
    "    plt.savefig(f'{dataset}_graph.png')\n",
    "\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "    \n",
    "    print('AFTER:', list(DAGnx.nodes()))\n",
    "    print(nx.to_numpy_array(DAGnx))\n",
    "\n",
    "    return all_data, DAGnx, causal_ordering, var_types, Y0, Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "d423be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "    \n",
    "# adapted from example GPT code  https://github.com/karpathy/ng-video-lecture\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single attention head.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size: int, input_dim: int, dropout_rate: float, dag: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(input_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(input_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(input_dim, head_size, bias=False)\n",
    "    \n",
    "        self.head_size = head_size\n",
    "        # user a register buffer (not a module parameter) for the creation of self.dag\n",
    "        # dag will determine what variables can communicate with each other\n",
    "        self.dag_orig = dag\n",
    "        self.register_buffer('dag_mod', self.dag_orig)  # include transpose\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.act = Swish()\n",
    "        self.att_wei = None\n",
    "        \n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Head module.\n",
    "        \"\"\"\n",
    "        K = self.key(X)  # B, T, hs\n",
    "        Q = self.query(X)  # B, T, hs\n",
    "        V = self.value(X)  # B, T, hs\n",
    "        B, T, HS = Q .shape\n",
    "        QK = torch.matmul(Q, K.transpose(1, 2)) / (self.head_size ** 0.5)\n",
    "        self.att_wei = QK.masked_fill(self.dag_mod == 0, float('-inf')) \n",
    "        self.att_wei = F.softmax(self.att_wei, dim=-1)\n",
    "        nan_rows = torch.any(torch.isnan(self.att_wei), dim=-1)  # check if any rows are <all> -inf, these need to be masked to 0\n",
    "        nan_mask = nan_rows.unsqueeze(-1).expand_as(self.att_wei)\n",
    "        self.att_wei = torch.where(nan_mask, torch.zeros_like(self.att_wei), self.att_wei) # set any rows have nan values (because they have no causal parents) to 0 to avoid nans\n",
    "        out = self.att_wei.transpose(1, 2) @ V  # B, T, hs  Transpose DAG to deal extract correct embeddings from V\n",
    "        out = self.act(out)        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention combining several heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int, input_dim: int, head_size: int, dropout_rate: float, dag: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size, input_dim=input_dim, dropout_rate=dropout_rate, dag=dag) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(int(head_size*num_heads), input_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiHeadAttention module.\n",
    "        \"\"\"\n",
    "        out = torch.cat([h(X) for h in self.heads], dim=-1)  # B, T, num_heads * head_size\n",
    "        out = self.dropout(self.projection(out))  # B, T, input_dim \n",
    "        return self.act(out)\n",
    "\n",
    "\n",
    "class FF(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, ff_n_embed: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, ff_n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_n_embed, input_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the FF module.\n",
    "        \"\"\"\n",
    "        out = self.net(X)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a block containing MultiHeadAttention followed by a feedforward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, ff_n_embed: int, num_heads: int, input_dim: int, head_size: int, dropout_rate: float, dag: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, input_dim=input_dim, head_size=head_size, dropout_rate=dropout_rate, dag=dag)\n",
    "        self.ff = FF(input_dim=input_dim, ff_n_embed=ff_n_embed, dropout_rate=dropout_rate)\n",
    "        if isinstance(dag, torch.Tensor):\n",
    "            dag = dag.clone().detach()\n",
    "        else:\n",
    "            dag = torch.tensor(dag, dtype=torch.float)  # Only convert to tensor if not already one\n",
    "        self.register_buffer('dag_mask', dag.unsqueeze(0))  # Adding batch dimension\n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Block module.\n",
    "        \"\"\"\n",
    "        mha_out = self.mha(X)\n",
    "        ff_out = self.ff(mha_out)\n",
    "        mha_out = mha_out + ff_out\n",
    "        return mha_out\n",
    "    \n",
    "\n",
    "class CaT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int, \n",
    "        num_heads: int, \n",
    "        ff_n_embed: int, \n",
    "        head_size: int, \n",
    "        n_layers: int, \n",
    "        dag: nx.DiGraph, \n",
    "        dropout_rate: float, \n",
    "        var_types_sorted: Dict[str, str], \n",
    "        causal_ordering: Dict[str, int], \n",
    "        device: torch.device\n",
    "    ):\n",
    "        '''\n",
    "        Initialize components of the Causal Transformer.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            ff_n_embed (int): Dimensionality of the feedforward network inside the multi-head attention.\n",
    "            head_size (int): Dimension of each attention head.\n",
    "            n_layers (int): Number of layers in the network.\n",
    "            dag (networkx.DiGraph): Topologically sorted directed acyclic graph.\n",
    "            dropout_rate (float): Dropout rate to use within attention and feedforward layers.\n",
    "            var_types_sorted (dict): Dictionary specifying the variable types ('bin', 'cont', 'cat').\n",
    "            causal_ordering (dict): Ordering of variables for causal relationships.\n",
    "            device (torch.device): The device the model should use.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_n_embed = ff_n_embed\n",
    "        self.head_size = head_size\n",
    "        self.nxdag = dag\n",
    "        self.orig_var_name_ordering = list(self.nxdag.nodes())\n",
    "        dag = torch.tensor(nx.to_numpy_array(self.nxdag)).to(device)\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.causal_ordering = causal_ordering\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.loss_func = MixedLoss(var_types_sorted, orig_var_name_ordering=self.orig_var_name_ordering)\n",
    "        self.lm_head = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Store original and setup DAG\n",
    "        self.original_dag = torch.tensor(dag, dtype=torch.float, device=device)\n",
    "        self.eye = torch.eye(self.original_dag.size(0), device=device)\n",
    "        self.was_shuffled = False \n",
    "\n",
    "        self.initialize_blocks()\n",
    "        \n",
    "        \n",
    "    def modify_dag(self, layer_index: int, dag: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adjusts the DAG for a given layer by optionally adding an identity matrix.\n",
    "\n",
    "        Args:\n",
    "            layer_index (int): Index of the current layer, determining if identity should be added.\n",
    "            dag (torch.Tensor): The current DAG tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The modified DAG tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        modified_dag = dag.clone()\n",
    "        if layer_index > 0:  # Add identity diagonal to ensure self-connections in subsequent layers\n",
    "            modified_dag += self.eye\n",
    "        return torch.clamp(modified_dag, 0, 1)\n",
    "\n",
    "    \n",
    "    def initialize_blocks(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes each layer/block with the appropriate DAG setup for the model.\n",
    "        \"\"\"\n",
    "        for i in range(self.n_layers):\n",
    "            current_dag = self.modify_dag(i, self.original_dag)\n",
    "            self.blocks.append(Block(ff_n_embed=self.ff_n_embed, num_heads=self.num_heads, \n",
    "                                     input_dim=self.input_dim, head_size=self.head_size, \n",
    "                                     dropout_rate=self.dropout_rate, dag=current_dag))\n",
    "\n",
    "    def reset_blocks(self) -> None:\n",
    "        \"\"\"\n",
    "        Resets the DAGs in all heads of all blocks to their original configurations.\n",
    "        \"\"\"\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            original_dag = self.modify_dag(i, self.original_dag)\n",
    "            for head in block.mha.heads:\n",
    "                head.dag_mod = original_dag\n",
    "                \n",
    "                \n",
    "    def forward(self, X: torch.Tensor, targets: Optional[torch.Tensor] = None, shuffling: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Dict]]:\n",
    "        \"\"\"\n",
    "        Processes input through the model, applying shuffling if specified.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor.\n",
    "            targets (torch.Tensor): Target tensor, optional.\n",
    "            shuffling (bool): Whether to shuffle the input and corresponding DAG.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor or tuple: Output tensor or tuple of output, loss, and loss tracker if targets provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        shuffle_ordering = None\n",
    "        if shuffling:\n",
    "            # Shuffle X, targets, and the DAG using the shuffler function\n",
    "            X, shuffled_dag, targets, shuffle_ordering = shuffler(X, targets, self.original_dag.clone().cpu().numpy())\n",
    "            shuffled_dag = torch.tensor(shuffled_dag, dtype=torch.float, device=self.device)\n",
    "\n",
    "            # Apply the shuffled DAG to each block\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                updated_dag = self.modify_dag(i, shuffled_dag)\n",
    "                for head in block.mha.heads:\n",
    "                    head.dag_mod = updated_dag\n",
    "            self.was_shuffled = True  # Mark that we have shuffled in this pass\n",
    "        else:\n",
    "            if self.was_shuffled:\n",
    "                # If the previous call used shuffling but this one does not, reset the DAGs\n",
    "                self.reset_blocks()\n",
    "                self.was_shuffled = False  # Reset the shuffling flag as we have reverted to the original DAG\n",
    "            shuffle_ordering = np.arange(X.shape[1])\n",
    "\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        X = self.lm_head(X)\n",
    "\n",
    "        if targets is None:\n",
    "            return X\n",
    "        else:\n",
    "            loss, loss_tracker = self.loss_func(X, targets, shuffle_ordering)\n",
    "            return X, loss, loss_tracker\n",
    "                    \n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))\n",
    "        m.weight.data.fill_(1)\n",
    "        # Check if the linear module has a bias term and set it to 1 if it exists\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def initialize_model_weights(model):\n",
    "    # Apply init_weights to all sub-modules of the model\n",
    "    model.apply(init_weights)\n",
    "\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, var_types_sorted, orig_var_name_ordering):\n",
    "        super(MixedLoss, self).__init__()\n",
    "        self.orig_var_name_ordering = orig_var_name_ordering\n",
    "        self.var_types_sorted = var_types_sorted  # sorted types for determining which loss to use\n",
    "        self.cont_loss = nn.MSELoss()  # Loss for continuous variables\n",
    "        self.bin_loss = nn.BCEWithLogitsLoss()  # Loss for binary variables\n",
    "        self.cat_loss = nn.CrossEntropyLoss()   # takes logits for each class as input\n",
    "\n",
    "    def forward(self, pred, target, shuffle_ordering):\n",
    "        total_loss = 0\n",
    "        loss_tracking = {}\n",
    "        \n",
    "        pred = pred[:, shuffle_ordering]\n",
    "        target = target[:, shuffle_ordering]\n",
    "        \n",
    "    \n",
    "        for i, var_name in enumerate(self.orig_var_name_ordering):\n",
    "            \n",
    "            var_type = self.var_types_sorted[var_name]\n",
    "            idx = shuffle_ordering[i]\n",
    "\n",
    "            if var_type == 'cont':\n",
    "                loss = self.cont_loss(pred[:, i], target[:, i])\n",
    "            elif var_type == 'bin':\n",
    "                loss = self.bin_loss(pred[:, i], target[:, i])\n",
    "            elif var_type == 'cat':\n",
    "                loss = self.cat_loss(pred[:, i].unsqueeze(0), target[:, i].long())\n",
    "\n",
    "            loss_tracking[var_name] = loss.item()\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss, loss_tracking\n",
    "\n",
    "\n",
    "\n",
    "def shuffler(X, targets, dag):\n",
    "    shuffle_ordering = np.random.permutation(X.shape[1])\n",
    "    shuffled_X = X[:, shuffle_ordering]\n",
    "    shuffled_dag = dag[shuffle_ordering, :][:, shuffle_ordering]\n",
    "    shuffled_targets = targets[:, shuffle_ordering]\n",
    "    return shuffled_X, shuffled_dag, shuffled_targets, shuffle_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "33de7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER: ['A', 'C', 'Y', 'B']\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "AFTER: ['A', 'C', 'Y', 'B']\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Training data size: (80, 4, 5)  Validation data size: (20, 4, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.py \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "def plot_losses(loss_dict):\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    for key, values_list in loss_dict.items():\n",
    "        plt.plot(values_list, label=key)\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('losses.png')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def train(train_data, val_data, max_iters, eval_interval, eval_iters, device, model, batch_size, save_iter,\n",
    "          model_save_path, optimizer, start_iter=None, checkpointing_on=0):\n",
    "    train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "\n",
    "    if start_iter == None:\n",
    "        start_iter = 0\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,  max_iters//10, eta_min=1e-7, verbose=False)\n",
    "    all_var_losses = {}\n",
    "    for iter_ in range(start_iter, max_iters):\n",
    "        # train and update the model\n",
    "        model.train()\n",
    "\n",
    "        xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "        xb_mod = torch.clone(xb.detach())\n",
    "        X, loss, loss_dict = model(X=xb, targets=xb_mod)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "            for key in loss_dict.keys():\n",
    "                if key not in all_var_losses.keys():\n",
    "                    all_var_losses[key] = []\n",
    "                all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "            model.eval()\n",
    "            eval_loss = {}\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "\n",
    "                    xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                                   batch_size=batch_size)\n",
    "                    xb_mod = torch.clone(xb.detach())\n",
    "                    X, loss, loss_dict = model(X=xb, targets=xb_mod)\n",
    "                    losses[k] = loss.item()\n",
    "                eval_loss[split] = losses.mean()\n",
    "            model.train()\n",
    "            print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    "\n",
    "\n",
    "        if (iter_ > 1 ) and (iter_ != start_iter) and ((iter_ + 1) % save_iter == 0) and (checkpointing_on==1):\n",
    "            print('Saving model checkpoint.')\n",
    "            torch.save({\n",
    "                'iteration': iter_,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, os.path.join(model_save_path, 'model_{}_{}.ckpt'.format(iter_+1, np.round(loss.item(), 2))))\n",
    "\n",
    "    print('Finished training!')\n",
    "\n",
    "    plot_losses(all_var_losses)\n",
    "    \n",
    "    \n",
    "num_heads = 2\n",
    "head_size = 5\n",
    "n_layers = 2\n",
    "ff_n_embed = 5  # number of channels/dimensions for the FF part of the block\n",
    "batch_size = 1\n",
    "validation_fraction = 0.2\n",
    "learning_rate = 0.001\n",
    "max_iters = 2\n",
    "dropout_rate = 0.1\n",
    "dataset = 'simple_test_v2'\n",
    "sample_size = 100\n",
    "seed = 1\n",
    "device = 'cpu'\n",
    "optimizer_name = \"Adam\"\n",
    "eval_interval = 2\n",
    "eval_iters = 5\n",
    "\n",
    "\n",
    "_, _, _, _, Y0, Y1 = generate_data(N=sample_size, seed=seed, dataset=dataset, standardize=False)\n",
    "ATE = (Y1 - Y0).mean()  # ATE based off a large sample\n",
    "\n",
    "all_data, DAG, causal_ordering, var_types, Y0, Y1 = generate_data(N=sample_size, seed=seed, dataset=dataset)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "print('Training data size:', train_data.shape, ' Validation data size:', val_data.shape)\n",
    "\n",
    "input_dim = all_data.shape[2]\n",
    "\n",
    "\n",
    "model = CaT(input_dim=input_dim, ff_n_embed=ff_n_embed, dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                dag=DAG,\n",
    "                causal_ordering=causal_ordering,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types_sorted=var_types,\n",
    "                ).to(device)\n",
    "\n",
    "initialize_model_weights(model)\n",
    "optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "8828677d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 2: train_loss 1122381657538560.0000, val loss 1122381657538560.0000\n"
     ]
    }
   ],
   "source": [
    "# train() func\n",
    "start_iter = None\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "\n",
    "if start_iter == None:\n",
    "    start_iter = 0\n",
    "\n",
    "all_var_losses = {}\n",
    "\n",
    "for iter_ in range(start_iter, max_iters):\n",
    "    model.train()\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "            \n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "2fd0f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0., 0., 0. ,1.],\n",
    " [0., 0. ,0., 0.],\n",
    " [0., 0., 0. ,0.],\n",
    " [0., 0., 0. ,0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7c21bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1,1,1],\n",
    " [2,2,2],\n",
    " [3,3,3],\n",
    " [4,4,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "4af16bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad817c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d3952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc78de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data=train_data,\n",
    "                      eval_iters=eval_iters,\n",
    "                      val_data=val_data,\n",
    "                      max_iters=max_iters,\n",
    "                      eval_interval=eval_interval,\n",
    "                      device=device,\n",
    "                      model=model,\n",
    "                      batch_size=batch_size,\n",
    "                      save_iter=save_iter,\n",
    "                      model_save_path=model_save_path,\n",
    "                      optimizer=optimizer,\n",
    "                      checkpointing_on=args.checkpointing_on\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "40580b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'C': 0, 'Y': 0, 'B': 1}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "817567b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b8ac65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8911b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x10 and 4x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_gpt_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_gpt_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x10 and 4x10)"
     ]
    }
   ],
   "source": [
    "a(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcbfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfb6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b13af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fc8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
