{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T11:49:10.406850Z",
     "start_time": "2024-09-28T11:49:09.195929Z"
    }
   },
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from test_model import CaT\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_full_ordering, reorder_dag\n",
    "import utils\n",
    "import utils.inference\n",
    "from utils import inference\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "\n",
    "\n",
    "data_dir = 'real_world_data'\n",
    "data_fn = 'real_world_dataset.csv'\n",
    "data_path = os.path.join(data_dir, data_fn)\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "    \n",
    "w2_age = ['W2_Age_year']\n",
    "w2_keyworker = ['W2_Keyworker']\n",
    "w2_adults = ['W2_Adults_household']\n",
    "w2_children = ['W2_Children_household'] \n",
    "w2_income = ['W2_Change_Income'] \n",
    "w2_covid_anx = ['W2_COVID19_anxiety']\n",
    "w2_risk1month = ['W2_RISK_1month'] \n",
    "\n",
    "w2_gender = ['W2_Gender']\n",
    "\n",
    "# attach style {1,2,3,4}\n",
    "# 1 secure\n",
    "# 2 avoidant/fearful avoidant\n",
    "# 3 anxious\n",
    "# 4 dismissive\n",
    "w2_attachment = ['W2_Attach_Style']\n",
    "\n",
    "w2_loneliness = ['W2_Loneliness1',\n",
    "                'W2_Loneliness2',\n",
    "                'W2_Loneliness3']\n",
    "w2_chronicill = ['W2_Chronic_illness_self']\n",
    "w2_rel_stat = ['W2_Relationship']\n",
    "w2_social_dist = ['W2_SocialDistance1',\n",
    "                'W2_SocialDistance2',\n",
    "                'W2_SocialDistance3',\n",
    "                'W2_SocialDistance4',\n",
    "                'W2_SocialDistance5',\n",
    "                'W2_SocialDistance6',\n",
    "                'W2_SocialDistance7',\n",
    "                'W2_SocialDistance8',\n",
    "                'W2_SocialDistance9',\n",
    "                'W2_SocialDistance10',\n",
    "                'W2_SocialDistance11',\n",
    "                'W2_SocialDistance12',\n",
    "                'W2_SocialDistance13',\n",
    "                'W2_SocialDistance14',\n",
    "                'W2_SocialDistance15',\n",
    "                'W2_SocialDistance16']\n",
    "\n",
    "w2_depression = ['W2_Dep_1',\n",
    "            'W2_Dep_2',\n",
    "            'W2_Dep_3',\n",
    "            'W2_Dep_4',\n",
    "            'W2_Dep_5',\n",
    "            'W2_Dep_6',\n",
    "            'W2_Dep_7',\n",
    "            'W2_Dep_8',\n",
    "            'W2_Dep_9']\n",
    "w2_anx = ['W2_GAD_1',\n",
    "        'W2_GAD_2',\n",
    "        'W2_GAD_3',\n",
    "        'W2_GAD_4',\n",
    "        'W2_GAD_5',\n",
    "        'W2_GAD_6',\n",
    "        'W2_GAD_7']\n",
    "\n",
    "# wave 3 variables\n",
    "w3_age = ['W3_Age_year']  \n",
    "w3_gender = ['W3_Age_year']\n",
    "w3_attachment = ['W3_Attach_style']\n",
    "# wave 1 variables\n",
    "w1_age = ['W1_Age_year']  \n",
    "w1_gender = ['W1_Age_year']\n",
    "w1_attachment = ['W3_Attach_style']  # no W1 value exists, so just try W3 again\n",
    "\n",
    "all_vars = [w2_anx, w2_depression, w2_social_dist, w2_age, w2_keyworker, w2_adults, w2_children, w2_income, w2_covid_anx, w2_risk1month,\n",
    "            w2_gender, w2_attachment, w2_loneliness, w2_chronicill, w2_rel_stat]\n",
    "\n",
    "\n",
    "# Construct mapping for each variable group\n",
    "var_mapping = {\n",
    "    'age': w2_age,\n",
    "    'keyworker': w2_keyworker,\n",
    "    'adults': w2_adults,\n",
    "    'children': w2_children,\n",
    "    'income': w2_income,\n",
    "    'covid_anx': w2_covid_anx,\n",
    "    'risk1month': w2_risk1month,\n",
    "    'gender': w2_gender,\n",
    "    'attachment': w2_attachment,\n",
    "    'loneliness': w2_loneliness,\n",
    "    'chronicill': w2_chronicill,\n",
    "    'relationship_status': w2_rel_stat,\n",
    "    'social_dist': w2_social_dist,\n",
    "    'depression': w2_depression,\n",
    "    'anxiety': w2_anx\n",
    "}\n",
    "\n",
    "var_types = {key: 'cont' for key in var_mapping.keys()}\n",
    "# Create the directed acyclic graph (DAG)\n",
    "DAGnx = nx.DiGraph()\n",
    "\n",
    "# Add links based on the relationships you provided\n",
    "DAGnx.add_edges_from([\n",
    "    ('gender', 'relationship_status'),\n",
    "    ('gender', 'attachment'),\n",
    "    ('gender', 'anxiety'),\n",
    "    ('gender', 'depression'),\n",
    "    ('gender', 'social_dist'),\n",
    "    ('gender', 'loneliness'),\n",
    "    ('attachment', 'loneliness'),\n",
    "    ('attachment', 'anxiety'),\n",
    "    ('attachment', 'depression'),\n",
    "    ('attachment', 'social_dist'),\n",
    "    ('relationship_status', 'anxiety'),\n",
    "    ('relationship_status', 'depression'),\n",
    "    ('relationship_status', 'social_dist'),\n",
    "    ('loneliness', 'anxiety'),\n",
    "    ('loneliness', 'depression'),\n",
    "    ('loneliness', 'social_dist'),\n",
    "    ('chronicill', 'anxiety'),\n",
    "    ('chronicill', 'depression'),\n",
    "    ('chronicill', 'social_dist'),\n",
    "    ('chronicill', 'loneliness'),\n",
    "    ('age', 'anxiety'),\n",
    "    ('age', 'depression'),\n",
    "    ('age', 'social_dist'),\n",
    "    ('keyworker', 'anxiety'),\n",
    "    ('keyworker', 'depression'),\n",
    "    ('keyworker', 'social_dist'),\n",
    "    ('adults', 'anxiety'),\n",
    "    ('adults', 'depression'),\n",
    "    ('adults', 'social_dist'),\n",
    "    ('children', 'anxiety'),\n",
    "    ('children', 'depression'),\n",
    "    ('children', 'social_dist'),\n",
    "    ('income', 'anxiety'),\n",
    "    ('income', 'depression'),\n",
    "    ('income', 'social_dist'),\n",
    "    ('covid_anx', 'anxiety'),\n",
    "    ('covid_anx', 'depression'),\n",
    "    ('covid_anx', 'social_dist'),\n",
    "    ('risk1month', 'anxiety'),\n",
    "    ('risk1month', 'depression'),\n",
    "    ('risk1month', 'social_dist')\n",
    "])\n",
    "\n",
    " "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:49:10.411231Z",
     "start_time": "2024-09-28T11:49:10.408141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DAGnx = utils.utils.reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "var_names = list(DAGnx.nodes())\n",
    "causal_ordering = utils.utils.get_full_ordering(DAGnx)\n",
    "ordered_var_types = dict(sorted(var_types.items(), key=lambda item: causal_ordering[item[0]]))"
   ],
   "id": "a31b52c9e9e3c6ee",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:49:10.441865Z",
     "start_time": "2024-09-28T11:49:10.412126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming df is your pandas DataFrame\n",
    "def extract_variable_data(df, var_names, var_mapping):\n",
    "    arraylist = []\n",
    "    \n",
    "    for var in var_names:\n",
    "        # Extract the column names from the var_mapping dictionary\n",
    "        columns = var_mapping.get(var)\n",
    "        if columns:\n",
    "            # Select the relevant columns from the DataFrame\n",
    "            subset = df[columns]\n",
    "            # Convert the subset to a NumPy array and append to the list\n",
    "            arraylist.append(subset.to_numpy())\n",
    "    \n",
    "    return arraylist\n",
    "    \n",
    "data = pd.read_csv(os.path.join(data_dir, 'real_world_dataset.csv'))\n",
    "arraylist = extract_variable_data(data, var_names, var_mapping)"
   ],
   "id": "1c71a6f702ca06eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:49:10.455608Z",
     "start_time": "2024-09-28T11:49:10.442779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savez_compressed('real_world_data.npz', *arraylist)\n",
    "\n",
    "# To load the arrays back\n",
    "loaded_data = np.load('real_world_data.npz')\n",
    "arraylist_loaded = [loaded_data[key] for key in loaded_data.files]"
   ],
   "id": "96ba77fe22ba3916",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:49:10.465107Z",
     "start_time": "2024-09-28T11:49:10.457065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_data, mask = utils.utils.pad_vectors_with_mask(arraylist_loaded)\n",
    "all_data.shape"
   ],
   "id": "5e57c8a132bc22a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 15, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:54:31.399599Z",
     "start_time": "2024-09-28T11:54:31.395334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shuffling = 1\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 100000\n",
    "batch_size = 10\n",
    "max_iters = 200000\n",
    "eval_interval = 1000\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.0\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 2e-4\n",
    "ff_n_embed = 50\n",
    "num_heads = 10\n",
    "n_layers = 10\n",
    "head_size = 50\n",
    "embed_dim = 50\n",
    "d = data.shape[-1]\n",
    "\n",
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)"
   ],
   "id": "585f34939ae8f5e2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:50:43.287119Z",
     "start_time": "2024-09-28T11:54:32.223711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_sims = 100 \n",
    "subsample_fraction = 0.9  # For example, use 80% of the data in each bootstrap iteration\n",
    "\n",
    "# Lists to store the bootstrap estimates\n",
    "dep_sec_fear = []\n",
    "dep_sec_anx = []\n",
    "dep_sec_avoid  = []\n",
    "\n",
    "anx_sec_fear = []\n",
    "anx_sec_anx = []\n",
    "anx_sec_avoid  = []\n",
    "\n",
    "for i in range(num_sims):\n",
    "    print('BOOTSTRAP ITERATION:', i+1, ' of', num_sims)\n",
    "    # Bootstrap: resample with replacement from a subsample of the original data\n",
    "    num_subsample = int(subsample_fraction * len(all_data))  # Subsampling size\n",
    "    indices = np.random.choice(np.arange(len(all_data)), size=num_subsample, replace=True)  # Subsampling with replacement\n",
    "    \n",
    "    train_inds = indices[int(validation_fraction * len(indices)):]\n",
    "    \n",
    "    train_data = all_data[train_inds]\n",
    "    train_data = torch.from_numpy(train_data).float()\n",
    "    \n",
    "    input_dim = all_data.shape[2]\n",
    "    \n",
    "    model = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                embed_dim= embed_dim,\n",
    "                dag=DAGnx,\n",
    "                use_batch_norm=False,\n",
    "                causal_ordering=causal_ordering,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types=var_types, activation_function='Swish'\n",
    "                ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_iters:\n",
    "            return float(epoch) / float(max(1, warmup_iters))\n",
    "        return 1.0\n",
    "    \n",
    "    warmup_iters = max_iters // 5  # Number of iterations for warmup\n",
    "    scheduler_warmup = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "    scheduler_cyclic = CosineAnnealingLR(optimizer, T_max=max_iters - warmup_iters)\n",
    "    \n",
    "    \n",
    "    # Model training\n",
    "    for iter_ in range(0, max_iters):\n",
    "        model.train()\n",
    "    \n",
    "        xb = get_batch(train_data=train_data, val_data=None, split='train', device=device, batch_size=batch_size)\n",
    "        xb_mod = torch.clone(xb.detach())\n",
    "        X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "        # print('during training orig', model.blocks[0].mha.heads[0].dag_orig)\n",
    "        # print('during training mod', model.blocks[0].mha.heads[0].dag_mod)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter_ < warmup_iters:\n",
    "            scheduler_warmup.step()\n",
    "        else:\n",
    "            scheduler_cyclic.step()\n",
    "            \n",
    "        if iter_ % 100 == 0 or iter_ == max_iters - 1:\n",
    "            print(f\"Iteration {iter_ + 1}/{max_iters}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Causal inference after training\n",
    "    ci = utils.inference.CausalInference(model=model, device=device, mask=mask)\n",
    "    depression_index = inference.find_element_in_list(var_names, target_string='depression')\n",
    "    anx_index = inference.find_element_in_list(var_names, target_string='anxiety')    \n",
    "    # Bootstrapping intervention comparisons for the 6 estimands\n",
    "    D0 = ci.forward(data=all_data, intervention_nodes_vals={'attachment': 0.0})  # Baseline: secure attachment\n",
    "    # print(model.blocks[0].mha.heads[0].dag_orig)\n",
    "    # print(model.blocks[0].mha.heads[0].dag_mod)\n",
    "    # \n",
    "    for att_val, dep_list, anx_list in zip([1.0, 2.0, 3.0], [dep_sec_fear, dep_sec_anx, dep_sec_avoid], [anx_sec_fear, anx_sec_anx, anx_sec_avoid]):\n",
    "        D1 = ci.forward(data=all_data, intervention_nodes_vals={'attachment': att_val})\n",
    "    \n",
    "        est_ATE_dep = (D1[:,depression_index] - D0[:,depression_index]).mean(0)\n",
    "        outcome_dims_dep = len(w2_depression)\n",
    "        est_ATE_dep = est_ATE_dep[0, :outcome_dims_dep].mean()\n",
    "        dep_list.append(est_ATE_dep)\n",
    "    \n",
    "        est_ATE_anx = (D1[:,anx_index] - D0[:,anx_index]).mean(0)\n",
    "        outcome_dims_anx = len(w2_anx)\n",
    "        est_ATE_anx = est_ATE_anx[0, :outcome_dims_anx].mean()\n",
    "        anx_list.append(est_ATE_anx)\n",
    "        \n",
    "        print('Att val:', att_val, 'dep_val:', est_ATE_dep)\n",
    "        print('Att val:', att_val, 'anx_val:', est_ATE_anx)\n",
    "\n",
    "# After collecting the bootstrap estimates, compute both the means and standard errors\n",
    "\n",
    "# Depression: secure to fearful avoidant\n",
    "dep_sec_fear_mean = np.mean(dep_sec_fear)\n",
    "dep_sec_fear_se = np.std(dep_sec_fear, ddof=1)\n",
    "\n",
    "# Depression: secure to anxious\n",
    "dep_sec_anx_mean = np.mean(dep_sec_anx)\n",
    "dep_sec_anx_se = np.std(dep_sec_anx, ddof=1)\n",
    "\n",
    "# Depression: secure to avoidant/dismissive\n",
    "dep_sec_avoid_mean = np.mean(dep_sec_avoid)\n",
    "dep_sec_avoid_se = np.std(dep_sec_avoid, ddof=1)\n",
    "\n",
    "# Anxiety: secure to fearful avoidant\n",
    "anx_sec_fear_mean = np.mean(anx_sec_fear)\n",
    "anx_sec_fear_se = np.std(anx_sec_fear, ddof=1)\n",
    "\n",
    "# Anxiety: secure to anxious\n",
    "anx_sec_anx_mean = np.mean(anx_sec_anx)\n",
    "anx_sec_anx_se = np.std(anx_sec_anx, ddof=1)\n",
    "\n",
    "# Anxiety: secure to avoidant/dismissive\n",
    "anx_sec_avoid_mean = np.mean(anx_sec_avoid)\n",
    "anx_sec_avoid_se = np.std(anx_sec_avoid, ddof=1)\n",
    "\n",
    "# Print the means and standard errors\n",
    "print(f\"Mean Depression (secure to fearful): {dep_sec_fear_mean}, SE: {dep_sec_fear_se}\")\n",
    "print(f\"Mean Depression (secure to anxious): {dep_sec_anx_mean}, SE: {dep_sec_anx_se}\")\n",
    "print(f\"Mean Depression (secure to avoidant): {dep_sec_avoid_mean}, SE: {dep_sec_avoid_se}\")\n",
    "print(f\"Mean Anxiety (secure to fearful): {anx_sec_fear_mean}, SE: {anx_sec_fear_se}\")\n",
    "print(f\"Mean Anxiety (secure to anxious): {anx_sec_anx_mean}, SE: {anx_sec_anx_se}\")\n",
    "print(f\"Mean Anxiety (secure to avoidant): {anx_sec_avoid_mean}, SE: {anx_sec_avoid_se}\")\n"
   ],
   "id": "ef6c3689b460861a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOTSTRAP ITERATION: 1  of 100\n",
      "Iteration 1/200000, Loss: 189.7647\n",
      "Iteration 101/200000, Loss: 452.6015\n",
      "Iteration 201/200000, Loss: 444.0952\n",
      "Iteration 301/200000, Loss: 343.1272\n",
      "Iteration 401/200000, Loss: 527.0229\n",
      "Iteration 501/200000, Loss: 374.5998\n",
      "Iteration 601/200000, Loss: 1060.5983\n",
      "Iteration 701/200000, Loss: 639.3994\n",
      "Iteration 801/200000, Loss: 114.8521\n",
      "Iteration 901/200000, Loss: 267.4406\n",
      "Iteration 1001/200000, Loss: 613.2446\n",
      "Iteration 1101/200000, Loss: 690.0612\n",
      "Iteration 1201/200000, Loss: 819.1526\n",
      "Iteration 1301/200000, Loss: 674.3413\n",
      "Iteration 1401/200000, Loss: 393.5178\n",
      "Iteration 1501/200000, Loss: 471.5853\n",
      "Iteration 1601/200000, Loss: 356.1085\n",
      "Iteration 1701/200000, Loss: 426.4746\n",
      "Iteration 1801/200000, Loss: 318.8799\n",
      "Iteration 1901/200000, Loss: 243.2748\n",
      "Iteration 2001/200000, Loss: 290.3391\n",
      "Iteration 2101/200000, Loss: 626.2444\n",
      "Iteration 2201/200000, Loss: 173.7710\n",
      "Iteration 2301/200000, Loss: 263.4580\n",
      "Iteration 2401/200000, Loss: 483.4724\n",
      "Iteration 2501/200000, Loss: 459.7811\n",
      "Iteration 2601/200000, Loss: 538.7281\n",
      "Iteration 2701/200000, Loss: 507.0681\n",
      "Iteration 2801/200000, Loss: 253.9461\n",
      "Iteration 2901/200000, Loss: 104.7415\n",
      "Iteration 3001/200000, Loss: 143.1372\n",
      "Iteration 3101/200000, Loss: 31.0828\n",
      "Iteration 3201/200000, Loss: 179.2698\n",
      "Iteration 3301/200000, Loss: 367.6760\n",
      "Iteration 3401/200000, Loss: 238.1712\n",
      "Iteration 3501/200000, Loss: 22.5076\n",
      "Iteration 3601/200000, Loss: 610.2702\n",
      "Iteration 3701/200000, Loss: 65.9189\n",
      "Iteration 3801/200000, Loss: 197.3024\n",
      "Iteration 3901/200000, Loss: 268.9630\n",
      "Iteration 4001/200000, Loss: 309.9624\n",
      "Iteration 4101/200000, Loss: 446.1108\n",
      "Iteration 4201/200000, Loss: 233.8052\n",
      "Iteration 4301/200000, Loss: 29.1541\n",
      "Iteration 4401/200000, Loss: 209.8843\n",
      "Iteration 4501/200000, Loss: 342.0717\n",
      "Iteration 4601/200000, Loss: 259.3450\n",
      "Iteration 4701/200000, Loss: 386.6019\n",
      "Iteration 4801/200000, Loss: 325.6561\n",
      "Iteration 4901/200000, Loss: 192.9198\n",
      "Iteration 5001/200000, Loss: 236.9890\n",
      "Iteration 5101/200000, Loss: 186.1599\n",
      "Iteration 5201/200000, Loss: 462.2140\n",
      "Iteration 5301/200000, Loss: 27.1659\n",
      "Iteration 5401/200000, Loss: 251.7599\n",
      "Iteration 5501/200000, Loss: 400.5788\n",
      "Iteration 5601/200000, Loss: 25.4674\n",
      "Iteration 5701/200000, Loss: 373.6621\n",
      "Iteration 5801/200000, Loss: 230.5029\n",
      "Iteration 5901/200000, Loss: 116.7718\n",
      "Iteration 6001/200000, Loss: 431.9970\n",
      "Iteration 6101/200000, Loss: 7.5424\n",
      "Iteration 6201/200000, Loss: 339.6338\n",
      "Iteration 6301/200000, Loss: 155.8794\n",
      "Iteration 6401/200000, Loss: 540.4471\n",
      "Iteration 6501/200000, Loss: 22.5923\n",
      "Iteration 6601/200000, Loss: 216.7023\n",
      "Iteration 6701/200000, Loss: 401.0982\n",
      "Iteration 6801/200000, Loss: 292.3474\n",
      "Iteration 6901/200000, Loss: 184.6897\n",
      "Iteration 7001/200000, Loss: 403.8464\n",
      "Iteration 7101/200000, Loss: 287.6656\n",
      "Iteration 7201/200000, Loss: 103.1026\n",
      "Iteration 7301/200000, Loss: 256.3822\n",
      "Iteration 7401/200000, Loss: 305.3990\n",
      "Iteration 7501/200000, Loss: 380.4959\n",
      "Iteration 7601/200000, Loss: 26.1823\n",
      "Iteration 7701/200000, Loss: 590.3204\n",
      "Iteration 7801/200000, Loss: 464.0853\n",
      "Iteration 7901/200000, Loss: 19.6514\n",
      "Iteration 8001/200000, Loss: 208.7373\n",
      "Iteration 8101/200000, Loss: 421.4162\n",
      "Iteration 8201/200000, Loss: 169.5098\n",
      "Iteration 8301/200000, Loss: 18.4381\n",
      "Iteration 8401/200000, Loss: 672.5176\n",
      "Iteration 8501/200000, Loss: 148.0496\n",
      "Iteration 8601/200000, Loss: 177.5095\n",
      "Iteration 8701/200000, Loss: 310.3376\n",
      "Iteration 8801/200000, Loss: 152.7817\n",
      "Iteration 8901/200000, Loss: 150.4769\n",
      "Iteration 9001/200000, Loss: 457.0139\n",
      "Iteration 9101/200000, Loss: 107.7615\n",
      "Iteration 9201/200000, Loss: 521.0820\n",
      "Iteration 9301/200000, Loss: 152.3002\n",
      "Iteration 9401/200000, Loss: 398.1949\n",
      "Iteration 9501/200000, Loss: 19.0286\n",
      "Iteration 9601/200000, Loss: 323.0038\n",
      "Iteration 9701/200000, Loss: 149.1280\n",
      "Iteration 9801/200000, Loss: 373.3934\n",
      "Iteration 9901/200000, Loss: 25.4901\n",
      "Iteration 10001/200000, Loss: 190.4348\n",
      "Iteration 10101/200000, Loss: 196.5228\n",
      "Iteration 10201/200000, Loss: 466.9086\n",
      "Iteration 10301/200000, Loss: 153.2005\n",
      "Iteration 10401/200000, Loss: 235.9677\n",
      "Iteration 10501/200000, Loss: 91.1504\n",
      "Iteration 10601/200000, Loss: 188.2745\n",
      "Iteration 10701/200000, Loss: 183.1833\n",
      "Iteration 10801/200000, Loss: 118.8484\n",
      "Iteration 10901/200000, Loss: 211.1768\n",
      "Iteration 11001/200000, Loss: 103.3729\n",
      "Iteration 11101/200000, Loss: 149.3971\n",
      "Iteration 11201/200000, Loss: 29.8077\n",
      "Iteration 11301/200000, Loss: 337.7834\n",
      "Iteration 11401/200000, Loss: 199.3514\n",
      "Iteration 11501/200000, Loss: 114.0646\n",
      "Iteration 11601/200000, Loss: 100.6098\n",
      "Iteration 11701/200000, Loss: 229.7413\n",
      "Iteration 11801/200000, Loss: 99.3495\n",
      "Iteration 11901/200000, Loss: 155.1996\n",
      "Iteration 12001/200000, Loss: 42.4097\n",
      "Iteration 12101/200000, Loss: 115.6991\n",
      "Iteration 12201/200000, Loss: 118.3522\n",
      "Iteration 12301/200000, Loss: 218.6594\n",
      "Iteration 12401/200000, Loss: 250.4190\n",
      "Iteration 12501/200000, Loss: 177.4494\n",
      "Iteration 12601/200000, Loss: 113.1911\n",
      "Iteration 12701/200000, Loss: 195.4568\n",
      "Iteration 12801/200000, Loss: 567.5584\n",
      "Iteration 12901/200000, Loss: 154.9818\n",
      "Iteration 13001/200000, Loss: 29.6056\n",
      "Iteration 13101/200000, Loss: 294.2686\n",
      "Iteration 13201/200000, Loss: 47.9380\n",
      "Iteration 13301/200000, Loss: 227.9628\n",
      "Iteration 13401/200000, Loss: 49.4292\n",
      "Iteration 13501/200000, Loss: 39.8691\n",
      "Iteration 13601/200000, Loss: 376.9418\n",
      "Iteration 13701/200000, Loss: 125.4316\n",
      "Iteration 13801/200000, Loss: 146.6353\n",
      "Iteration 13901/200000, Loss: 74.0439\n",
      "Iteration 14001/200000, Loss: 315.0821\n",
      "Iteration 14101/200000, Loss: 75.9732\n",
      "Iteration 14201/200000, Loss: 353.2446\n",
      "Iteration 14301/200000, Loss: 220.4224\n",
      "Iteration 14401/200000, Loss: 274.7404\n",
      "Iteration 14501/200000, Loss: 80.8763\n",
      "Iteration 14601/200000, Loss: 245.1974\n",
      "Iteration 14701/200000, Loss: 133.6192\n",
      "Iteration 14801/200000, Loss: 131.5940\n",
      "Iteration 14901/200000, Loss: 34.0378\n",
      "Iteration 15001/200000, Loss: 170.7765\n",
      "Iteration 15101/200000, Loss: 173.4318\n",
      "Iteration 15201/200000, Loss: 138.5751\n",
      "Iteration 15301/200000, Loss: 285.5405\n",
      "Iteration 15401/200000, Loss: 171.8426\n",
      "Iteration 15501/200000, Loss: 143.5340\n",
      "Iteration 15601/200000, Loss: 12.9339\n",
      "Iteration 15701/200000, Loss: 254.7675\n",
      "Iteration 15801/200000, Loss: 37.3649\n",
      "Iteration 15901/200000, Loss: 246.4238\n",
      "Iteration 16001/200000, Loss: 94.0237\n",
      "Iteration 16101/200000, Loss: 182.8468\n",
      "Iteration 16201/200000, Loss: 59.1540\n",
      "Iteration 16301/200000, Loss: 169.4509\n",
      "Iteration 16401/200000, Loss: 302.8319\n",
      "Iteration 16501/200000, Loss: 204.8298\n",
      "Iteration 16601/200000, Loss: 260.1091\n",
      "Iteration 16701/200000, Loss: 205.4823\n",
      "Iteration 16801/200000, Loss: 141.3958\n",
      "Iteration 16901/200000, Loss: 234.5171\n",
      "Iteration 17001/200000, Loss: 199.9959\n",
      "Iteration 17101/200000, Loss: 104.2465\n",
      "Iteration 17201/200000, Loss: 46.6660\n",
      "Iteration 17301/200000, Loss: 50.1031\n",
      "Iteration 17401/200000, Loss: 81.3736\n",
      "Iteration 17501/200000, Loss: 285.9941\n",
      "Iteration 17601/200000, Loss: 126.6480\n",
      "Iteration 17701/200000, Loss: 312.0082\n",
      "Iteration 17801/200000, Loss: 62.0542\n",
      "Iteration 17901/200000, Loss: 208.1439\n",
      "Iteration 18001/200000, Loss: 233.5675\n",
      "Iteration 18101/200000, Loss: 77.4013\n",
      "Iteration 18201/200000, Loss: 60.2980\n",
      "Iteration 18301/200000, Loss: 29.9644\n",
      "Iteration 18401/200000, Loss: 318.2654\n",
      "Iteration 18501/200000, Loss: 396.3546\n",
      "Iteration 18601/200000, Loss: 79.9653\n",
      "Iteration 18701/200000, Loss: 173.1861\n",
      "Iteration 18801/200000, Loss: 145.3458\n",
      "Iteration 18901/200000, Loss: 128.4559\n",
      "Iteration 19001/200000, Loss: 263.6162\n",
      "Iteration 19101/200000, Loss: 244.7559\n",
      "Iteration 19201/200000, Loss: 162.7191\n",
      "Iteration 19301/200000, Loss: 237.9213\n",
      "Iteration 19401/200000, Loss: 316.6688\n",
      "Iteration 19501/200000, Loss: 192.1148\n",
      "Iteration 19601/200000, Loss: 495.1075\n",
      "Iteration 19701/200000, Loss: 147.2318\n",
      "Iteration 19801/200000, Loss: 291.3223\n",
      "Iteration 19901/200000, Loss: 38.9623\n",
      "Iteration 20001/200000, Loss: 185.4313\n",
      "Iteration 20101/200000, Loss: 260.8900\n",
      "Iteration 20201/200000, Loss: 265.6028\n",
      "Iteration 20301/200000, Loss: 83.9253\n",
      "Iteration 20401/200000, Loss: 57.0675\n",
      "Iteration 20501/200000, Loss: 33.3887\n",
      "Iteration 20601/200000, Loss: 156.5563\n",
      "Iteration 20701/200000, Loss: 137.6349\n",
      "Iteration 20801/200000, Loss: 64.1640\n",
      "Iteration 20901/200000, Loss: 64.4184\n",
      "Iteration 21001/200000, Loss: 55.4526\n",
      "Iteration 21101/200000, Loss: 250.5323\n",
      "Iteration 21201/200000, Loss: 53.8579\n",
      "Iteration 21301/200000, Loss: 46.8460\n",
      "Iteration 21401/200000, Loss: 105.7282\n",
      "Iteration 21501/200000, Loss: 252.1243\n",
      "Iteration 21601/200000, Loss: 272.5082\n",
      "Iteration 21701/200000, Loss: 277.2239\n",
      "Iteration 21801/200000, Loss: 447.7806\n",
      "Iteration 21901/200000, Loss: 81.1587\n",
      "Iteration 22001/200000, Loss: 77.6858\n",
      "Iteration 22101/200000, Loss: 278.5817\n",
      "Iteration 22201/200000, Loss: 214.0281\n",
      "Iteration 22301/200000, Loss: 116.3810\n",
      "Iteration 22401/200000, Loss: 330.5529\n",
      "Iteration 22501/200000, Loss: 162.5111\n",
      "Iteration 22601/200000, Loss: 171.8294\n",
      "Iteration 22701/200000, Loss: 98.8813\n",
      "Iteration 22801/200000, Loss: 242.6105\n",
      "Iteration 22901/200000, Loss: 141.2916\n",
      "Iteration 23001/200000, Loss: 174.7702\n",
      "Iteration 23101/200000, Loss: 109.7849\n",
      "Iteration 23201/200000, Loss: 337.9531\n",
      "Iteration 23301/200000, Loss: 223.0920\n",
      "Iteration 23401/200000, Loss: 317.1248\n",
      "Iteration 23501/200000, Loss: 84.6852\n",
      "Iteration 23601/200000, Loss: 152.0348\n",
      "Iteration 23701/200000, Loss: 331.6604\n",
      "Iteration 23801/200000, Loss: 119.6543\n",
      "Iteration 23901/200000, Loss: 204.0304\n",
      "Iteration 24001/200000, Loss: 106.9584\n",
      "Iteration 24101/200000, Loss: 32.6176\n",
      "Iteration 24201/200000, Loss: 222.1094\n",
      "Iteration 24301/200000, Loss: 110.7767\n",
      "Iteration 24401/200000, Loss: 123.0309\n",
      "Iteration 24501/200000, Loss: 51.3614\n",
      "Iteration 24601/200000, Loss: 238.6041\n",
      "Iteration 24701/200000, Loss: 339.8539\n",
      "Iteration 24801/200000, Loss: 175.2534\n",
      "Iteration 24901/200000, Loss: 235.5329\n",
      "Iteration 25001/200000, Loss: 194.1254\n",
      "Iteration 25101/200000, Loss: 77.2497\n",
      "Iteration 25201/200000, Loss: 515.4205\n",
      "Iteration 25301/200000, Loss: 51.3912\n",
      "Iteration 25401/200000, Loss: 41.5294\n",
      "Iteration 25501/200000, Loss: 132.5407\n",
      "Iteration 25601/200000, Loss: 87.7394\n",
      "Iteration 25701/200000, Loss: 157.3610\n",
      "Iteration 25801/200000, Loss: 150.0025\n",
      "Iteration 25901/200000, Loss: 143.9053\n",
      "Iteration 26001/200000, Loss: 238.3482\n",
      "Iteration 26101/200000, Loss: 180.2730\n",
      "Iteration 26201/200000, Loss: 369.2061\n",
      "Iteration 26301/200000, Loss: 91.9697\n",
      "Iteration 26401/200000, Loss: 99.8214\n",
      "Iteration 26501/200000, Loss: 137.8195\n",
      "Iteration 26601/200000, Loss: 70.7817\n",
      "Iteration 26701/200000, Loss: 233.6690\n",
      "Iteration 26801/200000, Loss: 121.4670\n",
      "Iteration 26901/200000, Loss: 174.4714\n",
      "Iteration 27001/200000, Loss: 185.0947\n",
      "Iteration 27101/200000, Loss: 184.3677\n",
      "Iteration 27201/200000, Loss: 176.4858\n",
      "Iteration 27301/200000, Loss: 94.4509\n",
      "Iteration 27401/200000, Loss: 240.4372\n",
      "Iteration 27501/200000, Loss: 44.2534\n",
      "Iteration 27601/200000, Loss: 232.6647\n",
      "Iteration 27701/200000, Loss: 155.6712\n",
      "Iteration 27801/200000, Loss: 234.2251\n",
      "Iteration 27901/200000, Loss: 251.6291\n",
      "Iteration 28001/200000, Loss: 113.8639\n",
      "Iteration 28101/200000, Loss: 110.0834\n",
      "Iteration 28201/200000, Loss: 174.7775\n",
      "Iteration 28301/200000, Loss: 207.9483\n",
      "Iteration 28401/200000, Loss: 34.1022\n",
      "Iteration 28501/200000, Loss: 164.4660\n",
      "Iteration 28601/200000, Loss: 352.6376\n",
      "Iteration 28701/200000, Loss: 233.8567\n",
      "Iteration 28801/200000, Loss: 126.6453\n",
      "Iteration 28901/200000, Loss: 159.8387\n",
      "Iteration 29001/200000, Loss: 390.0185\n",
      "Iteration 29101/200000, Loss: 153.8336\n",
      "Iteration 29201/200000, Loss: 18.5389\n",
      "Iteration 29301/200000, Loss: 188.0002\n",
      "Iteration 29401/200000, Loss: 323.8296\n",
      "Iteration 29501/200000, Loss: 161.2614\n",
      "Iteration 29601/200000, Loss: 37.0543\n",
      "Iteration 29701/200000, Loss: 232.7005\n",
      "Iteration 29801/200000, Loss: 285.2368\n",
      "Iteration 29901/200000, Loss: 232.8495\n",
      "Iteration 30001/200000, Loss: 22.5780\n",
      "Iteration 30101/200000, Loss: 171.8146\n",
      "Iteration 30201/200000, Loss: 191.5455\n",
      "Iteration 30301/200000, Loss: 405.8473\n",
      "Iteration 30401/200000, Loss: 142.4767\n",
      "Iteration 30501/200000, Loss: 151.3385\n",
      "Iteration 30601/200000, Loss: 87.6196\n",
      "Iteration 30701/200000, Loss: 388.2675\n",
      "Iteration 30801/200000, Loss: 119.7594\n",
      "Iteration 30901/200000, Loss: 302.0547\n",
      "Iteration 31001/200000, Loss: 138.4480\n",
      "Iteration 31101/200000, Loss: 110.5917\n",
      "Iteration 31201/200000, Loss: 325.5818\n",
      "Iteration 31301/200000, Loss: 146.7820\n",
      "Iteration 31401/200000, Loss: 230.3417\n",
      "Iteration 31501/200000, Loss: 277.8081\n",
      "Iteration 31601/200000, Loss: 37.7399\n",
      "Iteration 31701/200000, Loss: 86.5838\n",
      "Iteration 31801/200000, Loss: 54.4231\n",
      "Iteration 31901/200000, Loss: 290.2134\n",
      "Iteration 32001/200000, Loss: 224.4450\n",
      "Iteration 32101/200000, Loss: 133.4261\n",
      "Iteration 32201/200000, Loss: 50.1028\n",
      "Iteration 32301/200000, Loss: 79.4843\n",
      "Iteration 32401/200000, Loss: 64.6362\n",
      "Iteration 32501/200000, Loss: 215.6737\n",
      "Iteration 32601/200000, Loss: 112.4523\n",
      "Iteration 32701/200000, Loss: 280.2716\n",
      "Iteration 32801/200000, Loss: 51.7833\n",
      "Iteration 32901/200000, Loss: 214.5800\n",
      "Iteration 33001/200000, Loss: 125.5621\n",
      "Iteration 33101/200000, Loss: 187.9711\n",
      "Iteration 33201/200000, Loss: 342.8353\n",
      "Iteration 33301/200000, Loss: 256.3116\n",
      "Iteration 33401/200000, Loss: 73.2614\n",
      "Iteration 33501/200000, Loss: 195.7267\n",
      "Iteration 33601/200000, Loss: 339.9284\n",
      "Iteration 33701/200000, Loss: 53.0774\n",
      "Iteration 33801/200000, Loss: 246.7333\n",
      "Iteration 33901/200000, Loss: 233.6857\n",
      "Iteration 34001/200000, Loss: 217.5165\n",
      "Iteration 34101/200000, Loss: 119.8962\n",
      "Iteration 34201/200000, Loss: 267.7659\n",
      "Iteration 34301/200000, Loss: 153.1079\n",
      "Iteration 34401/200000, Loss: 200.6882\n",
      "Iteration 34501/200000, Loss: 139.6003\n",
      "Iteration 34601/200000, Loss: 67.3564\n",
      "Iteration 34701/200000, Loss: 188.7097\n",
      "Iteration 34801/200000, Loss: 25.5248\n",
      "Iteration 34901/200000, Loss: 167.8923\n",
      "Iteration 35001/200000, Loss: 200.0987\n",
      "Iteration 35101/200000, Loss: 33.8176\n",
      "Iteration 35201/200000, Loss: 260.3969\n",
      "Iteration 35301/200000, Loss: 172.0095\n",
      "Iteration 35401/200000, Loss: 59.8025\n",
      "Iteration 35501/200000, Loss: 397.9437\n",
      "Iteration 35601/200000, Loss: 109.8109\n",
      "Iteration 35701/200000, Loss: 164.9641\n",
      "Iteration 35801/200000, Loss: 157.2891\n",
      "Iteration 35901/200000, Loss: 218.0045\n",
      "Iteration 36001/200000, Loss: 154.5940\n",
      "Iteration 36101/200000, Loss: 146.0829\n",
      "Iteration 36201/200000, Loss: 113.4782\n",
      "Iteration 36301/200000, Loss: 272.8384\n",
      "Iteration 36401/200000, Loss: 389.6528\n",
      "Iteration 36501/200000, Loss: 272.6677\n",
      "Iteration 36601/200000, Loss: 70.0209\n",
      "Iteration 36701/200000, Loss: 168.1396\n",
      "Iteration 36801/200000, Loss: 224.4467\n",
      "Iteration 36901/200000, Loss: 146.4441\n",
      "Iteration 37001/200000, Loss: 123.2629\n",
      "Iteration 37101/200000, Loss: 244.6042\n",
      "Iteration 37201/200000, Loss: 78.6680\n",
      "Iteration 37301/200000, Loss: 123.6288\n",
      "Iteration 37401/200000, Loss: 205.0056\n",
      "Iteration 37501/200000, Loss: 54.4368\n",
      "Iteration 37601/200000, Loss: 281.6041\n",
      "Iteration 37701/200000, Loss: 199.2624\n",
      "Iteration 37801/200000, Loss: 295.5111\n",
      "Iteration 37901/200000, Loss: 109.6678\n",
      "Iteration 38001/200000, Loss: 248.7459\n",
      "Iteration 38101/200000, Loss: 109.5410\n",
      "Iteration 38201/200000, Loss: 158.1171\n",
      "Iteration 38301/200000, Loss: 71.8991\n",
      "Iteration 38401/200000, Loss: 265.5865\n",
      "Iteration 38501/200000, Loss: 195.9453\n",
      "Iteration 38601/200000, Loss: 298.8327\n",
      "Iteration 38701/200000, Loss: 152.9700\n",
      "Iteration 38801/200000, Loss: 385.2824\n",
      "Iteration 38901/200000, Loss: 155.1556\n",
      "Iteration 39001/200000, Loss: 252.7168\n",
      "Iteration 39101/200000, Loss: 104.0564\n",
      "Iteration 39201/200000, Loss: 138.8235\n",
      "Iteration 39301/200000, Loss: 126.0119\n",
      "Iteration 39401/200000, Loss: 110.1056\n",
      "Iteration 39501/200000, Loss: 45.6449\n",
      "Iteration 39601/200000, Loss: 256.3036\n",
      "Iteration 39701/200000, Loss: 106.0829\n",
      "Iteration 39801/200000, Loss: 324.1046\n",
      "Iteration 39901/200000, Loss: 276.0482\n",
      "Iteration 40001/200000, Loss: 320.1091\n",
      "Iteration 40101/200000, Loss: 546.9591\n",
      "Iteration 40201/200000, Loss: 152.8313\n",
      "Iteration 40301/200000, Loss: 214.9269\n",
      "Iteration 40401/200000, Loss: 75.2856\n",
      "Iteration 40501/200000, Loss: 160.9135\n",
      "Iteration 40601/200000, Loss: 89.4747\n",
      "Iteration 40701/200000, Loss: 196.9883\n",
      "Iteration 40801/200000, Loss: 167.8623\n",
      "Iteration 40901/200000, Loss: 214.8510\n",
      "Iteration 41001/200000, Loss: 289.6493\n",
      "Iteration 41101/200000, Loss: 247.0101\n",
      "Iteration 41201/200000, Loss: 26.5651\n",
      "Iteration 41301/200000, Loss: 134.9046\n",
      "Iteration 41401/200000, Loss: 101.5816\n",
      "Iteration 41501/200000, Loss: 66.9073\n",
      "Iteration 41601/200000, Loss: 205.1102\n",
      "Iteration 41701/200000, Loss: 71.6626\n",
      "Iteration 41801/200000, Loss: 236.0878\n",
      "Iteration 41901/200000, Loss: 183.0517\n",
      "Iteration 42001/200000, Loss: 104.4973\n",
      "Iteration 42101/200000, Loss: 63.0113\n",
      "Iteration 42201/200000, Loss: 210.5911\n",
      "Iteration 42301/200000, Loss: 163.6057\n",
      "Iteration 42401/200000, Loss: 124.3214\n",
      "Iteration 42501/200000, Loss: 194.6002\n",
      "Iteration 42601/200000, Loss: 167.0203\n",
      "Iteration 42701/200000, Loss: 111.6842\n",
      "Iteration 42801/200000, Loss: 208.7350\n",
      "Iteration 42901/200000, Loss: 92.6142\n",
      "Iteration 43001/200000, Loss: 222.2742\n",
      "Iteration 43101/200000, Loss: 247.8997\n",
      "Iteration 43201/200000, Loss: 111.0626\n",
      "Iteration 43301/200000, Loss: 211.7973\n",
      "Iteration 43401/200000, Loss: 28.6941\n",
      "Iteration 43501/200000, Loss: 153.5236\n",
      "Iteration 43601/200000, Loss: 302.6732\n",
      "Iteration 43701/200000, Loss: 191.1147\n",
      "Iteration 43801/200000, Loss: 260.5451\n",
      "Iteration 43901/200000, Loss: 103.0958\n",
      "Iteration 44001/200000, Loss: 88.7910\n",
      "Iteration 44101/200000, Loss: 116.5757\n",
      "Iteration 44201/200000, Loss: 204.5266\n",
      "Iteration 44301/200000, Loss: 110.8979\n",
      "Iteration 44401/200000, Loss: 245.1457\n",
      "Iteration 44501/200000, Loss: 283.0664\n",
      "Iteration 44601/200000, Loss: 152.2480\n",
      "Iteration 44701/200000, Loss: 233.7368\n",
      "Iteration 44801/200000, Loss: 280.3580\n",
      "Iteration 44901/200000, Loss: 168.3654\n",
      "Iteration 45001/200000, Loss: 88.0239\n",
      "Iteration 45101/200000, Loss: 43.8326\n",
      "Iteration 45201/200000, Loss: 133.3562\n",
      "Iteration 45301/200000, Loss: 222.7684\n",
      "Iteration 45401/200000, Loss: 96.8578\n",
      "Iteration 45501/200000, Loss: 356.0699\n",
      "Iteration 45601/200000, Loss: 134.4535\n",
      "Iteration 45701/200000, Loss: 206.2339\n",
      "Iteration 45801/200000, Loss: 98.4255\n",
      "Iteration 45901/200000, Loss: 431.0602\n",
      "Iteration 46001/200000, Loss: 209.1812\n",
      "Iteration 46101/200000, Loss: 27.3710\n",
      "Iteration 46201/200000, Loss: 328.5621\n",
      "Iteration 46301/200000, Loss: 230.2041\n",
      "Iteration 46401/200000, Loss: 196.5233\n",
      "Iteration 46501/200000, Loss: 99.4481\n",
      "Iteration 46601/200000, Loss: 237.8331\n",
      "Iteration 46701/200000, Loss: 211.2631\n",
      "Iteration 46801/200000, Loss: 323.8690\n",
      "Iteration 46901/200000, Loss: 152.8217\n",
      "Iteration 47001/200000, Loss: 36.9158\n",
      "Iteration 47101/200000, Loss: 267.8032\n",
      "Iteration 47201/200000, Loss: 148.4353\n",
      "Iteration 47301/200000, Loss: 165.3769\n",
      "Iteration 47401/200000, Loss: 145.8680\n",
      "Iteration 47501/200000, Loss: 167.6835\n",
      "Iteration 47601/200000, Loss: 118.3092\n",
      "Iteration 47701/200000, Loss: 205.6323\n",
      "Iteration 47801/200000, Loss: 309.6710\n",
      "Iteration 47901/200000, Loss: 167.8523\n",
      "Iteration 48001/200000, Loss: 109.2333\n",
      "Iteration 48101/200000, Loss: 283.6392\n",
      "Iteration 48201/200000, Loss: 157.3162\n",
      "Iteration 48301/200000, Loss: 146.9156\n",
      "Iteration 48401/200000, Loss: 451.9814\n",
      "Iteration 48501/200000, Loss: 221.9500\n",
      "Iteration 48601/200000, Loss: 87.2543\n",
      "Iteration 48701/200000, Loss: 193.3097\n",
      "Iteration 48801/200000, Loss: 135.5991\n",
      "Iteration 48901/200000, Loss: 39.3767\n",
      "Iteration 49001/200000, Loss: 284.2220\n",
      "Iteration 49101/200000, Loss: 402.1112\n",
      "Iteration 49201/200000, Loss: 136.5587\n",
      "Iteration 49301/200000, Loss: 109.7625\n",
      "Iteration 49401/200000, Loss: 261.3989\n",
      "Iteration 49501/200000, Loss: 391.3128\n",
      "Iteration 49601/200000, Loss: 390.9381\n",
      "Iteration 49701/200000, Loss: 64.1170\n",
      "Iteration 49801/200000, Loss: 40.5586\n",
      "Iteration 49901/200000, Loss: 34.4199\n",
      "Iteration 50001/200000, Loss: 234.1277\n",
      "Iteration 50101/200000, Loss: 70.2407\n",
      "Iteration 50201/200000, Loss: 239.0611\n",
      "Iteration 50301/200000, Loss: 78.6370\n",
      "Iteration 50401/200000, Loss: 141.8133\n",
      "Iteration 50501/200000, Loss: 146.0164\n",
      "Iteration 50601/200000, Loss: 274.1919\n",
      "Iteration 50701/200000, Loss: 122.6183\n",
      "Iteration 50801/200000, Loss: 195.1003\n",
      "Iteration 50901/200000, Loss: 74.9776\n",
      "Iteration 51001/200000, Loss: 58.5578\n",
      "Iteration 51101/200000, Loss: 47.9884\n",
      "Iteration 51201/200000, Loss: 398.2881\n",
      "Iteration 51301/200000, Loss: 75.6653\n",
      "Iteration 51401/200000, Loss: 33.3148\n",
      "Iteration 51501/200000, Loss: 158.4669\n",
      "Iteration 51601/200000, Loss: 116.0194\n",
      "Iteration 51701/200000, Loss: 342.0067\n",
      "Iteration 51801/200000, Loss: 476.7508\n",
      "Iteration 51901/200000, Loss: 77.8680\n",
      "Iteration 52001/200000, Loss: 161.1564\n",
      "Iteration 52101/200000, Loss: 210.4398\n",
      "Iteration 52201/200000, Loss: 253.4404\n",
      "Iteration 52301/200000, Loss: 152.6238\n",
      "Iteration 52401/200000, Loss: 221.7171\n",
      "Iteration 52501/200000, Loss: 251.7400\n",
      "Iteration 52601/200000, Loss: 74.8074\n",
      "Iteration 52701/200000, Loss: 321.6916\n",
      "Iteration 52801/200000, Loss: 143.5072\n",
      "Iteration 52901/200000, Loss: 121.8363\n",
      "Iteration 53001/200000, Loss: 308.3761\n",
      "Iteration 53101/200000, Loss: 72.9147\n",
      "Iteration 53201/200000, Loss: 107.1822\n",
      "Iteration 53301/200000, Loss: 57.0042\n",
      "Iteration 53401/200000, Loss: 212.5813\n",
      "Iteration 53501/200000, Loss: 177.5678\n",
      "Iteration 53601/200000, Loss: 204.5033\n",
      "Iteration 53701/200000, Loss: 85.8755\n",
      "Iteration 53801/200000, Loss: 44.2028\n",
      "Iteration 53901/200000, Loss: 44.4323\n",
      "Iteration 54001/200000, Loss: 167.0275\n",
      "Iteration 54101/200000, Loss: 104.5086\n",
      "Iteration 54201/200000, Loss: 176.3290\n",
      "Iteration 54301/200000, Loss: 124.4122\n",
      "Iteration 54401/200000, Loss: 58.4170\n",
      "Iteration 54501/200000, Loss: 193.8368\n",
      "Iteration 54601/200000, Loss: 368.8808\n",
      "Iteration 54701/200000, Loss: 158.5896\n",
      "Iteration 54801/200000, Loss: 137.8472\n",
      "Iteration 54901/200000, Loss: 124.5740\n",
      "Iteration 55001/200000, Loss: 52.7525\n",
      "Iteration 55101/200000, Loss: 26.2699\n",
      "Iteration 55201/200000, Loss: 101.0986\n",
      "Iteration 55301/200000, Loss: 47.8316\n",
      "Iteration 55401/200000, Loss: 205.1805\n",
      "Iteration 55501/200000, Loss: 111.0503\n",
      "Iteration 55601/200000, Loss: 254.6742\n",
      "Iteration 55701/200000, Loss: 76.3852\n",
      "Iteration 55801/200000, Loss: 252.1418\n",
      "Iteration 55901/200000, Loss: 326.7992\n",
      "Iteration 56001/200000, Loss: 137.8692\n",
      "Iteration 56101/200000, Loss: 102.2419\n",
      "Iteration 56201/200000, Loss: 155.7830\n",
      "Iteration 56301/200000, Loss: 129.7424\n",
      "Iteration 56401/200000, Loss: 23.6048\n",
      "Iteration 56501/200000, Loss: 41.1591\n",
      "Iteration 56601/200000, Loss: 120.3915\n",
      "Iteration 56701/200000, Loss: 31.7418\n",
      "Iteration 56801/200000, Loss: 288.6469\n",
      "Iteration 56901/200000, Loss: 41.4604\n",
      "Iteration 57001/200000, Loss: 251.2500\n",
      "Iteration 57101/200000, Loss: 163.9196\n",
      "Iteration 57201/200000, Loss: 236.0025\n",
      "Iteration 57301/200000, Loss: 109.2483\n",
      "Iteration 57401/200000, Loss: 37.5756\n",
      "Iteration 57501/200000, Loss: 243.0478\n",
      "Iteration 57601/200000, Loss: 189.0053\n",
      "Iteration 57701/200000, Loss: 123.5765\n",
      "Iteration 57801/200000, Loss: 293.9878\n",
      "Iteration 57901/200000, Loss: 135.4360\n",
      "Iteration 58001/200000, Loss: 119.3412\n",
      "Iteration 58101/200000, Loss: 192.9262\n",
      "Iteration 58201/200000, Loss: 202.7232\n",
      "Iteration 58301/200000, Loss: 303.6993\n",
      "Iteration 58401/200000, Loss: 192.4096\n",
      "Iteration 58501/200000, Loss: 98.5803\n",
      "Iteration 58601/200000, Loss: 245.4390\n",
      "Iteration 58701/200000, Loss: 74.8291\n",
      "Iteration 58801/200000, Loss: 135.3801\n",
      "Iteration 58901/200000, Loss: 197.4765\n",
      "Iteration 59001/200000, Loss: 242.7881\n",
      "Iteration 59101/200000, Loss: 335.2104\n",
      "Iteration 59201/200000, Loss: 99.8643\n",
      "Iteration 59301/200000, Loss: 219.3918\n",
      "Iteration 59401/200000, Loss: 244.8623\n",
      "Iteration 59501/200000, Loss: 244.5465\n",
      "Iteration 59601/200000, Loss: 113.7729\n",
      "Iteration 59701/200000, Loss: 402.7264\n",
      "Iteration 59801/200000, Loss: 208.5934\n",
      "Iteration 59901/200000, Loss: 127.0483\n",
      "Iteration 60001/200000, Loss: 220.9464\n",
      "Iteration 60101/200000, Loss: 202.6301\n",
      "Iteration 60201/200000, Loss: 274.3063\n",
      "Iteration 60301/200000, Loss: 55.0730\n",
      "Iteration 60401/200000, Loss: 192.2784\n",
      "Iteration 60501/200000, Loss: 48.7482\n",
      "Iteration 60601/200000, Loss: 168.1781\n",
      "Iteration 60701/200000, Loss: 41.2923\n",
      "Iteration 60801/200000, Loss: 149.8950\n",
      "Iteration 60901/200000, Loss: 360.7718\n",
      "Iteration 61001/200000, Loss: 285.1439\n",
      "Iteration 61101/200000, Loss: 387.9062\n",
      "Iteration 61201/200000, Loss: 128.5140\n",
      "Iteration 61301/200000, Loss: 84.1998\n",
      "Iteration 61401/200000, Loss: 281.5052\n",
      "Iteration 61501/200000, Loss: 406.1035\n",
      "Iteration 61601/200000, Loss: 145.5018\n",
      "Iteration 61701/200000, Loss: 77.8470\n",
      "Iteration 61801/200000, Loss: 130.3986\n",
      "Iteration 61901/200000, Loss: 189.0421\n",
      "Iteration 62001/200000, Loss: 107.8988\n",
      "Iteration 62101/200000, Loss: 69.7656\n",
      "Iteration 62201/200000, Loss: 47.9637\n",
      "Iteration 62301/200000, Loss: 39.4303\n",
      "Iteration 62401/200000, Loss: 109.1068\n",
      "Iteration 62501/200000, Loss: 205.4460\n",
      "Iteration 62601/200000, Loss: 370.6122\n",
      "Iteration 62701/200000, Loss: 226.6286\n",
      "Iteration 62801/200000, Loss: 40.7740\n",
      "Iteration 62901/200000, Loss: 71.3773\n",
      "Iteration 63001/200000, Loss: 99.6684\n",
      "Iteration 63101/200000, Loss: 232.1338\n",
      "Iteration 63201/200000, Loss: 54.9205\n",
      "Iteration 63301/200000, Loss: 234.3517\n",
      "Iteration 63401/200000, Loss: 377.6608\n",
      "Iteration 63501/200000, Loss: 129.7598\n",
      "Iteration 63601/200000, Loss: 240.5965\n",
      "Iteration 63701/200000, Loss: 84.4987\n",
      "Iteration 63801/200000, Loss: 193.8391\n",
      "Iteration 63901/200000, Loss: 26.4359\n",
      "Iteration 64001/200000, Loss: 322.0681\n",
      "Iteration 64101/200000, Loss: 69.7311\n",
      "Iteration 64201/200000, Loss: 106.4952\n",
      "Iteration 64301/200000, Loss: 323.8604\n",
      "Iteration 64401/200000, Loss: 138.7826\n",
      "Iteration 64501/200000, Loss: 334.2207\n",
      "Iteration 64601/200000, Loss: 282.3647\n",
      "Iteration 64701/200000, Loss: 70.6545\n",
      "Iteration 64801/200000, Loss: 230.1419\n",
      "Iteration 64901/200000, Loss: 281.0443\n",
      "Iteration 65001/200000, Loss: 110.2677\n",
      "Iteration 65101/200000, Loss: 206.0937\n",
      "Iteration 65201/200000, Loss: 59.0437\n",
      "Iteration 65301/200000, Loss: 233.9710\n",
      "Iteration 65401/200000, Loss: 48.3869\n",
      "Iteration 65501/200000, Loss: 67.2282\n",
      "Iteration 65601/200000, Loss: 75.7781\n",
      "Iteration 65701/200000, Loss: 311.1824\n",
      "Iteration 65801/200000, Loss: 188.4843\n",
      "Iteration 65901/200000, Loss: 119.1645\n",
      "Iteration 66001/200000, Loss: 278.6306\n",
      "Iteration 66101/200000, Loss: 210.8769\n",
      "Iteration 66201/200000, Loss: 40.6277\n",
      "Iteration 66301/200000, Loss: 147.4116\n",
      "Iteration 66401/200000, Loss: 56.3147\n",
      "Iteration 66501/200000, Loss: 343.9992\n",
      "Iteration 66601/200000, Loss: 55.5540\n",
      "Iteration 66701/200000, Loss: 112.4455\n",
      "Iteration 66801/200000, Loss: 224.3468\n",
      "Iteration 66901/200000, Loss: 129.6126\n",
      "Iteration 67001/200000, Loss: 188.8081\n",
      "Iteration 67101/200000, Loss: 34.3491\n",
      "Iteration 67201/200000, Loss: 58.8993\n",
      "Iteration 67301/200000, Loss: 79.5902\n",
      "Iteration 67401/200000, Loss: 140.8559\n",
      "Iteration 67501/200000, Loss: 230.7439\n",
      "Iteration 67601/200000, Loss: 220.8323\n",
      "Iteration 67701/200000, Loss: 215.0767\n",
      "Iteration 67801/200000, Loss: 82.1450\n",
      "Iteration 67901/200000, Loss: 57.0452\n",
      "Iteration 68001/200000, Loss: 36.3419\n",
      "Iteration 68101/200000, Loss: 186.7925\n",
      "Iteration 68201/200000, Loss: 237.9405\n",
      "Iteration 68301/200000, Loss: 278.3105\n",
      "Iteration 68401/200000, Loss: 80.2840\n",
      "Iteration 68501/200000, Loss: 183.4948\n",
      "Iteration 68601/200000, Loss: 388.1530\n",
      "Iteration 68701/200000, Loss: 40.9214\n",
      "Iteration 68801/200000, Loss: 111.5439\n",
      "Iteration 68901/200000, Loss: 279.5021\n",
      "Iteration 69001/200000, Loss: 204.5179\n",
      "Iteration 69101/200000, Loss: 130.7944\n",
      "Iteration 69201/200000, Loss: 59.5406\n",
      "Iteration 69301/200000, Loss: 44.0847\n",
      "Iteration 69401/200000, Loss: 124.4254\n",
      "Iteration 69501/200000, Loss: 262.3271\n",
      "Iteration 69601/200000, Loss: 217.6962\n",
      "Iteration 69701/200000, Loss: 290.5355\n",
      "Iteration 69801/200000, Loss: 126.2119\n",
      "Iteration 69901/200000, Loss: 336.0813\n",
      "Iteration 70001/200000, Loss: 208.5381\n",
      "Iteration 70101/200000, Loss: 155.8867\n",
      "Iteration 70201/200000, Loss: 365.9773\n",
      "Iteration 70301/200000, Loss: 103.0607\n",
      "Iteration 70401/200000, Loss: 157.1540\n",
      "Iteration 70501/200000, Loss: 212.5288\n",
      "Iteration 70601/200000, Loss: 136.0713\n",
      "Iteration 70701/200000, Loss: 46.2051\n",
      "Iteration 70801/200000, Loss: 229.0354\n",
      "Iteration 70901/200000, Loss: 93.0348\n",
      "Iteration 71001/200000, Loss: 60.2086\n",
      "Iteration 71101/200000, Loss: 453.0941\n",
      "Iteration 71201/200000, Loss: 280.3921\n",
      "Iteration 71301/200000, Loss: 72.3736\n",
      "Iteration 71401/200000, Loss: 319.4459\n",
      "Iteration 71501/200000, Loss: 190.6261\n",
      "Iteration 71601/200000, Loss: 279.0213\n",
      "Iteration 71701/200000, Loss: 282.6100\n",
      "Iteration 71801/200000, Loss: 159.1606\n",
      "Iteration 71901/200000, Loss: 297.6762\n",
      "Iteration 72001/200000, Loss: 338.6559\n",
      "Iteration 72101/200000, Loss: 186.5023\n",
      "Iteration 72201/200000, Loss: 160.6733\n",
      "Iteration 72301/200000, Loss: 158.2305\n",
      "Iteration 72401/200000, Loss: 49.4583\n",
      "Iteration 72501/200000, Loss: 114.5987\n",
      "Iteration 72601/200000, Loss: 161.9864\n",
      "Iteration 72701/200000, Loss: 43.4363\n",
      "Iteration 72801/200000, Loss: 107.1041\n",
      "Iteration 72901/200000, Loss: 48.9434\n",
      "Iteration 73001/200000, Loss: 165.0665\n",
      "Iteration 73101/200000, Loss: 58.3184\n",
      "Iteration 73201/200000, Loss: 183.9461\n",
      "Iteration 73301/200000, Loss: 172.0374\n",
      "Iteration 73401/200000, Loss: 202.1742\n",
      "Iteration 73501/200000, Loss: 243.7902\n",
      "Iteration 73601/200000, Loss: 255.9728\n",
      "Iteration 73701/200000, Loss: 295.5079\n",
      "Iteration 73801/200000, Loss: 155.1001\n",
      "Iteration 73901/200000, Loss: 165.9830\n",
      "Iteration 74001/200000, Loss: 152.1595\n",
      "Iteration 74101/200000, Loss: 49.9889\n",
      "Iteration 74201/200000, Loss: 81.2476\n",
      "Iteration 74301/200000, Loss: 54.9669\n",
      "Iteration 74401/200000, Loss: 92.1316\n",
      "Iteration 74501/200000, Loss: 167.8701\n",
      "Iteration 74601/200000, Loss: 154.6210\n",
      "Iteration 74701/200000, Loss: 128.3166\n",
      "Iteration 74801/200000, Loss: 368.0133\n",
      "Iteration 74901/200000, Loss: 273.7911\n",
      "Iteration 75001/200000, Loss: 257.2931\n",
      "Iteration 75101/200000, Loss: 41.2502\n",
      "Iteration 75201/200000, Loss: 186.8887\n",
      "Iteration 75301/200000, Loss: 249.6275\n",
      "Iteration 75401/200000, Loss: 326.0756\n",
      "Iteration 75501/200000, Loss: 84.9311\n",
      "Iteration 75601/200000, Loss: 570.9419\n",
      "Iteration 75701/200000, Loss: 118.2239\n",
      "Iteration 75801/200000, Loss: 171.3992\n",
      "Iteration 75901/200000, Loss: 238.5021\n",
      "Iteration 76001/200000, Loss: 89.8023\n",
      "Iteration 76101/200000, Loss: 148.3627\n",
      "Iteration 76201/200000, Loss: 253.4239\n",
      "Iteration 76301/200000, Loss: 89.0848\n",
      "Iteration 76401/200000, Loss: 194.6541\n",
      "Iteration 76501/200000, Loss: 344.1898\n",
      "Iteration 76601/200000, Loss: 37.3695\n",
      "Iteration 76701/200000, Loss: 283.9495\n",
      "Iteration 76801/200000, Loss: 76.1418\n",
      "Iteration 76901/200000, Loss: 201.9456\n",
      "Iteration 77001/200000, Loss: 47.7548\n",
      "Iteration 77101/200000, Loss: 220.7927\n",
      "Iteration 77201/200000, Loss: 129.3427\n",
      "Iteration 77301/200000, Loss: 112.8867\n",
      "Iteration 77401/200000, Loss: 209.7716\n",
      "Iteration 77501/200000, Loss: 381.8022\n",
      "Iteration 77601/200000, Loss: 139.3051\n",
      "Iteration 77701/200000, Loss: 402.1895\n",
      "Iteration 77801/200000, Loss: 266.9744\n",
      "Iteration 77901/200000, Loss: 132.2892\n",
      "Iteration 78001/200000, Loss: 62.9077\n",
      "Iteration 78101/200000, Loss: 275.7254\n",
      "Iteration 78201/200000, Loss: 162.2196\n",
      "Iteration 78301/200000, Loss: 31.7979\n",
      "Iteration 78401/200000, Loss: 156.0231\n",
      "Iteration 78501/200000, Loss: 296.9757\n",
      "Iteration 78601/200000, Loss: 189.7568\n",
      "Iteration 78701/200000, Loss: 182.8308\n",
      "Iteration 78801/200000, Loss: 156.2792\n",
      "Iteration 78901/200000, Loss: 182.8371\n",
      "Iteration 79001/200000, Loss: 204.6593\n",
      "Iteration 79101/200000, Loss: 292.9326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 58\u001B[0m\n\u001B[1;32m     56\u001B[0m xb \u001B[38;5;241m=\u001B[39m get_batch(train_data\u001B[38;5;241m=\u001B[39mtrain_data, val_data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     57\u001B[0m xb_mod \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mclone(xb\u001B[38;5;241m.\u001B[39mdetach())\n\u001B[0;32m---> 58\u001B[0m X, loss, loss_dict \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxb_mod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffling\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# print('during training orig', model.blocks[0].mha.heads[0].dag_orig)\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# print('during training mod', model.blocks[0].mha.heads[0].dag_mod)\u001B[39;00m\n\u001B[1;32m     61\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad(set_to_none\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/GitHub/Causal_Transformer/CaT/test_model.py:386\u001B[0m, in \u001B[0;36mCaT.forward\u001B[0;34m(self, X, mask, targets, shuffling)\u001B[0m\n\u001B[1;32m    384\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_0\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 386\u001B[0m     Y \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    388\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(Y)\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m targets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/GitHub/Causal_Transformer/CaT/test_model.py:236\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, X, Y)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03mForward pass for the Block module.\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    235\u001B[0m mha_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmha(X, Y)\n\u001B[0;32m--> 236\u001B[0m ff_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mff\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmha_out\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m mha_out \u001B[38;5;241m=\u001B[39m mha_out \u001B[38;5;241m+\u001B[39m ff_out\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/GitHub/Causal_Transformer/CaT/test_model.py:195\u001B[0m, in \u001B[0;36mFF.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    192\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;124;03m    Forward pass for the FF module.\u001B[39;00m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 195\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/dropout.py:59\u001B[0m, in \u001B[0;36mDropout.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/functional.py:1295\u001B[0m, in \u001B[0;36mdropout\u001B[0;34m(input, p, training, inplace)\u001B[0m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mdropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m(\u001B[38;5;28minput\u001B[39m, p, training)\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/_VF.py:27\u001B[0m, in \u001B[0;36mVFModule.__getattr__\u001B[0;34m(self, attr)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr):\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:54:08.179384Z",
     "start_time": "2024-09-28T11:54:08.179298Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7d8b5a961e9e203e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88548901ebb66ac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c2ac665d003b2bff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d2d39fa2bcbc3352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "56115b49c435ee19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
