{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84f0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import CaT\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_full_ordering, reorder_dag\n",
    "\n",
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 100000\n",
    "batch_size = 100\n",
    "max_iters = 100000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.3\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 5e-3\n",
    "ff_n_embed = 6\n",
    "num_heads = 2\n",
    "n_layers = 2\n",
    "head_size = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37367d6c",
   "metadata": {},
   "source": [
    "We create data from a 'true' DAG (which is DAGnx3 below) but also provide two other DAGs which are incorrect in different ways. DAGnx1 is fully exogenous, and DAGnx2 has one missing edge and another directed edge reversed.\n",
    "\n",
    "We imagine we are interested in the 'total' effect of X -> Y.  As these simulations are linear, this effect can be calculated easily by hand.\n",
    "\n",
    "In the case where X1 -> Y and X1 -> X2 -> Y, the total effect is the sum of the effect from both paths. e.g. if X1 -> Y has a coefficient of  0.8,  X1 -> X2 of 0.8, and X2 -> Y of 0.7, then the total effect is 0.8 + (0.8 x 0.7) = 1.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de6b09c-fb7a-4a95-ab23-954550e69e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X1', 'X2', 'X3', 'Y'] ATE of X1 on Y: [1.12]\n",
      "(100000, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(N, d=1):\n",
    "    DAGnx1 = nx.DiGraph()\n",
    "    DAGnx2 = nx.DiGraph()\n",
    "    DAGnx3 = nx.DiGraph()  # correct graph\n",
    "    \n",
    "    Ux1 = np.random.randn(N,d)\n",
    "    X1 =  Ux1\n",
    "\n",
    "    Ux2 = np.random.randn(N,d)\n",
    "    X2 =  0.8 * X1 + Ux2\n",
    "    \n",
    "    X20 = Ux2\n",
    "    X21 = 0.8 + Ux2\n",
    "\n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y =  0.8 * X1 + 0.4 * X2 + Uy\n",
    "    \n",
    "    Ux3 = np.random.randn(N,d)\n",
    "    X3 = 0.7 * Y + 0.6 * X1 + Ux3\n",
    "\n",
    "    Y0 = 0.4 * X20 + Uy \n",
    "    Y1 = 0.8 + 0.4 * X21 + Uy\n",
    "\n",
    "    all_data_dict = {'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X1': 'cont', 'X2': 'cont', 'X3': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx1.add_edges_from([('X1', 'Y'), ('X2', 'Y'), ('X3', 'Y')])\n",
    "    DAGnx1 = reorder_dag(dag=DAGnx1)  # topologically sorted dag\n",
    "    var_names1 = list(DAGnx1.nodes())  # topologically ordered list of variables\n",
    "    all_data1 = np.stack([all_data_dict[key] for key in var_names1], axis=1)\n",
    "    causal_ordering1 = get_full_ordering(DAGnx1)\n",
    "    ex_dag_stuff = (all_data1, DAGnx1, var_names1, causal_ordering1, var_types)\n",
    "    \n",
    "    DAGnx2.add_edges_from([('X1', 'Y'), ('X1', 'X2'), ('X2', 'Y'), ('X3', 'Y')])\n",
    "    DAGnx2 = reorder_dag(dag=DAGnx2)  # topologically sorted dag\n",
    "    var_names2 = list(DAGnx2.nodes())  # topologically ordered list of variables\n",
    "    all_data2 = np.stack([all_data_dict[key] for key in var_names2], axis=1)\n",
    "    causal_ordering2 = get_full_ordering(DAGnx2)\n",
    "    mediated_dag_stuff = (all_data2, DAGnx2, var_names2, causal_ordering2, var_types)\n",
    "    \n",
    "    DAGnx3.add_edges_from([('X1', 'Y'), ('X1', 'X2'), ('X1', 'X3'), ('X2', 'Y'), ('Y', 'X3')])\n",
    "    DAGnx3 = reorder_dag(dag=DAGnx3)  # topologically sorted dag\n",
    "    var_names3 = list(DAGnx3.nodes())  # topologically ordered list of variables\n",
    "    all_data3 = np.stack([all_data_dict[key] for key in var_names3], axis=1)\n",
    "    causal_ordering3 = get_full_ordering(DAGnx3)\n",
    "    correct_dag_stuff =  (all_data3, DAGnx3, var_names3, causal_ordering3, var_types)\n",
    "\n",
    "    return ex_dag_stuff, mediated_dag_stuff, correct_dag_stuff, Y0, Y1\n",
    "\n",
    "d=1\n",
    "_, _, _, Y0, Y1 = generate_data(N=1000000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  \n",
    "ex_dag_stuff, mediated_dag_stuff, correct_dag_stuff, Y0, Y1 = generate_data(N=sample_size, d=d)\n",
    "\n",
    "all_data1, DAGnx1, var_names1, causal_ordering1, var_types1 = ex_dag_stuff\n",
    "all_data2, DAGnx2, var_names2, causal_ordering2, var_types2 = mediated_dag_stuff\n",
    "all_data3, DAGnx3, var_names3, causal_ordering3, var_types3 = correct_dag_stuff\n",
    "\n",
    "print(var_names1, 'ATE of X1 on Y:', ATE)\n",
    "print(all_data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277cb8c-d1a1-44dc-a567-f0a7d2803887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7865de21-8b33-4b80-b3df-41ddedac0119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect DAG 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect DAG 2:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct DAG:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Incorrect DAG 1:')\n",
    "nx.draw_planar(\n",
    "    DAGnx1,\n",
    "    with_labels=True,\n",
    "    node_size=1000,\n",
    "    node_color=\"#ffff8f\",\n",
    "    width=0.5,\n",
    "    font_size=10,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print('Incorrect DAG 2:')\n",
    "nx.draw_planar(\n",
    "    DAGnx2,\n",
    "    with_labels=True,\n",
    "    node_size=1000,\n",
    "    node_color=\"#ffff8f\",\n",
    "    width=0.5,\n",
    "    font_size=10,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print('Correct DAG:')\n",
    "nx.draw_planar(\n",
    "    DAGnx3,\n",
    "    with_labels=True,\n",
    "    node_size=1000,\n",
    "    node_color=\"#ffff8f\",\n",
    "    width=0.5,\n",
    "    font_size=10,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6dea20",
   "metadata": {},
   "source": [
    "We run the model with the same data each time (although the column ordering of the data is different in each case, according to the assumed underlying DAG), and provide fit statistics as well as estimated causal links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541753a",
   "metadata": {},
   "source": [
    "### Incorrect DAG 1 (fully exogenous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab383e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 100000: train_loss 2.4101, val loss 2.3839\n",
      "step 100 of 100000: train_loss 0.7666, val loss 0.7437\n",
      "step 200 of 100000: train_loss 0.7163, val loss 0.7397\n",
      "step 300 of 100000: train_loss 0.6994, val loss 0.7230\n",
      "step 400 of 100000: train_loss 0.6929, val loss 0.7008\n",
      "step 500 of 100000: train_loss 0.7558, val loss 0.7669\n",
      "step 600 of 100000: train_loss 0.6983, val loss 0.7122\n",
      "step 700 of 100000: train_loss 0.6966, val loss 0.6932\n",
      "step 800 of 100000: train_loss 0.7059, val loss 0.7303\n",
      "step 900 of 100000: train_loss 0.6915, val loss 0.7133\n",
      "step 1000 of 100000: train_loss 0.7072, val loss 0.7154\n",
      "step 1100 of 100000: train_loss 0.7106, val loss 0.7222\n",
      "step 1200 of 100000: train_loss 0.7069, val loss 0.7036\n",
      "step 1300 of 100000: train_loss 0.6959, val loss 0.6978\n",
      "step 1400 of 100000: train_loss 0.7033, val loss 0.6932\n",
      "step 1500 of 100000: train_loss 0.6986, val loss 0.7096\n",
      "step 1600 of 100000: train_loss 0.7112, val loss 0.7317\n",
      "step 1700 of 100000: train_loss 0.6940, val loss 0.6906\n",
      "step 1800 of 100000: train_loss 0.7221, val loss 0.7394\n",
      "step 1900 of 100000: train_loss 0.6877, val loss 0.7136\n",
      "step 2000 of 100000: train_loss 0.7112, val loss 0.7213\n",
      "step 2100 of 100000: train_loss 0.6953, val loss 0.7192\n",
      "step 2200 of 100000: train_loss 0.6991, val loss 0.7099\n",
      "step 2300 of 100000: train_loss 0.6778, val loss 0.6894\n",
      "step 2400 of 100000: train_loss 0.6930, val loss 0.7221\n",
      "step 2500 of 100000: train_loss 0.7152, val loss 0.7133\n",
      "step 2600 of 100000: train_loss 0.6981, val loss 0.6866\n",
      "step 2700 of 100000: train_loss 0.6943, val loss 0.6838\n",
      "step 2800 of 100000: train_loss 0.7127, val loss 0.7144\n",
      "step 2900 of 100000: train_loss 0.6990, val loss 0.6862\n",
      "step 3000 of 100000: train_loss 0.7195, val loss 0.7171\n",
      "step 3100 of 100000: train_loss 0.6950, val loss 0.6855\n",
      "step 3200 of 100000: train_loss 0.6665, val loss 0.7011\n",
      "step 3300 of 100000: train_loss 0.7179, val loss 0.7170\n",
      "step 3400 of 100000: train_loss 0.6805, val loss 0.7115\n",
      "step 3500 of 100000: train_loss 0.6895, val loss 0.7055\n",
      "step 3600 of 100000: train_loss 0.6974, val loss 0.7125\n",
      "step 3700 of 100000: train_loss 0.7092, val loss 0.7086\n",
      "step 3800 of 100000: train_loss 0.7160, val loss 0.7219\n",
      "step 3900 of 100000: train_loss 0.6931, val loss 0.6999\n",
      "step 4000 of 100000: train_loss 0.7037, val loss 0.7002\n",
      "step 4100 of 100000: train_loss 0.6772, val loss 0.6836\n",
      "step 4200 of 100000: train_loss 0.7124, val loss 0.7250\n",
      "step 4300 of 100000: train_loss 0.6782, val loss 0.6862\n",
      "step 4400 of 100000: train_loss 0.6979, val loss 0.7095\n",
      "step 4500 of 100000: train_loss 0.6978, val loss 0.7148\n",
      "step 4600 of 100000: train_loss 0.7030, val loss 0.6957\n",
      "step 4700 of 100000: train_loss 0.6753, val loss 0.6899\n",
      "step 4800 of 100000: train_loss 0.6915, val loss 0.6880\n",
      "step 4900 of 100000: train_loss 0.6906, val loss 0.6877\n",
      "step 5000 of 100000: train_loss 0.7084, val loss 0.7021\n",
      "step 5100 of 100000: train_loss 0.6921, val loss 0.7045\n",
      "step 5200 of 100000: train_loss 0.6930, val loss 0.6793\n",
      "step 5300 of 100000: train_loss 0.6874, val loss 0.6734\n",
      "step 5400 of 100000: train_loss 0.7397, val loss 0.7453\n",
      "step 5500 of 100000: train_loss 0.6770, val loss 0.7110\n",
      "step 5600 of 100000: train_loss 0.7043, val loss 0.7015\n",
      "step 5700 of 100000: train_loss 0.7097, val loss 0.7120\n",
      "step 5800 of 100000: train_loss 0.7360, val loss 0.7518\n",
      "step 5900 of 100000: train_loss 0.7127, val loss 0.7316\n",
      "step 6000 of 100000: train_loss 0.6968, val loss 0.6895\n",
      "step 6100 of 100000: train_loss 0.6656, val loss 0.6976\n",
      "step 6200 of 100000: train_loss 0.6861, val loss 0.6876\n",
      "step 6300 of 100000: train_loss 0.6799, val loss 0.6967\n",
      "step 6400 of 100000: train_loss 0.7005, val loss 0.6852\n",
      "step 6500 of 100000: train_loss 0.7241, val loss 0.7022\n",
      "step 6600 of 100000: train_loss 0.6819, val loss 0.6952\n",
      "step 6700 of 100000: train_loss 0.6665, val loss 0.7101\n",
      "step 6800 of 100000: train_loss 0.6930, val loss 0.6978\n",
      "step 6900 of 100000: train_loss 0.6974, val loss 0.7017\n",
      "step 7000 of 100000: train_loss 0.7045, val loss 0.7080\n",
      "step 7100 of 100000: train_loss 0.6937, val loss 0.6980\n",
      "step 7200 of 100000: train_loss 0.7742, val loss 0.7621\n",
      "step 7300 of 100000: train_loss 0.7059, val loss 0.7103\n",
      "step 7400 of 100000: train_loss 0.6895, val loss 0.6857\n",
      "step 7500 of 100000: train_loss 0.6834, val loss 0.6965\n",
      "step 7600 of 100000: train_loss 0.7065, val loss 0.6913\n",
      "step 7700 of 100000: train_loss 0.6947, val loss 0.6984\n",
      "step 7800 of 100000: train_loss 0.6990, val loss 0.6876\n",
      "step 7900 of 100000: train_loss 0.7110, val loss 0.7247\n",
      "step 8000 of 100000: train_loss 0.6931, val loss 0.7020\n",
      "step 8100 of 100000: train_loss 0.6995, val loss 0.7071\n",
      "step 8200 of 100000: train_loss 0.6826, val loss 0.7019\n",
      "step 8300 of 100000: train_loss 0.7040, val loss 0.7138\n",
      "step 8400 of 100000: train_loss 0.6769, val loss 0.6892\n",
      "step 8500 of 100000: train_loss 0.7021, val loss 0.7118\n",
      "step 8600 of 100000: train_loss 0.6918, val loss 0.6954\n",
      "step 8700 of 100000: train_loss 0.7041, val loss 0.7009\n",
      "step 8800 of 100000: train_loss 0.6824, val loss 0.7020\n",
      "step 8900 of 100000: train_loss 0.6921, val loss 0.7149\n",
      "step 9000 of 100000: train_loss 0.6807, val loss 0.7035\n",
      "step 9100 of 100000: train_loss 0.7417, val loss 0.7409\n",
      "step 9200 of 100000: train_loss 0.6917, val loss 0.6907\n",
      "step 9300 of 100000: train_loss 0.7001, val loss 0.6971\n",
      "step 9400 of 100000: train_loss 0.7033, val loss 0.6916\n",
      "step 9500 of 100000: train_loss 0.6937, val loss 0.6951\n",
      "step 9600 of 100000: train_loss 0.6939, val loss 0.6905\n",
      "step 9700 of 100000: train_loss 0.6848, val loss 0.7009\n",
      "step 9800 of 100000: train_loss 0.6842, val loss 0.6885\n",
      "step 9900 of 100000: train_loss 0.6936, val loss 0.6902\n",
      "step 10000 of 100000: train_loss 0.7006, val loss 0.6900\n",
      "step 10100 of 100000: train_loss 0.6953, val loss 0.6911\n",
      "step 10200 of 100000: train_loss 0.7069, val loss 0.7027\n",
      "step 10300 of 100000: train_loss 0.6972, val loss 0.6878\n",
      "step 10400 of 100000: train_loss 0.6826, val loss 0.6723\n",
      "step 10500 of 100000: train_loss 0.6888, val loss 0.7031\n",
      "step 10600 of 100000: train_loss 0.6845, val loss 0.6990\n",
      "step 10700 of 100000: train_loss 0.6977, val loss 0.7152\n",
      "step 10800 of 100000: train_loss 0.6976, val loss 0.6910\n",
      "step 10900 of 100000: train_loss 0.7006, val loss 0.7086\n",
      "step 11000 of 100000: train_loss 0.6911, val loss 0.6972\n",
      "step 11100 of 100000: train_loss 0.6885, val loss 0.6760\n",
      "step 11200 of 100000: train_loss 0.6920, val loss 0.6934\n",
      "step 11300 of 100000: train_loss 0.7038, val loss 0.7150\n",
      "step 11400 of 100000: train_loss 0.6951, val loss 0.6960\n",
      "step 11500 of 100000: train_loss 0.7006, val loss 0.7012\n",
      "step 11600 of 100000: train_loss 0.7145, val loss 0.7261\n",
      "step 11700 of 100000: train_loss 0.6863, val loss 0.7025\n",
      "step 11800 of 100000: train_loss 0.7100, val loss 0.6998\n",
      "step 11900 of 100000: train_loss 0.7271, val loss 0.7284\n",
      "step 12000 of 100000: train_loss 0.7075, val loss 0.7237\n",
      "step 12100 of 100000: train_loss 0.6951, val loss 0.6946\n",
      "step 12200 of 100000: train_loss 0.6707, val loss 0.7061\n",
      "step 12300 of 100000: train_loss 0.6763, val loss 0.7071\n",
      "step 12400 of 100000: train_loss 0.6834, val loss 0.6979\n",
      "step 12500 of 100000: train_loss 0.6722, val loss 0.6836\n",
      "step 12600 of 100000: train_loss 0.7121, val loss 0.7013\n",
      "step 12700 of 100000: train_loss 0.6970, val loss 0.7020\n",
      "step 12800 of 100000: train_loss 0.6962, val loss 0.7177\n",
      "step 12900 of 100000: train_loss 0.6824, val loss 0.6935\n",
      "step 13000 of 100000: train_loss 0.6952, val loss 0.6949\n",
      "step 13100 of 100000: train_loss 0.7089, val loss 0.7305\n",
      "step 13200 of 100000: train_loss 0.6834, val loss 0.7093\n",
      "step 13300 of 100000: train_loss 0.6891, val loss 0.6903\n",
      "step 13400 of 100000: train_loss 0.6795, val loss 0.6635\n",
      "step 13500 of 100000: train_loss 0.6945, val loss 0.6788\n",
      "step 13600 of 100000: train_loss 0.6907, val loss 0.6847\n",
      "step 13700 of 100000: train_loss 0.7065, val loss 0.7158\n",
      "step 13800 of 100000: train_loss 0.7006, val loss 0.6938\n",
      "step 13900 of 100000: train_loss 0.6769, val loss 0.7134\n",
      "step 14000 of 100000: train_loss 0.6837, val loss 0.6837\n",
      "step 14100 of 100000: train_loss 0.6634, val loss 0.7232\n",
      "step 14200 of 100000: train_loss 0.6850, val loss 0.7029\n",
      "step 14300 of 100000: train_loss 0.7080, val loss 0.6993\n",
      "step 14400 of 100000: train_loss 0.6908, val loss 0.7002\n",
      "step 14500 of 100000: train_loss 0.6883, val loss 0.6916\n",
      "step 14600 of 100000: train_loss 0.6850, val loss 0.7039\n",
      "step 14700 of 100000: train_loss 0.7146, val loss 0.7280\n",
      "step 14800 of 100000: train_loss 0.6761, val loss 0.6862\n",
      "step 14900 of 100000: train_loss 0.7132, val loss 0.6887\n",
      "step 15000 of 100000: train_loss 0.6816, val loss 0.6947\n",
      "step 15100 of 100000: train_loss 0.6782, val loss 0.6997\n",
      "step 15200 of 100000: train_loss 0.7032, val loss 0.6944\n",
      "step 15300 of 100000: train_loss 0.6948, val loss 0.6923\n",
      "step 15400 of 100000: train_loss 0.6998, val loss 0.6844\n",
      "step 15500 of 100000: train_loss 0.6774, val loss 0.7031\n",
      "step 15600 of 100000: train_loss 0.6996, val loss 0.7230\n",
      "step 15700 of 100000: train_loss 0.7030, val loss 0.6870\n",
      "step 15800 of 100000: train_loss 0.6949, val loss 0.6995\n",
      "step 15900 of 100000: train_loss 0.6936, val loss 0.6983\n",
      "step 16000 of 100000: train_loss 0.6828, val loss 0.6998\n",
      "step 16100 of 100000: train_loss 0.6808, val loss 0.6916\n",
      "step 16200 of 100000: train_loss 0.6954, val loss 0.7024\n",
      "step 16300 of 100000: train_loss 0.7363, val loss 0.7352\n",
      "step 16400 of 100000: train_loss 0.6755, val loss 0.6986\n",
      "step 16500 of 100000: train_loss 0.6967, val loss 0.7098\n",
      "step 16600 of 100000: train_loss 0.6925, val loss 0.6978\n",
      "step 16700 of 100000: train_loss 0.6849, val loss 0.6969\n",
      "step 16800 of 100000: train_loss 0.6964, val loss 0.6946\n",
      "step 16900 of 100000: train_loss 0.7026, val loss 0.6911\n",
      "step 17000 of 100000: train_loss 0.6965, val loss 0.6855\n",
      "step 17100 of 100000: train_loss 0.7010, val loss 0.6925\n",
      "step 17200 of 100000: train_loss 0.6939, val loss 0.7144\n",
      "step 17300 of 100000: train_loss 0.6806, val loss 0.6959\n",
      "step 17400 of 100000: train_loss 0.6962, val loss 0.6882\n",
      "step 17500 of 100000: train_loss 0.6892, val loss 0.6966\n",
      "step 17600 of 100000: train_loss 0.6911, val loss 0.7140\n",
      "step 17700 of 100000: train_loss 0.6979, val loss 0.7104\n",
      "step 17800 of 100000: train_loss 0.6858, val loss 0.6916\n",
      "step 17900 of 100000: train_loss 0.6725, val loss 0.7017\n",
      "step 18000 of 100000: train_loss 0.7024, val loss 0.6917\n",
      "step 18100 of 100000: train_loss 0.6827, val loss 0.6844\n",
      "step 18200 of 100000: train_loss 0.7070, val loss 0.6848\n",
      "step 18300 of 100000: train_loss 0.6923, val loss 0.6866\n",
      "step 18400 of 100000: train_loss 0.6948, val loss 0.7229\n",
      "step 18500 of 100000: train_loss 0.7090, val loss 0.6974\n",
      "step 18600 of 100000: train_loss 0.6868, val loss 0.6750\n",
      "step 18700 of 100000: train_loss 0.7025, val loss 0.7204\n",
      "step 18800 of 100000: train_loss 0.7634, val loss 0.7663\n",
      "step 18900 of 100000: train_loss 0.6910, val loss 0.6899\n",
      "step 19000 of 100000: train_loss 0.6972, val loss 0.6975\n",
      "step 19100 of 100000: train_loss 0.7125, val loss 0.7065\n",
      "step 19200 of 100000: train_loss 0.7039, val loss 0.7025\n",
      "step 19300 of 100000: train_loss 0.7176, val loss 0.6908\n",
      "step 19400 of 100000: train_loss 0.7117, val loss 0.7167\n",
      "step 19500 of 100000: train_loss 0.6874, val loss 0.7059\n",
      "step 19600 of 100000: train_loss 0.6980, val loss 0.7049\n",
      "step 19700 of 100000: train_loss 0.6824, val loss 0.7089\n",
      "step 19800 of 100000: train_loss 0.6728, val loss 0.6830\n",
      "step 19900 of 100000: train_loss 0.6919, val loss 0.6823\n",
      "step 20000 of 100000: train_loss 0.6954, val loss 0.6999\n",
      "step 20100 of 100000: train_loss 0.6842, val loss 0.7031\n",
      "step 20200 of 100000: train_loss 0.7044, val loss 0.6934\n",
      "step 20300 of 100000: train_loss 0.7146, val loss 0.7109\n",
      "step 20400 of 100000: train_loss 0.6936, val loss 0.6900\n",
      "step 20500 of 100000: train_loss 0.6996, val loss 0.7185\n",
      "step 20600 of 100000: train_loss 0.7052, val loss 0.7049\n",
      "step 20700 of 100000: train_loss 0.7015, val loss 0.6929\n",
      "step 20800 of 100000: train_loss 0.6818, val loss 0.6851\n",
      "step 20900 of 100000: train_loss 0.6889, val loss 0.7004\n",
      "step 21000 of 100000: train_loss 0.7029, val loss 0.7112\n",
      "step 21100 of 100000: train_loss 0.6880, val loss 0.6869\n",
      "step 21200 of 100000: train_loss 0.6828, val loss 0.6988\n",
      "step 21300 of 100000: train_loss 0.6804, val loss 0.6967\n",
      "step 21400 of 100000: train_loss 0.7040, val loss 0.6847\n",
      "step 21500 of 100000: train_loss 0.6988, val loss 0.7275\n",
      "step 21600 of 100000: train_loss 0.7030, val loss 0.6876\n",
      "step 21700 of 100000: train_loss 0.6758, val loss 0.6977\n",
      "step 21800 of 100000: train_loss 0.7038, val loss 0.6872\n",
      "step 21900 of 100000: train_loss 0.6931, val loss 0.7100\n",
      "step 22000 of 100000: train_loss 0.6999, val loss 0.6925\n",
      "step 22100 of 100000: train_loss 0.6983, val loss 0.6854\n",
      "step 22200 of 100000: train_loss 0.6728, val loss 0.6819\n",
      "step 22300 of 100000: train_loss 0.6978, val loss 0.7130\n",
      "step 22400 of 100000: train_loss 0.6860, val loss 0.6950\n",
      "step 22500 of 100000: train_loss 0.6853, val loss 0.7071\n",
      "step 22600 of 100000: train_loss 0.7041, val loss 0.6990\n",
      "step 22700 of 100000: train_loss 0.6918, val loss 0.7022\n",
      "step 22800 of 100000: train_loss 0.6953, val loss 0.6985\n",
      "step 22900 of 100000: train_loss 0.7054, val loss 0.6842\n",
      "step 23000 of 100000: train_loss 0.6880, val loss 0.7007\n",
      "step 23100 of 100000: train_loss 0.7053, val loss 0.7059\n",
      "step 23200 of 100000: train_loss 0.6889, val loss 0.6936\n",
      "step 23300 of 100000: train_loss 0.6863, val loss 0.6885\n",
      "step 23400 of 100000: train_loss 0.6763, val loss 0.7044\n",
      "step 23500 of 100000: train_loss 0.6967, val loss 0.6922\n",
      "step 23600 of 100000: train_loss 0.6965, val loss 0.6942\n",
      "step 23700 of 100000: train_loss 0.6798, val loss 0.7000\n",
      "step 23800 of 100000: train_loss 0.6997, val loss 0.6853\n",
      "step 23900 of 100000: train_loss 0.7017, val loss 0.6958\n",
      "step 24000 of 100000: train_loss 0.6832, val loss 0.6953\n",
      "step 24100 of 100000: train_loss 0.6757, val loss 0.7037\n",
      "step 24200 of 100000: train_loss 0.6877, val loss 0.6892\n",
      "step 24300 of 100000: train_loss 0.7236, val loss 0.7060\n",
      "step 24400 of 100000: train_loss 0.6907, val loss 0.6906\n",
      "step 24500 of 100000: train_loss 0.7037, val loss 0.6944\n",
      "step 24600 of 100000: train_loss 0.6943, val loss 0.6991\n",
      "step 24700 of 100000: train_loss 0.7019, val loss 0.7043\n",
      "step 24800 of 100000: train_loss 0.6929, val loss 0.6956\n",
      "step 24900 of 100000: train_loss 0.6731, val loss 0.6869\n",
      "step 25000 of 100000: train_loss 0.6971, val loss 0.6883\n",
      "step 25100 of 100000: train_loss 0.6856, val loss 0.6991\n",
      "step 25200 of 100000: train_loss 0.6933, val loss 0.6959\n",
      "step 25300 of 100000: train_loss 0.6711, val loss 0.6878\n",
      "step 25400 of 100000: train_loss 0.6979, val loss 0.6977\n",
      "step 25500 of 100000: train_loss 0.7084, val loss 0.6929\n",
      "step 25600 of 100000: train_loss 0.6863, val loss 0.6974\n",
      "step 25700 of 100000: train_loss 0.6972, val loss 0.7039\n",
      "step 25800 of 100000: train_loss 0.6872, val loss 0.6691\n",
      "step 25900 of 100000: train_loss 0.6814, val loss 0.6937\n",
      "step 26000 of 100000: train_loss 0.7109, val loss 0.7328\n",
      "step 26100 of 100000: train_loss 0.7093, val loss 0.7018\n",
      "step 26200 of 100000: train_loss 0.6864, val loss 0.6932\n",
      "step 26300 of 100000: train_loss 0.6872, val loss 0.6695\n",
      "step 26400 of 100000: train_loss 0.6882, val loss 0.7052\n",
      "step 26500 of 100000: train_loss 0.6834, val loss 0.6997\n",
      "step 26600 of 100000: train_loss 0.6765, val loss 0.6915\n",
      "step 26700 of 100000: train_loss 0.6992, val loss 0.7082\n",
      "step 26800 of 100000: train_loss 0.6966, val loss 0.6899\n",
      "step 26900 of 100000: train_loss 0.6799, val loss 0.7201\n",
      "step 27000 of 100000: train_loss 0.7078, val loss 0.7040\n",
      "step 27100 of 100000: train_loss 0.7034, val loss 0.6868\n",
      "step 27200 of 100000: train_loss 0.6930, val loss 0.7072\n",
      "step 27300 of 100000: train_loss 0.7088, val loss 0.7029\n",
      "step 27400 of 100000: train_loss 0.6938, val loss 0.7053\n",
      "step 27500 of 100000: train_loss 0.6965, val loss 0.7023\n",
      "step 27600 of 100000: train_loss 0.7013, val loss 0.6910\n",
      "step 27700 of 100000: train_loss 0.6966, val loss 0.6916\n",
      "step 27800 of 100000: train_loss 0.7354, val loss 0.7282\n",
      "step 27900 of 100000: train_loss 0.7038, val loss 0.6838\n",
      "step 28000 of 100000: train_loss 0.6805, val loss 0.6893\n",
      "step 28100 of 100000: train_loss 0.6854, val loss 0.6991\n",
      "step 28200 of 100000: train_loss 0.6740, val loss 0.6883\n",
      "step 28300 of 100000: train_loss 0.7018, val loss 0.7066\n",
      "step 28400 of 100000: train_loss 0.6938, val loss 0.7227\n",
      "step 28500 of 100000: train_loss 0.6939, val loss 0.6928\n",
      "step 28600 of 100000: train_loss 0.6924, val loss 0.7087\n",
      "step 28700 of 100000: train_loss 0.6883, val loss 0.6835\n",
      "step 28800 of 100000: train_loss 0.7161, val loss 0.6946\n",
      "step 28900 of 100000: train_loss 0.6877, val loss 0.7108\n",
      "step 29000 of 100000: train_loss 0.6860, val loss 0.6977\n",
      "step 29100 of 100000: train_loss 0.6913, val loss 0.6876\n",
      "step 29200 of 100000: train_loss 0.6779, val loss 0.6970\n",
      "step 29300 of 100000: train_loss 0.6850, val loss 0.7030\n",
      "step 29400 of 100000: train_loss 0.6952, val loss 0.6921\n",
      "step 29500 of 100000: train_loss 0.6956, val loss 0.6997\n",
      "step 29600 of 100000: train_loss 0.6896, val loss 0.6827\n",
      "step 29700 of 100000: train_loss 0.6883, val loss 0.6924\n",
      "step 29800 of 100000: train_loss 0.6824, val loss 0.6956\n",
      "step 29900 of 100000: train_loss 0.6799, val loss 0.6854\n",
      "step 30000 of 100000: train_loss 0.7065, val loss 0.7259\n",
      "step 30100 of 100000: train_loss 0.7095, val loss 0.7207\n",
      "step 30200 of 100000: train_loss 0.6929, val loss 0.7061\n",
      "step 30300 of 100000: train_loss 0.6858, val loss 0.6801\n",
      "step 30400 of 100000: train_loss 0.6805, val loss 0.6988\n",
      "step 30500 of 100000: train_loss 0.6903, val loss 0.6878\n",
      "step 30600 of 100000: train_loss 0.6861, val loss 0.6923\n",
      "step 30700 of 100000: train_loss 0.6857, val loss 0.6955\n",
      "step 30800 of 100000: train_loss 0.6949, val loss 0.7077\n",
      "step 30900 of 100000: train_loss 0.7127, val loss 0.6882\n",
      "step 31000 of 100000: train_loss 0.6909, val loss 0.6893\n",
      "step 31100 of 100000: train_loss 0.6923, val loss 0.6704\n",
      "step 31200 of 100000: train_loss 0.6929, val loss 0.6876\n",
      "step 31300 of 100000: train_loss 0.7025, val loss 0.7005\n",
      "step 31400 of 100000: train_loss 0.6784, val loss 0.6911\n",
      "step 31500 of 100000: train_loss 0.6878, val loss 0.6936\n",
      "step 31600 of 100000: train_loss 0.7016, val loss 0.6965\n",
      "step 31700 of 100000: train_loss 0.6754, val loss 0.7008\n",
      "step 31800 of 100000: train_loss 0.6956, val loss 0.7164\n",
      "step 31900 of 100000: train_loss 0.6800, val loss 0.7117\n",
      "step 32000 of 100000: train_loss 0.6963, val loss 0.6778\n",
      "step 32100 of 100000: train_loss 0.7023, val loss 0.6772\n",
      "step 32200 of 100000: train_loss 0.7060, val loss 0.6917\n",
      "step 32300 of 100000: train_loss 0.7045, val loss 0.7040\n",
      "step 32400 of 100000: train_loss 0.7051, val loss 0.6918\n",
      "step 32500 of 100000: train_loss 0.6822, val loss 0.7115\n",
      "step 32600 of 100000: train_loss 0.6627, val loss 0.7004\n",
      "step 32700 of 100000: train_loss 0.6909, val loss 0.6935\n",
      "step 32800 of 100000: train_loss 0.6667, val loss 0.7071\n",
      "step 32900 of 100000: train_loss 0.6846, val loss 0.6925\n",
      "step 33000 of 100000: train_loss 0.6733, val loss 0.6983\n",
      "step 33100 of 100000: train_loss 0.6858, val loss 0.6948\n",
      "step 33200 of 100000: train_loss 0.6950, val loss 0.6964\n",
      "step 33300 of 100000: train_loss 0.6893, val loss 0.6966\n",
      "step 33400 of 100000: train_loss 0.6903, val loss 0.6986\n",
      "step 33500 of 100000: train_loss 0.7150, val loss 0.7177\n",
      "step 33600 of 100000: train_loss 0.6929, val loss 0.7036\n",
      "step 33700 of 100000: train_loss 0.6887, val loss 0.6779\n",
      "step 33800 of 100000: train_loss 0.7104, val loss 0.7099\n",
      "step 33900 of 100000: train_loss 0.6993, val loss 0.6986\n",
      "step 34000 of 100000: train_loss 0.7005, val loss 0.7066\n",
      "step 34100 of 100000: train_loss 0.6773, val loss 0.6949\n",
      "step 34200 of 100000: train_loss 0.7059, val loss 0.7257\n",
      "step 34300 of 100000: train_loss 0.6873, val loss 0.6892\n",
      "step 34400 of 100000: train_loss 0.7020, val loss 0.7056\n",
      "step 34500 of 100000: train_loss 0.6966, val loss 0.7008\n",
      "step 34600 of 100000: train_loss 0.6773, val loss 0.6959\n",
      "step 34700 of 100000: train_loss 0.6861, val loss 0.7125\n",
      "step 34800 of 100000: train_loss 0.7120, val loss 0.7084\n",
      "step 34900 of 100000: train_loss 0.7090, val loss 0.6859\n",
      "step 35000 of 100000: train_loss 0.6831, val loss 0.6982\n",
      "step 35100 of 100000: train_loss 0.7074, val loss 0.7333\n",
      "step 35200 of 100000: train_loss 0.7061, val loss 0.7091\n",
      "step 35300 of 100000: train_loss 0.6736, val loss 0.7018\n",
      "step 35400 of 100000: train_loss 0.6976, val loss 0.6743\n",
      "step 35500 of 100000: train_loss 0.6997, val loss 0.6844\n",
      "step 35600 of 100000: train_loss 0.6916, val loss 0.7056\n",
      "step 35700 of 100000: train_loss 0.6794, val loss 0.6916\n",
      "step 35800 of 100000: train_loss 0.6727, val loss 0.6874\n",
      "step 35900 of 100000: train_loss 0.6834, val loss 0.7037\n",
      "step 36000 of 100000: train_loss 0.6972, val loss 0.6832\n",
      "step 36100 of 100000: train_loss 0.6985, val loss 0.7152\n",
      "step 36200 of 100000: train_loss 0.6867, val loss 0.7005\n",
      "step 36300 of 100000: train_loss 0.6974, val loss 0.7056\n",
      "step 36400 of 100000: train_loss 0.6766, val loss 0.6990\n",
      "step 36500 of 100000: train_loss 0.7131, val loss 0.7264\n",
      "step 36600 of 100000: train_loss 0.6684, val loss 0.6817\n",
      "step 36700 of 100000: train_loss 0.6936, val loss 0.6999\n",
      "step 36800 of 100000: train_loss 0.6783, val loss 0.6859\n",
      "step 36900 of 100000: train_loss 0.7034, val loss 0.7123\n",
      "step 37000 of 100000: train_loss 0.6896, val loss 0.7103\n",
      "step 37100 of 100000: train_loss 0.7121, val loss 0.7011\n",
      "step 37200 of 100000: train_loss 0.6854, val loss 0.6915\n",
      "step 37300 of 100000: train_loss 0.7058, val loss 0.6856\n",
      "step 37400 of 100000: train_loss 0.6806, val loss 0.6814\n",
      "step 37500 of 100000: train_loss 0.6949, val loss 0.7046\n",
      "step 37600 of 100000: train_loss 0.7001, val loss 0.6947\n",
      "step 37700 of 100000: train_loss 0.7023, val loss 0.6798\n",
      "step 37800 of 100000: train_loss 0.6743, val loss 0.6904\n",
      "step 37900 of 100000: train_loss 0.6762, val loss 0.6850\n",
      "step 38000 of 100000: train_loss 0.7007, val loss 0.6892\n",
      "step 38100 of 100000: train_loss 0.6978, val loss 0.6846\n",
      "step 38200 of 100000: train_loss 0.6766, val loss 0.6946\n",
      "step 38300 of 100000: train_loss 0.7101, val loss 0.6992\n",
      "step 38400 of 100000: train_loss 0.7014, val loss 0.6916\n",
      "step 38500 of 100000: train_loss 0.6828, val loss 0.7079\n",
      "step 38600 of 100000: train_loss 0.7019, val loss 0.6946\n",
      "step 38700 of 100000: train_loss 0.6796, val loss 0.7148\n",
      "step 38800 of 100000: train_loss 0.6880, val loss 0.7088\n",
      "step 38900 of 100000: train_loss 0.6865, val loss 0.6970\n",
      "step 39000 of 100000: train_loss 0.6835, val loss 0.7014\n",
      "step 39100 of 100000: train_loss 0.6928, val loss 0.6927\n",
      "step 39200 of 100000: train_loss 0.6836, val loss 0.7048\n",
      "step 39300 of 100000: train_loss 0.6896, val loss 0.6909\n",
      "step 39400 of 100000: train_loss 0.7078, val loss 0.7116\n",
      "step 39500 of 100000: train_loss 0.7017, val loss 0.6959\n",
      "step 39600 of 100000: train_loss 0.6773, val loss 0.7063\n",
      "step 39700 of 100000: train_loss 0.7009, val loss 0.7285\n",
      "step 39800 of 100000: train_loss 0.7007, val loss 0.7021\n",
      "step 39900 of 100000: train_loss 0.6810, val loss 0.7013\n",
      "step 40000 of 100000: train_loss 0.7087, val loss 0.7059\n",
      "step 40100 of 100000: train_loss 0.6975, val loss 0.7039\n",
      "step 40200 of 100000: train_loss 0.6767, val loss 0.6846\n",
      "step 40300 of 100000: train_loss 0.7076, val loss 0.7110\n",
      "step 40400 of 100000: train_loss 0.6957, val loss 0.7042\n",
      "step 40500 of 100000: train_loss 0.7038, val loss 0.6958\n",
      "step 40600 of 100000: train_loss 0.6786, val loss 0.7033\n",
      "step 40700 of 100000: train_loss 0.6930, val loss 0.7034\n",
      "step 40800 of 100000: train_loss 0.7049, val loss 0.7227\n",
      "step 40900 of 100000: train_loss 0.6860, val loss 0.7072\n",
      "step 41000 of 100000: train_loss 0.6782, val loss 0.7006\n",
      "step 41100 of 100000: train_loss 0.6892, val loss 0.6981\n",
      "step 41200 of 100000: train_loss 0.7011, val loss 0.6704\n",
      "step 41300 of 100000: train_loss 0.6936, val loss 0.7110\n",
      "step 41400 of 100000: train_loss 0.6977, val loss 0.6901\n",
      "step 41500 of 100000: train_loss 0.6859, val loss 0.7038\n",
      "step 41600 of 100000: train_loss 0.6863, val loss 0.6890\n",
      "step 41700 of 100000: train_loss 0.6953, val loss 0.6825\n",
      "step 41800 of 100000: train_loss 0.6969, val loss 0.6822\n",
      "step 41900 of 100000: train_loss 0.6629, val loss 0.6984\n",
      "step 42000 of 100000: train_loss 0.6958, val loss 0.7061\n",
      "step 42100 of 100000: train_loss 0.6722, val loss 0.7112\n",
      "step 42200 of 100000: train_loss 0.6928, val loss 0.7176\n",
      "step 42300 of 100000: train_loss 0.6999, val loss 0.6863\n",
      "step 42400 of 100000: train_loss 0.7000, val loss 0.6985\n",
      "step 42500 of 100000: train_loss 0.6852, val loss 0.6999\n",
      "step 42600 of 100000: train_loss 0.6958, val loss 0.6937\n",
      "step 42700 of 100000: train_loss 0.7085, val loss 0.6941\n",
      "step 42800 of 100000: train_loss 0.6995, val loss 0.7074\n",
      "step 42900 of 100000: train_loss 0.6603, val loss 0.7092\n",
      "step 43000 of 100000: train_loss 0.6771, val loss 0.7039\n",
      "step 43100 of 100000: train_loss 0.6909, val loss 0.7006\n",
      "step 43200 of 100000: train_loss 0.6549, val loss 0.6923\n",
      "step 43300 of 100000: train_loss 0.6906, val loss 0.6970\n",
      "step 43400 of 100000: train_loss 0.6875, val loss 0.7034\n",
      "step 43500 of 100000: train_loss 0.7108, val loss 0.7179\n",
      "step 43600 of 100000: train_loss 0.6890, val loss 0.6871\n",
      "step 43700 of 100000: train_loss 0.6911, val loss 0.7102\n",
      "step 43800 of 100000: train_loss 0.7017, val loss 0.6910\n",
      "step 43900 of 100000: train_loss 0.6818, val loss 0.6867\n",
      "step 44000 of 100000: train_loss 0.6817, val loss 0.6959\n",
      "step 44100 of 100000: train_loss 0.6886, val loss 0.6857\n",
      "step 44200 of 100000: train_loss 0.7055, val loss 0.6852\n",
      "step 44300 of 100000: train_loss 0.6896, val loss 0.7051\n",
      "step 44400 of 100000: train_loss 0.6636, val loss 0.6969\n",
      "step 44500 of 100000: train_loss 0.6931, val loss 0.7105\n",
      "step 44600 of 100000: train_loss 0.6725, val loss 0.7063\n",
      "step 44700 of 100000: train_loss 0.6904, val loss 0.7030\n",
      "step 44800 of 100000: train_loss 0.6880, val loss 0.7224\n",
      "step 44900 of 100000: train_loss 0.7383, val loss 0.7123\n",
      "step 45000 of 100000: train_loss 0.6867, val loss 0.6906\n",
      "step 45100 of 100000: train_loss 0.6995, val loss 0.6936\n",
      "step 45200 of 100000: train_loss 0.6853, val loss 0.7055\n",
      "step 45300 of 100000: train_loss 0.6916, val loss 0.6964\n",
      "step 45400 of 100000: train_loss 0.7037, val loss 0.7088\n",
      "step 45500 of 100000: train_loss 0.6892, val loss 0.6884\n",
      "step 45600 of 100000: train_loss 0.6945, val loss 0.7177\n",
      "step 45700 of 100000: train_loss 0.6868, val loss 0.6971\n",
      "step 45800 of 100000: train_loss 0.6862, val loss 0.7021\n",
      "step 45900 of 100000: train_loss 0.6907, val loss 0.7051\n",
      "step 46000 of 100000: train_loss 0.6789, val loss 0.7014\n",
      "step 46100 of 100000: train_loss 0.6869, val loss 0.6948\n",
      "step 46200 of 100000: train_loss 0.6919, val loss 0.7187\n",
      "step 46300 of 100000: train_loss 0.7121, val loss 0.7102\n",
      "step 46400 of 100000: train_loss 0.6739, val loss 0.7029\n",
      "step 46500 of 100000: train_loss 0.6929, val loss 0.7021\n",
      "step 46600 of 100000: train_loss 0.6766, val loss 0.6950\n",
      "step 46700 of 100000: train_loss 0.6672, val loss 0.7176\n",
      "step 46800 of 100000: train_loss 0.7163, val loss 0.7341\n",
      "step 46900 of 100000: train_loss 0.6807, val loss 0.6913\n",
      "step 47000 of 100000: train_loss 0.6852, val loss 0.6879\n",
      "step 47100 of 100000: train_loss 0.6891, val loss 0.7026\n",
      "step 47200 of 100000: train_loss 0.6986, val loss 0.6965\n",
      "step 47300 of 100000: train_loss 0.7023, val loss 0.6924\n",
      "step 47400 of 100000: train_loss 0.7152, val loss 0.7120\n",
      "step 47500 of 100000: train_loss 0.6848, val loss 0.7032\n",
      "step 47600 of 100000: train_loss 0.6853, val loss 0.7043\n",
      "step 47700 of 100000: train_loss 0.6909, val loss 0.6864\n",
      "step 47800 of 100000: train_loss 0.6751, val loss 0.7004\n",
      "step 47900 of 100000: train_loss 0.6642, val loss 0.6862\n",
      "step 48000 of 100000: train_loss 0.6815, val loss 0.7077\n",
      "step 48100 of 100000: train_loss 0.6556, val loss 0.6955\n",
      "step 48200 of 100000: train_loss 0.6898, val loss 0.6964\n",
      "step 48300 of 100000: train_loss 0.6819, val loss 0.6959\n",
      "step 48400 of 100000: train_loss 0.6819, val loss 0.6848\n",
      "step 48500 of 100000: train_loss 0.6892, val loss 0.6783\n",
      "step 48600 of 100000: train_loss 0.7020, val loss 0.6892\n",
      "step 48700 of 100000: train_loss 0.6872, val loss 0.6956\n",
      "step 48800 of 100000: train_loss 0.6974, val loss 0.7082\n",
      "step 48900 of 100000: train_loss 0.7034, val loss 0.7042\n",
      "step 49000 of 100000: train_loss 0.7194, val loss 0.6811\n",
      "step 49100 of 100000: train_loss 0.6876, val loss 0.6989\n",
      "step 49200 of 100000: train_loss 0.6868, val loss 0.6856\n",
      "step 49300 of 100000: train_loss 0.6907, val loss 0.7038\n",
      "step 49400 of 100000: train_loss 0.7090, val loss 0.6992\n",
      "step 49500 of 100000: train_loss 0.6841, val loss 0.6992\n",
      "step 49600 of 100000: train_loss 0.7035, val loss 0.7114\n",
      "step 49700 of 100000: train_loss 0.7039, val loss 0.7053\n",
      "step 49800 of 100000: train_loss 0.7012, val loss 0.7012\n",
      "step 49900 of 100000: train_loss 0.7097, val loss 0.6931\n",
      "step 50000 of 100000: train_loss 0.7209, val loss 0.7255\n",
      "step 50100 of 100000: train_loss 0.6947, val loss 0.7119\n",
      "step 50200 of 100000: train_loss 0.6820, val loss 0.6974\n",
      "step 50300 of 100000: train_loss 0.7027, val loss 0.6900\n",
      "step 50400 of 100000: train_loss 0.6989, val loss 0.6928\n",
      "step 50500 of 100000: train_loss 0.7087, val loss 0.6978\n",
      "step 50600 of 100000: train_loss 0.6842, val loss 0.7076\n",
      "step 50700 of 100000: train_loss 0.7060, val loss 0.6998\n",
      "step 50800 of 100000: train_loss 0.6830, val loss 0.6936\n",
      "step 50900 of 100000: train_loss 0.6897, val loss 0.6889\n",
      "step 51000 of 100000: train_loss 0.6933, val loss 0.7088\n",
      "step 51100 of 100000: train_loss 0.6911, val loss 0.7051\n",
      "step 51200 of 100000: train_loss 0.7041, val loss 0.7043\n",
      "step 51300 of 100000: train_loss 0.6742, val loss 0.6890\n",
      "step 51400 of 100000: train_loss 0.6941, val loss 0.7015\n",
      "step 51500 of 100000: train_loss 0.6796, val loss 0.7219\n",
      "step 51600 of 100000: train_loss 0.6809, val loss 0.6886\n",
      "step 51700 of 100000: train_loss 0.6971, val loss 0.7061\n",
      "step 51800 of 100000: train_loss 0.7042, val loss 0.7012\n",
      "step 51900 of 100000: train_loss 0.6797, val loss 0.7022\n",
      "step 52000 of 100000: train_loss 0.6938, val loss 0.6794\n",
      "step 52100 of 100000: train_loss 0.6987, val loss 0.6839\n",
      "step 52200 of 100000: train_loss 0.7112, val loss 0.7060\n",
      "step 52300 of 100000: train_loss 0.6871, val loss 0.6656\n",
      "step 52400 of 100000: train_loss 0.6841, val loss 0.6866\n",
      "step 52500 of 100000: train_loss 0.7043, val loss 0.7022\n",
      "step 52600 of 100000: train_loss 0.7039, val loss 0.7058\n",
      "step 52700 of 100000: train_loss 0.6888, val loss 0.6945\n",
      "step 52800 of 100000: train_loss 0.6851, val loss 0.6980\n",
      "step 52900 of 100000: train_loss 0.6701, val loss 0.7047\n",
      "step 53000 of 100000: train_loss 0.6988, val loss 0.6908\n",
      "step 53100 of 100000: train_loss 0.6976, val loss 0.6903\n",
      "step 53200 of 100000: train_loss 0.6710, val loss 0.6996\n",
      "step 53300 of 100000: train_loss 0.6745, val loss 0.6936\n",
      "step 53400 of 100000: train_loss 0.6908, val loss 0.7067\n",
      "step 53500 of 100000: train_loss 0.6804, val loss 0.7011\n",
      "step 53600 of 100000: train_loss 0.6752, val loss 0.6909\n",
      "step 53700 of 100000: train_loss 0.6964, val loss 0.7140\n",
      "step 53800 of 100000: train_loss 0.6872, val loss 0.6978\n",
      "step 53900 of 100000: train_loss 0.6817, val loss 0.6933\n",
      "step 54000 of 100000: train_loss 0.6976, val loss 0.6802\n",
      "step 54100 of 100000: train_loss 0.7173, val loss 0.7100\n",
      "step 54200 of 100000: train_loss 0.7097, val loss 0.7016\n",
      "step 54300 of 100000: train_loss 0.6788, val loss 0.6941\n",
      "step 54400 of 100000: train_loss 0.7007, val loss 0.6929\n",
      "step 54500 of 100000: train_loss 0.7062, val loss 0.7058\n",
      "step 54600 of 100000: train_loss 0.6793, val loss 0.7136\n",
      "step 54700 of 100000: train_loss 0.6859, val loss 0.7245\n",
      "step 54800 of 100000: train_loss 0.7037, val loss 0.7026\n",
      "step 54900 of 100000: train_loss 0.6915, val loss 0.6910\n",
      "step 55000 of 100000: train_loss 0.6715, val loss 0.6925\n",
      "step 55100 of 100000: train_loss 0.6854, val loss 0.7089\n",
      "step 55200 of 100000: train_loss 0.6907, val loss 0.6963\n",
      "step 55300 of 100000: train_loss 0.7071, val loss 0.6879\n",
      "step 55400 of 100000: train_loss 0.6943, val loss 0.6908\n",
      "step 55500 of 100000: train_loss 0.6799, val loss 0.6918\n",
      "step 55600 of 100000: train_loss 0.6729, val loss 0.6826\n",
      "step 55700 of 100000: train_loss 0.6923, val loss 0.6890\n",
      "step 55800 of 100000: train_loss 0.6956, val loss 0.7076\n",
      "step 55900 of 100000: train_loss 0.6935, val loss 0.6912\n",
      "step 56000 of 100000: train_loss 0.7025, val loss 0.7028\n",
      "step 56100 of 100000: train_loss 0.6923, val loss 0.7062\n",
      "step 56200 of 100000: train_loss 0.6999, val loss 0.6999\n",
      "step 56300 of 100000: train_loss 0.6874, val loss 0.7042\n",
      "step 56400 of 100000: train_loss 0.6746, val loss 0.6954\n",
      "step 56500 of 100000: train_loss 0.7034, val loss 0.6864\n",
      "step 56600 of 100000: train_loss 0.6910, val loss 0.6892\n",
      "step 56700 of 100000: train_loss 0.6923, val loss 0.6886\n",
      "step 56800 of 100000: train_loss 0.6772, val loss 0.6782\n",
      "step 56900 of 100000: train_loss 0.6839, val loss 0.7124\n",
      "step 57000 of 100000: train_loss 0.6874, val loss 0.7167\n",
      "step 57100 of 100000: train_loss 0.6933, val loss 0.7005\n",
      "step 57200 of 100000: train_loss 0.6844, val loss 0.6979\n",
      "step 57300 of 100000: train_loss 0.6970, val loss 0.6815\n",
      "step 57400 of 100000: train_loss 0.6869, val loss 0.7110\n",
      "step 57500 of 100000: train_loss 0.6860, val loss 0.6960\n",
      "step 57600 of 100000: train_loss 0.7047, val loss 0.7014\n",
      "step 57700 of 100000: train_loss 0.6887, val loss 0.7162\n",
      "step 57800 of 100000: train_loss 0.6807, val loss 0.6992\n",
      "step 57900 of 100000: train_loss 0.6761, val loss 0.7059\n",
      "step 58000 of 100000: train_loss 0.6932, val loss 0.7112\n",
      "step 58100 of 100000: train_loss 0.6984, val loss 0.7084\n",
      "step 58200 of 100000: train_loss 0.6794, val loss 0.6944\n",
      "step 58300 of 100000: train_loss 0.6831, val loss 0.6899\n",
      "step 58400 of 100000: train_loss 0.6744, val loss 0.6927\n",
      "step 58500 of 100000: train_loss 0.6833, val loss 0.6955\n",
      "step 58600 of 100000: train_loss 0.7018, val loss 0.7095\n",
      "step 58700 of 100000: train_loss 0.6925, val loss 0.7114\n",
      "step 58800 of 100000: train_loss 0.6843, val loss 0.6866\n",
      "step 58900 of 100000: train_loss 0.6838, val loss 0.7089\n",
      "step 59000 of 100000: train_loss 0.6943, val loss 0.6909\n",
      "step 59100 of 100000: train_loss 0.7216, val loss 0.7201\n",
      "step 59200 of 100000: train_loss 0.6928, val loss 0.7070\n",
      "step 59300 of 100000: train_loss 0.6979, val loss 0.6852\n",
      "step 59400 of 100000: train_loss 0.7062, val loss 0.7083\n",
      "step 59500 of 100000: train_loss 0.7020, val loss 0.7082\n",
      "step 59600 of 100000: train_loss 0.7011, val loss 0.6871\n",
      "step 59700 of 100000: train_loss 0.7053, val loss 0.6762\n",
      "step 59800 of 100000: train_loss 0.6859, val loss 0.7050\n",
      "step 59900 of 100000: train_loss 0.6789, val loss 0.7117\n",
      "step 60000 of 100000: train_loss 0.6907, val loss 0.7002\n",
      "step 60100 of 100000: train_loss 0.6813, val loss 0.6852\n",
      "step 60200 of 100000: train_loss 0.6795, val loss 0.6812\n",
      "step 60300 of 100000: train_loss 0.6905, val loss 0.6992\n",
      "step 60400 of 100000: train_loss 0.6953, val loss 0.7018\n",
      "step 60500 of 100000: train_loss 0.6832, val loss 0.6953\n",
      "step 60600 of 100000: train_loss 0.6839, val loss 0.6821\n",
      "step 60700 of 100000: train_loss 0.6802, val loss 0.6930\n",
      "step 60800 of 100000: train_loss 0.6722, val loss 0.6840\n",
      "step 60900 of 100000: train_loss 0.6780, val loss 0.6959\n",
      "step 61000 of 100000: train_loss 0.6747, val loss 0.7104\n",
      "step 61100 of 100000: train_loss 0.7069, val loss 0.7023\n",
      "step 61200 of 100000: train_loss 0.6988, val loss 0.6773\n",
      "step 61300 of 100000: train_loss 0.7002, val loss 0.7049\n",
      "step 61400 of 100000: train_loss 0.6770, val loss 0.6698\n",
      "step 61500 of 100000: train_loss 0.6887, val loss 0.7064\n",
      "step 61600 of 100000: train_loss 0.6874, val loss 0.7018\n",
      "step 61700 of 100000: train_loss 0.6812, val loss 0.7103\n",
      "step 61800 of 100000: train_loss 0.6911, val loss 0.7141\n",
      "step 61900 of 100000: train_loss 0.7080, val loss 0.7150\n",
      "step 62000 of 100000: train_loss 0.6804, val loss 0.6900\n",
      "step 62100 of 100000: train_loss 0.6954, val loss 0.7023\n",
      "step 62200 of 100000: train_loss 0.6949, val loss 0.6918\n",
      "step 62300 of 100000: train_loss 0.6980, val loss 0.6961\n",
      "step 62400 of 100000: train_loss 0.6802, val loss 0.6930\n",
      "step 62500 of 100000: train_loss 0.6820, val loss 0.7090\n",
      "step 62600 of 100000: train_loss 0.6899, val loss 0.7087\n",
      "step 62700 of 100000: train_loss 0.7002, val loss 0.6892\n",
      "step 62800 of 100000: train_loss 0.6844, val loss 0.7030\n",
      "step 62900 of 100000: train_loss 0.6824, val loss 0.6859\n",
      "step 63000 of 100000: train_loss 0.6811, val loss 0.6782\n",
      "step 63100 of 100000: train_loss 0.6931, val loss 0.6989\n",
      "step 63200 of 100000: train_loss 0.6806, val loss 0.6905\n",
      "step 63300 of 100000: train_loss 0.6965, val loss 0.6953\n",
      "step 63400 of 100000: train_loss 0.6859, val loss 0.6825\n",
      "step 63500 of 100000: train_loss 0.7043, val loss 0.7024\n",
      "step 63600 of 100000: train_loss 0.6763, val loss 0.6961\n",
      "step 63700 of 100000: train_loss 0.7001, val loss 0.7095\n",
      "step 63800 of 100000: train_loss 0.6931, val loss 0.6997\n",
      "step 63900 of 100000: train_loss 0.6853, val loss 0.6771\n",
      "step 64000 of 100000: train_loss 0.6967, val loss 0.6882\n",
      "step 64100 of 100000: train_loss 0.7162, val loss 0.7136\n",
      "step 64200 of 100000: train_loss 0.7066, val loss 0.7012\n",
      "step 64300 of 100000: train_loss 0.6898, val loss 0.6964\n",
      "step 64400 of 100000: train_loss 0.6877, val loss 0.7022\n",
      "step 64500 of 100000: train_loss 0.6983, val loss 0.7145\n",
      "step 64600 of 100000: train_loss 0.6901, val loss 0.6982\n",
      "step 64700 of 100000: train_loss 0.6834, val loss 0.6868\n",
      "step 64800 of 100000: train_loss 0.6661, val loss 0.6892\n",
      "step 64900 of 100000: train_loss 0.6943, val loss 0.6975\n",
      "step 65000 of 100000: train_loss 0.6985, val loss 0.7006\n",
      "step 65100 of 100000: train_loss 0.6937, val loss 0.6732\n",
      "step 65200 of 100000: train_loss 0.6854, val loss 0.7113\n",
      "step 65300 of 100000: train_loss 0.6885, val loss 0.7002\n",
      "step 65400 of 100000: train_loss 0.6673, val loss 0.6890\n",
      "step 65500 of 100000: train_loss 0.6796, val loss 0.6963\n",
      "step 65600 of 100000: train_loss 0.6895, val loss 0.6993\n",
      "step 65700 of 100000: train_loss 0.7064, val loss 0.6906\n",
      "step 65800 of 100000: train_loss 0.6859, val loss 0.7150\n",
      "step 65900 of 100000: train_loss 0.6676, val loss 0.7024\n",
      "step 66000 of 100000: train_loss 0.6831, val loss 0.6924\n",
      "step 66100 of 100000: train_loss 0.7010, val loss 0.6784\n",
      "step 66200 of 100000: train_loss 0.6755, val loss 0.6919\n",
      "step 66300 of 100000: train_loss 0.6700, val loss 0.6760\n",
      "step 66400 of 100000: train_loss 0.6945, val loss 0.6981\n",
      "step 66500 of 100000: train_loss 0.6828, val loss 0.6817\n",
      "step 66600 of 100000: train_loss 0.6860, val loss 0.6713\n",
      "step 66700 of 100000: train_loss 0.6874, val loss 0.6840\n",
      "step 66800 of 100000: train_loss 0.6993, val loss 0.6789\n",
      "step 66900 of 100000: train_loss 0.6993, val loss 0.6883\n",
      "step 67000 of 100000: train_loss 0.6880, val loss 0.7126\n",
      "step 67100 of 100000: train_loss 0.7072, val loss 0.7127\n",
      "step 67200 of 100000: train_loss 0.6925, val loss 0.7066\n",
      "step 67300 of 100000: train_loss 0.6766, val loss 0.6725\n",
      "step 67400 of 100000: train_loss 0.6748, val loss 0.6843\n",
      "step 67500 of 100000: train_loss 0.7217, val loss 0.7388\n",
      "step 67600 of 100000: train_loss 0.6887, val loss 0.7135\n",
      "step 67700 of 100000: train_loss 0.7048, val loss 0.7151\n",
      "step 67800 of 100000: train_loss 0.6872, val loss 0.6951\n",
      "step 67900 of 100000: train_loss 0.6976, val loss 0.7005\n",
      "step 68000 of 100000: train_loss 0.6927, val loss 0.6970\n",
      "step 68100 of 100000: train_loss 0.6912, val loss 0.7073\n",
      "step 68200 of 100000: train_loss 0.6671, val loss 0.7102\n",
      "step 68300 of 100000: train_loss 0.6877, val loss 0.7071\n",
      "step 68400 of 100000: train_loss 0.6786, val loss 0.7014\n",
      "step 68500 of 100000: train_loss 0.6941, val loss 0.7073\n",
      "step 68600 of 100000: train_loss 0.6758, val loss 0.6989\n",
      "step 68700 of 100000: train_loss 0.6904, val loss 0.7216\n",
      "step 68800 of 100000: train_loss 0.6845, val loss 0.7084\n",
      "step 68900 of 100000: train_loss 0.6950, val loss 0.7119\n",
      "step 69000 of 100000: train_loss 0.7151, val loss 0.7180\n",
      "step 69100 of 100000: train_loss 0.6829, val loss 0.7009\n",
      "step 69200 of 100000: train_loss 0.6948, val loss 0.7104\n",
      "step 69300 of 100000: train_loss 0.6916, val loss 0.7096\n",
      "step 69400 of 100000: train_loss 0.7072, val loss 0.7041\n",
      "step 69500 of 100000: train_loss 0.6944, val loss 0.6967\n",
      "step 69600 of 100000: train_loss 0.6810, val loss 0.6963\n",
      "step 69700 of 100000: train_loss 0.7105, val loss 0.7166\n",
      "step 69800 of 100000: train_loss 0.6862, val loss 0.7006\n",
      "step 69900 of 100000: train_loss 0.6989, val loss 0.7255\n",
      "step 70000 of 100000: train_loss 0.6689, val loss 0.6793\n",
      "step 70100 of 100000: train_loss 0.6819, val loss 0.7030\n",
      "step 70200 of 100000: train_loss 0.6812, val loss 0.6911\n",
      "step 70300 of 100000: train_loss 0.6886, val loss 0.7151\n",
      "step 70400 of 100000: train_loss 0.6956, val loss 0.6983\n",
      "step 70500 of 100000: train_loss 0.6990, val loss 0.7020\n",
      "step 70600 of 100000: train_loss 0.6825, val loss 0.7142\n",
      "step 70700 of 100000: train_loss 0.7043, val loss 0.7232\n",
      "step 70800 of 100000: train_loss 0.6850, val loss 0.7016\n",
      "step 70900 of 100000: train_loss 0.6856, val loss 0.7141\n",
      "step 71000 of 100000: train_loss 0.6842, val loss 0.6983\n",
      "step 71100 of 100000: train_loss 0.7080, val loss 0.6990\n",
      "step 71200 of 100000: train_loss 0.6861, val loss 0.7017\n",
      "step 71300 of 100000: train_loss 0.6815, val loss 0.6973\n",
      "step 71400 of 100000: train_loss 0.6849, val loss 0.6937\n",
      "step 71500 of 100000: train_loss 0.7016, val loss 0.6766\n",
      "step 71600 of 100000: train_loss 0.6730, val loss 0.6924\n",
      "step 71700 of 100000: train_loss 0.6953, val loss 0.6937\n",
      "step 71800 of 100000: train_loss 0.6971, val loss 0.7088\n",
      "step 71900 of 100000: train_loss 0.6936, val loss 0.6968\n",
      "step 72000 of 100000: train_loss 0.7168, val loss 0.7025\n",
      "step 72100 of 100000: train_loss 0.7196, val loss 0.7162\n",
      "step 72200 of 100000: train_loss 0.6901, val loss 0.6956\n",
      "step 72300 of 100000: train_loss 0.6821, val loss 0.7026\n",
      "step 72400 of 100000: train_loss 0.6725, val loss 0.7074\n",
      "step 72500 of 100000: train_loss 0.6861, val loss 0.6916\n",
      "step 72600 of 100000: train_loss 0.7040, val loss 0.7173\n",
      "step 72700 of 100000: train_loss 0.6920, val loss 0.7025\n",
      "step 72800 of 100000: train_loss 0.6776, val loss 0.6933\n",
      "step 72900 of 100000: train_loss 0.6982, val loss 0.7086\n",
      "step 73000 of 100000: train_loss 0.7002, val loss 0.6979\n",
      "step 73100 of 100000: train_loss 0.6819, val loss 0.6969\n",
      "step 73200 of 100000: train_loss 0.6944, val loss 0.7144\n",
      "step 73300 of 100000: train_loss 0.6977, val loss 0.6877\n",
      "step 73400 of 100000: train_loss 0.6850, val loss 0.6973\n",
      "step 73500 of 100000: train_loss 0.6785, val loss 0.6945\n",
      "step 73600 of 100000: train_loss 0.6848, val loss 0.7218\n",
      "step 73700 of 100000: train_loss 0.6874, val loss 0.7033\n",
      "step 73800 of 100000: train_loss 0.7143, val loss 0.7009\n",
      "step 73900 of 100000: train_loss 0.6919, val loss 0.7015\n",
      "step 74000 of 100000: train_loss 0.6954, val loss 0.7050\n",
      "step 74100 of 100000: train_loss 0.6797, val loss 0.6980\n",
      "step 74200 of 100000: train_loss 0.6778, val loss 0.6989\n",
      "step 74300 of 100000: train_loss 0.6957, val loss 0.7037\n",
      "step 74400 of 100000: train_loss 0.6839, val loss 0.6732\n",
      "step 74500 of 100000: train_loss 0.6992, val loss 0.7051\n",
      "step 74600 of 100000: train_loss 0.6961, val loss 0.6898\n",
      "step 74700 of 100000: train_loss 0.6767, val loss 0.6896\n",
      "step 74800 of 100000: train_loss 0.6977, val loss 0.6961\n",
      "step 74900 of 100000: train_loss 0.6962, val loss 0.7024\n",
      "step 75000 of 100000: train_loss 0.7055, val loss 0.7002\n",
      "step 75100 of 100000: train_loss 0.7208, val loss 0.7208\n",
      "step 75200 of 100000: train_loss 0.6808, val loss 0.6969\n",
      "step 75300 of 100000: train_loss 0.7096, val loss 0.7087\n",
      "step 75400 of 100000: train_loss 0.6882, val loss 0.6979\n",
      "step 75500 of 100000: train_loss 0.6848, val loss 0.7032\n",
      "step 75600 of 100000: train_loss 0.6941, val loss 0.7210\n",
      "step 75700 of 100000: train_loss 0.7104, val loss 0.6988\n",
      "step 75800 of 100000: train_loss 0.6928, val loss 0.6900\n",
      "step 75900 of 100000: train_loss 0.6841, val loss 0.7124\n",
      "step 76000 of 100000: train_loss 0.6804, val loss 0.6859\n",
      "step 76100 of 100000: train_loss 0.7014, val loss 0.6912\n",
      "step 76200 of 100000: train_loss 0.6842, val loss 0.7011\n",
      "step 76300 of 100000: train_loss 0.6967, val loss 0.6849\n",
      "step 76400 of 100000: train_loss 0.7126, val loss 0.7029\n",
      "step 76500 of 100000: train_loss 0.6914, val loss 0.6964\n",
      "step 76600 of 100000: train_loss 0.6836, val loss 0.6915\n",
      "step 76700 of 100000: train_loss 0.6946, val loss 0.7186\n",
      "step 76800 of 100000: train_loss 0.6840, val loss 0.6789\n",
      "step 76900 of 100000: train_loss 0.6872, val loss 0.7025\n",
      "step 77000 of 100000: train_loss 0.6899, val loss 0.7008\n",
      "step 77100 of 100000: train_loss 0.7010, val loss 0.7118\n",
      "step 77200 of 100000: train_loss 0.7052, val loss 0.6854\n",
      "step 77300 of 100000: train_loss 0.6937, val loss 0.6825\n",
      "step 77400 of 100000: train_loss 0.6912, val loss 0.7038\n",
      "step 77500 of 100000: train_loss 0.6745, val loss 0.6971\n",
      "step 77600 of 100000: train_loss 0.6904, val loss 0.6946\n",
      "step 77700 of 100000: train_loss 0.6842, val loss 0.7115\n",
      "step 77800 of 100000: train_loss 0.6896, val loss 0.6995\n",
      "step 77900 of 100000: train_loss 0.6880, val loss 0.7065\n",
      "step 78000 of 100000: train_loss 0.6866, val loss 0.6988\n",
      "step 78100 of 100000: train_loss 0.6867, val loss 0.6959\n",
      "step 78200 of 100000: train_loss 0.7288, val loss 0.7344\n",
      "step 78300 of 100000: train_loss 0.6853, val loss 0.6849\n",
      "step 78400 of 100000: train_loss 0.6924, val loss 0.6803\n",
      "step 78500 of 100000: train_loss 0.6749, val loss 0.6913\n",
      "step 78600 of 100000: train_loss 0.6964, val loss 0.7047\n",
      "step 78700 of 100000: train_loss 0.6978, val loss 0.7054\n",
      "step 78800 of 100000: train_loss 0.6943, val loss 0.7057\n",
      "step 78900 of 100000: train_loss 0.7153, val loss 0.7076\n",
      "step 79000 of 100000: train_loss 0.6999, val loss 0.6970\n",
      "step 79100 of 100000: train_loss 0.7101, val loss 0.7089\n",
      "step 79200 of 100000: train_loss 0.6759, val loss 0.6932\n",
      "step 79300 of 100000: train_loss 0.6734, val loss 0.6941\n",
      "step 79400 of 100000: train_loss 0.7154, val loss 0.6967\n",
      "step 79500 of 100000: train_loss 0.6919, val loss 0.6946\n",
      "step 79600 of 100000: train_loss 0.7058, val loss 0.6962\n",
      "step 79700 of 100000: train_loss 0.6871, val loss 0.7094\n",
      "step 79800 of 100000: train_loss 0.6909, val loss 0.6895\n",
      "step 79900 of 100000: train_loss 0.6788, val loss 0.6813\n",
      "step 80000 of 100000: train_loss 0.6774, val loss 0.6993\n",
      "step 80100 of 100000: train_loss 0.6962, val loss 0.6863\n",
      "step 80200 of 100000: train_loss 0.6960, val loss 0.6983\n",
      "step 80300 of 100000: train_loss 0.6887, val loss 0.6830\n",
      "step 80400 of 100000: train_loss 0.6935, val loss 0.7012\n",
      "step 80500 of 100000: train_loss 0.6927, val loss 0.7078\n",
      "step 80600 of 100000: train_loss 0.6879, val loss 0.6994\n",
      "step 80700 of 100000: train_loss 0.6946, val loss 0.6791\n",
      "step 80800 of 100000: train_loss 0.7188, val loss 0.6912\n",
      "step 80900 of 100000: train_loss 0.7037, val loss 0.7012\n",
      "step 81000 of 100000: train_loss 0.6906, val loss 0.6995\n",
      "step 81100 of 100000: train_loss 0.6861, val loss 0.7137\n",
      "step 81200 of 100000: train_loss 0.6920, val loss 0.6777\n",
      "step 81300 of 100000: train_loss 0.6849, val loss 0.6915\n",
      "step 81400 of 100000: train_loss 0.6729, val loss 0.7069\n",
      "step 81500 of 100000: train_loss 0.6895, val loss 0.6753\n",
      "step 81600 of 100000: train_loss 0.6999, val loss 0.6836\n",
      "step 81700 of 100000: train_loss 0.6767, val loss 0.6997\n",
      "step 81800 of 100000: train_loss 0.7338, val loss 0.7102\n",
      "step 81900 of 100000: train_loss 0.6791, val loss 0.6920\n",
      "step 82000 of 100000: train_loss 0.6970, val loss 0.7105\n",
      "step 82100 of 100000: train_loss 0.7066, val loss 0.7250\n",
      "step 82200 of 100000: train_loss 0.6949, val loss 0.7040\n",
      "step 82300 of 100000: train_loss 0.6881, val loss 0.7216\n",
      "step 82400 of 100000: train_loss 0.7006, val loss 0.6973\n",
      "step 82500 of 100000: train_loss 0.6980, val loss 0.7117\n",
      "step 82600 of 100000: train_loss 0.6818, val loss 0.6978\n",
      "step 82700 of 100000: train_loss 0.7083, val loss 0.6927\n",
      "step 82800 of 100000: train_loss 0.7062, val loss 0.7046\n",
      "step 82900 of 100000: train_loss 0.7052, val loss 0.6945\n",
      "step 83000 of 100000: train_loss 0.6853, val loss 0.6961\n",
      "step 83100 of 100000: train_loss 0.6958, val loss 0.6950\n",
      "step 83200 of 100000: train_loss 0.7016, val loss 0.7017\n",
      "step 83300 of 100000: train_loss 0.6959, val loss 0.6928\n",
      "step 83400 of 100000: train_loss 0.7009, val loss 0.7088\n",
      "step 83500 of 100000: train_loss 0.6955, val loss 0.6991\n",
      "step 83600 of 100000: train_loss 0.6929, val loss 0.6891\n",
      "step 83700 of 100000: train_loss 0.6797, val loss 0.6976\n",
      "step 83800 of 100000: train_loss 0.6935, val loss 0.6961\n",
      "step 83900 of 100000: train_loss 0.6837, val loss 0.6831\n",
      "step 84000 of 100000: train_loss 0.6919, val loss 0.6952\n",
      "step 84100 of 100000: train_loss 0.6798, val loss 0.6952\n",
      "step 84200 of 100000: train_loss 0.7019, val loss 0.6890\n",
      "step 84300 of 100000: train_loss 0.6927, val loss 0.7269\n",
      "step 84400 of 100000: train_loss 0.6915, val loss 0.7103\n",
      "step 84500 of 100000: train_loss 0.7080, val loss 0.6916\n",
      "step 84600 of 100000: train_loss 0.7137, val loss 0.7117\n",
      "step 84700 of 100000: train_loss 0.6856, val loss 0.6898\n",
      "step 84800 of 100000: train_loss 0.7086, val loss 0.6999\n",
      "step 84900 of 100000: train_loss 0.6721, val loss 0.6932\n",
      "step 85000 of 100000: train_loss 0.6759, val loss 0.6918\n",
      "step 85100 of 100000: train_loss 0.6858, val loss 0.6965\n",
      "step 85200 of 100000: train_loss 0.6992, val loss 0.6931\n",
      "step 85300 of 100000: train_loss 0.6872, val loss 0.6946\n",
      "step 85400 of 100000: train_loss 0.6775, val loss 0.7164\n",
      "step 85500 of 100000: train_loss 0.6823, val loss 0.6888\n",
      "step 85600 of 100000: train_loss 0.6968, val loss 0.6682\n",
      "step 85700 of 100000: train_loss 0.6939, val loss 0.6975\n",
      "step 85800 of 100000: train_loss 0.6917, val loss 0.6912\n",
      "step 85900 of 100000: train_loss 0.6932, val loss 0.6911\n",
      "step 86000 of 100000: train_loss 0.6817, val loss 0.6856\n",
      "step 86100 of 100000: train_loss 0.7117, val loss 0.6998\n",
      "step 86200 of 100000: train_loss 0.6932, val loss 0.7042\n",
      "step 86300 of 100000: train_loss 0.6906, val loss 0.6849\n",
      "step 86400 of 100000: train_loss 0.6940, val loss 0.7051\n",
      "step 86500 of 100000: train_loss 0.6754, val loss 0.7066\n",
      "step 86600 of 100000: train_loss 0.6780, val loss 0.6950\n",
      "step 86700 of 100000: train_loss 0.6946, val loss 0.6993\n",
      "step 86800 of 100000: train_loss 0.7029, val loss 0.6903\n",
      "step 86900 of 100000: train_loss 0.7060, val loss 0.6919\n",
      "step 87000 of 100000: train_loss 0.6797, val loss 0.6784\n",
      "step 87100 of 100000: train_loss 0.6655, val loss 0.7045\n",
      "step 87200 of 100000: train_loss 0.6788, val loss 0.6767\n",
      "step 87300 of 100000: train_loss 0.7217, val loss 0.7221\n",
      "step 87400 of 100000: train_loss 0.6825, val loss 0.6884\n",
      "step 87500 of 100000: train_loss 0.6764, val loss 0.7058\n",
      "step 87600 of 100000: train_loss 0.7018, val loss 0.6814\n",
      "step 87700 of 100000: train_loss 0.7005, val loss 0.6920\n",
      "step 87800 of 100000: train_loss 0.6924, val loss 0.7074\n",
      "step 87900 of 100000: train_loss 0.6784, val loss 0.6983\n",
      "step 88000 of 100000: train_loss 0.6799, val loss 0.6961\n",
      "step 88100 of 100000: train_loss 0.6906, val loss 0.6915\n",
      "step 88200 of 100000: train_loss 0.6825, val loss 0.6999\n",
      "step 88300 of 100000: train_loss 0.7238, val loss 0.7138\n",
      "step 88400 of 100000: train_loss 0.6875, val loss 0.6970\n",
      "step 88500 of 100000: train_loss 0.6703, val loss 0.6791\n",
      "step 88600 of 100000: train_loss 0.6858, val loss 0.6942\n",
      "step 88700 of 100000: train_loss 0.7177, val loss 0.7218\n",
      "step 88800 of 100000: train_loss 0.6944, val loss 0.7090\n",
      "step 88900 of 100000: train_loss 0.6759, val loss 0.7119\n",
      "step 89000 of 100000: train_loss 0.6801, val loss 0.6925\n",
      "step 89100 of 100000: train_loss 0.6934, val loss 0.6903\n",
      "step 89200 of 100000: train_loss 0.7065, val loss 0.7034\n",
      "step 89300 of 100000: train_loss 0.6972, val loss 0.7040\n",
      "step 89400 of 100000: train_loss 0.6975, val loss 0.7012\n",
      "step 89500 of 100000: train_loss 0.7126, val loss 0.7096\n",
      "step 89600 of 100000: train_loss 0.6949, val loss 0.6959\n",
      "step 89700 of 100000: train_loss 0.7194, val loss 0.7223\n",
      "step 89800 of 100000: train_loss 0.6812, val loss 0.7178\n",
      "step 89900 of 100000: train_loss 0.6678, val loss 0.7030\n",
      "step 90000 of 100000: train_loss 0.7029, val loss 0.7149\n",
      "step 90100 of 100000: train_loss 0.7047, val loss 0.7243\n",
      "step 90200 of 100000: train_loss 0.6892, val loss 0.6924\n",
      "step 90300 of 100000: train_loss 0.7134, val loss 0.7057\n",
      "step 90400 of 100000: train_loss 0.6765, val loss 0.6886\n",
      "step 90500 of 100000: train_loss 0.6869, val loss 0.6894\n",
      "step 90600 of 100000: train_loss 0.7019, val loss 0.7056\n",
      "step 90700 of 100000: train_loss 0.6814, val loss 0.6895\n",
      "step 90800 of 100000: train_loss 0.7058, val loss 0.7154\n",
      "step 90900 of 100000: train_loss 0.6795, val loss 0.6986\n",
      "step 91000 of 100000: train_loss 0.6929, val loss 0.7078\n",
      "step 91100 of 100000: train_loss 0.7021, val loss 0.7104\n",
      "step 91200 of 100000: train_loss 0.6806, val loss 0.7015\n",
      "step 91300 of 100000: train_loss 0.6885, val loss 0.7089\n",
      "step 91400 of 100000: train_loss 0.6870, val loss 0.6762\n",
      "step 91500 of 100000: train_loss 0.6759, val loss 0.7127\n",
      "step 91600 of 100000: train_loss 0.6805, val loss 0.7008\n",
      "step 91700 of 100000: train_loss 0.6702, val loss 0.6995\n",
      "step 91800 of 100000: train_loss 0.7054, val loss 0.7073\n",
      "step 91900 of 100000: train_loss 0.7184, val loss 0.7119\n",
      "step 92000 of 100000: train_loss 0.7060, val loss 0.7026\n",
      "step 92100 of 100000: train_loss 0.6962, val loss 0.6950\n",
      "step 92200 of 100000: train_loss 0.6918, val loss 0.7187\n",
      "step 92300 of 100000: train_loss 0.6777, val loss 0.7055\n",
      "step 92400 of 100000: train_loss 0.7014, val loss 0.6955\n",
      "step 92500 of 100000: train_loss 0.6863, val loss 0.7225\n",
      "step 92600 of 100000: train_loss 0.6888, val loss 0.7031\n",
      "step 92700 of 100000: train_loss 0.6942, val loss 0.6875\n",
      "step 92800 of 100000: train_loss 0.7116, val loss 0.7054\n",
      "step 92900 of 100000: train_loss 0.6748, val loss 0.6863\n",
      "step 93000 of 100000: train_loss 0.6923, val loss 0.6819\n",
      "step 93100 of 100000: train_loss 0.6955, val loss 0.7147\n",
      "step 93200 of 100000: train_loss 0.6944, val loss 0.6920\n",
      "step 93300 of 100000: train_loss 0.6927, val loss 0.6800\n",
      "step 93400 of 100000: train_loss 0.6935, val loss 0.6888\n",
      "step 93500 of 100000: train_loss 0.7477, val loss 0.7423\n",
      "step 93600 of 100000: train_loss 0.6870, val loss 0.6995\n",
      "step 93700 of 100000: train_loss 0.7045, val loss 0.6781\n",
      "step 93800 of 100000: train_loss 0.6951, val loss 0.7226\n",
      "step 93900 of 100000: train_loss 0.6871, val loss 0.6828\n",
      "step 94000 of 100000: train_loss 0.7196, val loss 0.7223\n",
      "step 94100 of 100000: train_loss 0.6798, val loss 0.6908\n",
      "step 94200 of 100000: train_loss 0.7081, val loss 0.7104\n",
      "step 94300 of 100000: train_loss 0.6999, val loss 0.6933\n",
      "step 94400 of 100000: train_loss 0.6720, val loss 0.6960\n",
      "step 94500 of 100000: train_loss 0.6901, val loss 0.6777\n",
      "step 94600 of 100000: train_loss 0.6885, val loss 0.7054\n",
      "step 94700 of 100000: train_loss 0.6838, val loss 0.7107\n",
      "step 94800 of 100000: train_loss 0.6896, val loss 0.6822\n",
      "step 94900 of 100000: train_loss 0.6728, val loss 0.7092\n",
      "step 95000 of 100000: train_loss 0.7001, val loss 0.7128\n",
      "step 95100 of 100000: train_loss 0.6892, val loss 0.6988\n",
      "step 95200 of 100000: train_loss 0.6772, val loss 0.7004\n",
      "step 95300 of 100000: train_loss 0.6902, val loss 0.7121\n",
      "step 95400 of 100000: train_loss 0.7042, val loss 0.6846\n",
      "step 95500 of 100000: train_loss 0.6835, val loss 0.6954\n",
      "step 95600 of 100000: train_loss 0.6905, val loss 0.6983\n",
      "step 95700 of 100000: train_loss 0.7069, val loss 0.7391\n",
      "step 95800 of 100000: train_loss 0.6917, val loss 0.7070\n",
      "step 95900 of 100000: train_loss 0.6681, val loss 0.6919\n",
      "step 96000 of 100000: train_loss 0.7154, val loss 0.7074\n",
      "step 96100 of 100000: train_loss 0.6857, val loss 0.7015\n",
      "step 96200 of 100000: train_loss 0.6693, val loss 0.6936\n",
      "step 96300 of 100000: train_loss 0.6934, val loss 0.6937\n",
      "step 96400 of 100000: train_loss 0.6923, val loss 0.7043\n",
      "step 96500 of 100000: train_loss 0.6925, val loss 0.6937\n",
      "step 96600 of 100000: train_loss 0.6737, val loss 0.6864\n",
      "step 96700 of 100000: train_loss 0.6761, val loss 0.6888\n",
      "step 96800 of 100000: train_loss 0.7007, val loss 0.7033\n",
      "step 96900 of 100000: train_loss 0.6940, val loss 0.6888\n",
      "step 97000 of 100000: train_loss 0.6901, val loss 0.6755\n",
      "step 97100 of 100000: train_loss 0.6803, val loss 0.6916\n",
      "step 97200 of 100000: train_loss 0.6712, val loss 0.6751\n",
      "step 97300 of 100000: train_loss 0.6713, val loss 0.6888\n",
      "step 97400 of 100000: train_loss 0.6915, val loss 0.6982\n",
      "step 97500 of 100000: train_loss 0.6798, val loss 0.6948\n",
      "step 97600 of 100000: train_loss 0.6876, val loss 0.6886\n",
      "step 97700 of 100000: train_loss 0.6856, val loss 0.6849\n",
      "step 97800 of 100000: train_loss 0.6962, val loss 0.6996\n",
      "step 97900 of 100000: train_loss 0.6860, val loss 0.6908\n",
      "step 98000 of 100000: train_loss 0.6992, val loss 0.6959\n",
      "step 98100 of 100000: train_loss 0.6728, val loss 0.6888\n",
      "step 98200 of 100000: train_loss 0.7038, val loss 0.7295\n",
      "step 98300 of 100000: train_loss 0.6761, val loss 0.6886\n",
      "step 98400 of 100000: train_loss 0.6936, val loss 0.6901\n",
      "step 98500 of 100000: train_loss 0.6955, val loss 0.6905\n",
      "step 98600 of 100000: train_loss 0.6966, val loss 0.6940\n",
      "step 98700 of 100000: train_loss 0.6933, val loss 0.6984\n",
      "step 98800 of 100000: train_loss 0.6966, val loss 0.6986\n",
      "step 98900 of 100000: train_loss 0.6953, val loss 0.7001\n",
      "step 99000 of 100000: train_loss 0.7093, val loss 0.7129\n",
      "step 99100 of 100000: train_loss 0.6973, val loss 0.6922\n",
      "step 99200 of 100000: train_loss 0.6834, val loss 0.6873\n",
      "step 99300 of 100000: train_loss 0.6951, val loss 0.6893\n",
      "step 99400 of 100000: train_loss 0.6813, val loss 0.7150\n",
      "step 99500 of 100000: train_loss 0.6800, val loss 0.6869\n",
      "step 99600 of 100000: train_loss 0.6804, val loss 0.7058\n",
      "step 99700 of 100000: train_loss 0.7069, val loss 0.6940\n",
      "step 99800 of 100000: train_loss 0.7473, val loss 0.7491\n",
      "step 99900 of 100000: train_loss 0.7244, val loss 0.7049\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(0, len(all_data1))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data1[train_inds]\n",
    "val_data = all_data1[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "\n",
    "input_dim = all_data1.shape[2]\n",
    "\n",
    "model1 = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx1,\n",
    "                causal_ordering=causal_ordering1,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types=var_types1,\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model1.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model1(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model1.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model1(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model1.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545ff6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [1.36] est ATE: [0.41333425] error: [0.94666575]\n",
      "Mean Squared Error Across All Vars: tensor(1.8704)\n",
      "Mean Squared Error Across Outcome: tensor(0.6755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "   \n",
    "model1.eval()\n",
    "inf = inference.CausalInference(model=model1, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X1':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X1':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names1.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data1, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data1, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model1(train_data.to(device))\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1].detach().cpu().numpy())\n",
    "print('Mean Squared Error Across All Vars:', ((train_data - preds.detach().cpu())**2).mean())\n",
    "print('Mean Squared Error Across Outcome:', ((train_data[:,effect_index,:] - preds[:,effect_index,:].detach().cpu())**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f6d34",
   "metadata": {},
   "source": [
    "## Incorrect DAG 2 (missing and reverse edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e280e550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 100000: train_loss 4.8074, val loss 4.7529\n",
      "step 100 of 100000: train_loss 2.2040, val loss 2.1906\n",
      "step 200 of 100000: train_loss 1.8700, val loss 1.8562\n",
      "step 300 of 100000: train_loss 1.7606, val loss 1.7790\n",
      "step 400 of 100000: train_loss 1.7314, val loss 1.7692\n",
      "step 500 of 100000: train_loss 1.7488, val loss 1.7697\n",
      "step 600 of 100000: train_loss 1.7325, val loss 1.7176\n",
      "step 700 of 100000: train_loss 1.7661, val loss 1.7250\n",
      "step 800 of 100000: train_loss 1.7229, val loss 1.7408\n",
      "step 900 of 100000: train_loss 1.7295, val loss 1.7581\n",
      "step 1000 of 100000: train_loss 1.7567, val loss 1.7457\n",
      "step 1100 of 100000: train_loss 1.7451, val loss 1.7356\n",
      "step 1200 of 100000: train_loss 1.7394, val loss 1.7209\n",
      "step 1300 of 100000: train_loss 1.7344, val loss 1.7203\n",
      "step 1400 of 100000: train_loss 1.7130, val loss 1.7385\n",
      "step 1500 of 100000: train_loss 1.7139, val loss 1.7419\n",
      "step 1600 of 100000: train_loss 1.7192, val loss 1.7246\n",
      "step 1700 of 100000: train_loss 1.7733, val loss 1.7748\n",
      "step 1800 of 100000: train_loss 1.7541, val loss 1.7143\n",
      "step 1900 of 100000: train_loss 1.7179, val loss 1.7109\n",
      "step 2000 of 100000: train_loss 1.7484, val loss 1.7269\n",
      "step 2100 of 100000: train_loss 1.7416, val loss 1.7169\n",
      "step 2200 of 100000: train_loss 1.7274, val loss 1.7161\n",
      "step 2300 of 100000: train_loss 1.7532, val loss 1.7131\n",
      "step 2400 of 100000: train_loss 1.7301, val loss 1.7230\n",
      "step 2500 of 100000: train_loss 1.7012, val loss 1.7270\n",
      "step 2600 of 100000: train_loss 1.7331, val loss 1.7167\n",
      "step 2700 of 100000: train_loss 1.7126, val loss 1.7148\n",
      "step 2800 of 100000: train_loss 1.7132, val loss 1.7449\n",
      "step 2900 of 100000: train_loss 1.7488, val loss 1.6973\n",
      "step 3000 of 100000: train_loss 1.7500, val loss 1.7244\n",
      "step 3100 of 100000: train_loss 1.7417, val loss 1.7371\n",
      "step 3200 of 100000: train_loss 1.7247, val loss 1.7420\n",
      "step 3300 of 100000: train_loss 1.7225, val loss 1.7175\n",
      "step 3400 of 100000: train_loss 1.7171, val loss 1.7656\n",
      "step 3500 of 100000: train_loss 1.7299, val loss 1.7157\n",
      "step 3600 of 100000: train_loss 1.7359, val loss 1.7086\n",
      "step 3700 of 100000: train_loss 1.7340, val loss 1.7320\n",
      "step 3800 of 100000: train_loss 1.7252, val loss 1.6914\n",
      "step 3900 of 100000: train_loss 1.7079, val loss 1.7255\n",
      "step 4000 of 100000: train_loss 1.7093, val loss 1.7145\n",
      "step 4100 of 100000: train_loss 1.7229, val loss 1.7352\n",
      "step 4200 of 100000: train_loss 1.7152, val loss 1.6952\n",
      "step 4300 of 100000: train_loss 1.6934, val loss 1.7320\n",
      "step 4400 of 100000: train_loss 1.7247, val loss 1.6967\n",
      "step 4500 of 100000: train_loss 1.7066, val loss 1.7441\n",
      "step 4600 of 100000: train_loss 1.7014, val loss 1.7092\n",
      "step 4700 of 100000: train_loss 1.7363, val loss 1.6722\n",
      "step 4800 of 100000: train_loss 1.7217, val loss 1.7285\n",
      "step 4900 of 100000: train_loss 1.7201, val loss 1.7024\n",
      "step 5000 of 100000: train_loss 1.7274, val loss 1.7495\n",
      "step 5100 of 100000: train_loss 1.7241, val loss 1.7086\n",
      "step 5200 of 100000: train_loss 1.7311, val loss 1.6948\n",
      "step 5300 of 100000: train_loss 1.7318, val loss 1.7324\n",
      "step 5400 of 100000: train_loss 1.7355, val loss 1.7027\n",
      "step 5500 of 100000: train_loss 1.7207, val loss 1.6910\n",
      "step 5600 of 100000: train_loss 1.7372, val loss 1.7041\n",
      "step 5700 of 100000: train_loss 1.7504, val loss 1.6919\n",
      "step 5800 of 100000: train_loss 1.7356, val loss 1.7648\n",
      "step 5900 of 100000: train_loss 1.7092, val loss 1.7052\n",
      "step 6000 of 100000: train_loss 1.7136, val loss 1.7296\n",
      "step 6100 of 100000: train_loss 1.7221, val loss 1.7040\n",
      "step 6200 of 100000: train_loss 1.7430, val loss 1.6966\n",
      "step 6300 of 100000: train_loss 1.7154, val loss 1.6934\n",
      "step 6400 of 100000: train_loss 1.6972, val loss 1.6995\n",
      "step 6500 of 100000: train_loss 1.7342, val loss 1.7257\n",
      "step 6600 of 100000: train_loss 1.7086, val loss 1.7191\n",
      "step 6700 of 100000: train_loss 1.6878, val loss 1.7036\n",
      "step 6800 of 100000: train_loss 1.7133, val loss 1.7420\n",
      "step 6900 of 100000: train_loss 1.7494, val loss 1.7264\n",
      "step 7000 of 100000: train_loss 1.7185, val loss 1.7069\n",
      "step 7100 of 100000: train_loss 1.7334, val loss 1.7239\n",
      "step 7200 of 100000: train_loss 1.7076, val loss 1.6918\n",
      "step 7300 of 100000: train_loss 1.6999, val loss 1.7086\n",
      "step 7400 of 100000: train_loss 1.7489, val loss 1.7171\n",
      "step 7500 of 100000: train_loss 1.7142, val loss 1.7183\n",
      "step 7600 of 100000: train_loss 1.7168, val loss 1.7141\n",
      "step 7700 of 100000: train_loss 1.6943, val loss 1.7190\n",
      "step 7800 of 100000: train_loss 1.7042, val loss 1.7132\n",
      "step 7900 of 100000: train_loss 1.7006, val loss 1.7070\n",
      "step 8000 of 100000: train_loss 1.7129, val loss 1.7052\n",
      "step 8100 of 100000: train_loss 1.7350, val loss 1.6928\n",
      "step 8200 of 100000: train_loss 1.7188, val loss 1.6877\n",
      "step 8300 of 100000: train_loss 1.7131, val loss 1.7050\n",
      "step 8400 of 100000: train_loss 1.7351, val loss 1.7276\n",
      "step 8500 of 100000: train_loss 1.7304, val loss 1.7252\n",
      "step 8600 of 100000: train_loss 1.7256, val loss 1.6834\n",
      "step 8700 of 100000: train_loss 1.7054, val loss 1.7155\n",
      "step 8800 of 100000: train_loss 1.7459, val loss 1.7143\n",
      "step 8900 of 100000: train_loss 1.7014, val loss 1.7266\n",
      "step 9000 of 100000: train_loss 1.7179, val loss 1.6920\n",
      "step 9100 of 100000: train_loss 1.7138, val loss 1.7227\n",
      "step 9200 of 100000: train_loss 1.7160, val loss 1.6996\n",
      "step 9300 of 100000: train_loss 1.7596, val loss 1.7270\n",
      "step 9400 of 100000: train_loss 1.7203, val loss 1.7156\n",
      "step 9500 of 100000: train_loss 1.6868, val loss 1.7025\n",
      "step 9600 of 100000: train_loss 1.7259, val loss 1.6954\n",
      "step 9700 of 100000: train_loss 1.7334, val loss 1.6991\n",
      "step 9800 of 100000: train_loss 1.7198, val loss 1.7027\n",
      "step 9900 of 100000: train_loss 1.6949, val loss 1.6914\n",
      "step 10000 of 100000: train_loss 1.7206, val loss 1.6885\n",
      "step 10100 of 100000: train_loss 1.7132, val loss 1.7144\n",
      "step 10200 of 100000: train_loss 1.6834, val loss 1.7204\n",
      "step 10300 of 100000: train_loss 1.7061, val loss 1.6713\n",
      "step 10400 of 100000: train_loss 1.7227, val loss 1.7248\n",
      "step 10500 of 100000: train_loss 1.7105, val loss 1.6858\n",
      "step 10600 of 100000: train_loss 1.7050, val loss 1.7311\n",
      "step 10700 of 100000: train_loss 1.7080, val loss 1.7104\n",
      "step 10800 of 100000: train_loss 1.6940, val loss 1.6847\n",
      "step 10900 of 100000: train_loss 1.7079, val loss 1.7240\n",
      "step 11000 of 100000: train_loss 1.6652, val loss 1.7223\n",
      "step 11100 of 100000: train_loss 1.7349, val loss 1.6958\n",
      "step 11200 of 100000: train_loss 1.7047, val loss 1.7029\n",
      "step 11300 of 100000: train_loss 1.7328, val loss 1.7016\n",
      "step 11400 of 100000: train_loss 1.7261, val loss 1.7093\n",
      "step 11500 of 100000: train_loss 1.7154, val loss 1.6884\n",
      "step 11600 of 100000: train_loss 1.7773, val loss 1.7229\n",
      "step 11700 of 100000: train_loss 1.7048, val loss 1.7294\n",
      "step 11800 of 100000: train_loss 1.7379, val loss 1.7098\n",
      "step 11900 of 100000: train_loss 1.7105, val loss 1.7070\n",
      "step 12000 of 100000: train_loss 1.7346, val loss 1.7188\n",
      "step 12100 of 100000: train_loss 1.6856, val loss 1.6991\n",
      "step 12200 of 100000: train_loss 1.7352, val loss 1.7036\n",
      "step 12300 of 100000: train_loss 1.7351, val loss 1.7082\n",
      "step 12400 of 100000: train_loss 1.6752, val loss 1.6968\n",
      "step 12500 of 100000: train_loss 1.6952, val loss 1.7247\n",
      "step 12600 of 100000: train_loss 1.7427, val loss 1.6948\n",
      "step 12700 of 100000: train_loss 1.7042, val loss 1.6809\n",
      "step 12800 of 100000: train_loss 1.6967, val loss 1.7013\n",
      "step 12900 of 100000: train_loss 1.7084, val loss 1.7019\n",
      "step 13000 of 100000: train_loss 1.7104, val loss 1.7371\n",
      "step 13100 of 100000: train_loss 1.7240, val loss 1.7066\n",
      "step 13200 of 100000: train_loss 1.7281, val loss 1.7244\n",
      "step 13300 of 100000: train_loss 1.7101, val loss 1.6951\n",
      "step 13400 of 100000: train_loss 1.7288, val loss 1.7083\n",
      "step 13500 of 100000: train_loss 1.7126, val loss 1.7112\n",
      "step 13600 of 100000: train_loss 1.7194, val loss 1.7165\n",
      "step 13700 of 100000: train_loss 1.7084, val loss 1.6849\n",
      "step 13800 of 100000: train_loss 1.6916, val loss 1.7077\n",
      "step 13900 of 100000: train_loss 1.7512, val loss 1.7130\n",
      "step 14000 of 100000: train_loss 1.7354, val loss 1.7145\n",
      "step 14100 of 100000: train_loss 1.6746, val loss 1.7001\n",
      "step 14200 of 100000: train_loss 1.6868, val loss 1.7411\n",
      "step 14300 of 100000: train_loss 1.7188, val loss 1.7101\n",
      "step 14400 of 100000: train_loss 1.7067, val loss 1.7061\n",
      "step 14500 of 100000: train_loss 1.7100, val loss 1.6563\n",
      "step 14600 of 100000: train_loss 1.7184, val loss 1.7297\n",
      "step 14700 of 100000: train_loss 1.7457, val loss 1.7219\n",
      "step 14800 of 100000: train_loss 1.7084, val loss 1.7022\n",
      "step 14900 of 100000: train_loss 1.7011, val loss 1.7096\n",
      "step 15000 of 100000: train_loss 1.7032, val loss 1.7075\n",
      "step 15100 of 100000: train_loss 1.7100, val loss 1.6977\n",
      "step 15200 of 100000: train_loss 1.7019, val loss 1.7099\n",
      "step 15300 of 100000: train_loss 1.7437, val loss 1.7163\n",
      "step 15400 of 100000: train_loss 1.7285, val loss 1.7143\n",
      "step 15500 of 100000: train_loss 1.7082, val loss 1.7251\n",
      "step 15600 of 100000: train_loss 1.7404, val loss 1.7115\n",
      "step 15700 of 100000: train_loss 1.7002, val loss 1.7128\n",
      "step 15800 of 100000: train_loss 1.7423, val loss 1.6955\n",
      "step 15900 of 100000: train_loss 1.7130, val loss 1.6984\n",
      "step 16000 of 100000: train_loss 1.6998, val loss 1.6974\n",
      "step 16100 of 100000: train_loss 1.7082, val loss 1.7298\n",
      "step 16200 of 100000: train_loss 1.7059, val loss 1.7142\n",
      "step 16300 of 100000: train_loss 1.7029, val loss 1.7071\n",
      "step 16400 of 100000: train_loss 1.7240, val loss 1.7194\n",
      "step 16500 of 100000: train_loss 1.7341, val loss 1.6962\n",
      "step 16600 of 100000: train_loss 1.7039, val loss 1.6881\n",
      "step 16700 of 100000: train_loss 1.7196, val loss 1.7195\n",
      "step 16800 of 100000: train_loss 1.7343, val loss 1.7030\n",
      "step 16900 of 100000: train_loss 1.7084, val loss 1.7050\n",
      "step 17000 of 100000: train_loss 1.7059, val loss 1.7214\n",
      "step 17100 of 100000: train_loss 1.7299, val loss 1.7010\n",
      "step 17200 of 100000: train_loss 1.6934, val loss 1.7205\n",
      "step 17300 of 100000: train_loss 1.7429, val loss 1.7170\n",
      "step 17400 of 100000: train_loss 1.7247, val loss 1.7284\n",
      "step 17500 of 100000: train_loss 1.7215, val loss 1.7025\n",
      "step 17600 of 100000: train_loss 1.7281, val loss 1.7217\n",
      "step 17700 of 100000: train_loss 1.6944, val loss 1.7168\n",
      "step 17800 of 100000: train_loss 1.7208, val loss 1.6783\n",
      "step 17900 of 100000: train_loss 1.7126, val loss 1.7290\n",
      "step 18000 of 100000: train_loss 1.7156, val loss 1.6766\n",
      "step 18100 of 100000: train_loss 1.7387, val loss 1.6967\n",
      "step 18200 of 100000: train_loss 1.7327, val loss 1.7015\n",
      "step 18300 of 100000: train_loss 1.7159, val loss 1.6946\n",
      "step 18400 of 100000: train_loss 1.7281, val loss 1.6861\n",
      "step 18500 of 100000: train_loss 1.7159, val loss 1.6922\n",
      "step 18600 of 100000: train_loss 1.7021, val loss 1.6955\n",
      "step 18700 of 100000: train_loss 1.7392, val loss 1.7300\n",
      "step 18800 of 100000: train_loss 1.6765, val loss 1.6865\n",
      "step 18900 of 100000: train_loss 1.7495, val loss 1.6979\n",
      "step 19000 of 100000: train_loss 1.6822, val loss 1.6911\n",
      "step 19100 of 100000: train_loss 1.7397, val loss 1.7040\n",
      "step 19200 of 100000: train_loss 1.7255, val loss 1.7006\n",
      "step 19300 of 100000: train_loss 1.7087, val loss 1.7024\n",
      "step 19400 of 100000: train_loss 1.7241, val loss 1.7058\n",
      "step 19500 of 100000: train_loss 1.7287, val loss 1.7004\n",
      "step 19600 of 100000: train_loss 1.6849, val loss 1.7204\n",
      "step 19700 of 100000: train_loss 1.6867, val loss 1.7357\n",
      "step 19800 of 100000: train_loss 1.7160, val loss 1.7401\n",
      "step 19900 of 100000: train_loss 1.7068, val loss 1.7086\n",
      "step 20000 of 100000: train_loss 1.7259, val loss 1.6793\n",
      "step 20100 of 100000: train_loss 1.6983, val loss 1.6765\n",
      "step 20200 of 100000: train_loss 1.7010, val loss 1.7282\n",
      "step 20300 of 100000: train_loss 1.7147, val loss 1.7191\n",
      "step 20400 of 100000: train_loss 1.7140, val loss 1.7158\n",
      "step 20500 of 100000: train_loss 1.6846, val loss 1.6817\n",
      "step 20600 of 100000: train_loss 1.6926, val loss 1.6993\n",
      "step 20700 of 100000: train_loss 1.7107, val loss 1.7299\n",
      "step 20800 of 100000: train_loss 1.6858, val loss 1.7380\n",
      "step 20900 of 100000: train_loss 1.6789, val loss 1.7327\n",
      "step 21000 of 100000: train_loss 1.7467, val loss 1.7125\n",
      "step 21100 of 100000: train_loss 1.7009, val loss 1.7151\n",
      "step 21200 of 100000: train_loss 1.7237, val loss 1.7228\n",
      "step 21300 of 100000: train_loss 1.7105, val loss 1.6756\n",
      "step 21400 of 100000: train_loss 1.6810, val loss 1.6789\n",
      "step 21500 of 100000: train_loss 1.6887, val loss 1.7151\n",
      "step 21600 of 100000: train_loss 1.7376, val loss 1.7028\n",
      "step 21700 of 100000: train_loss 1.7342, val loss 1.6902\n",
      "step 21800 of 100000: train_loss 1.7134, val loss 1.7137\n",
      "step 21900 of 100000: train_loss 1.7246, val loss 1.6880\n",
      "step 22000 of 100000: train_loss 1.7189, val loss 1.7358\n",
      "step 22100 of 100000: train_loss 1.6991, val loss 1.7170\n",
      "step 22200 of 100000: train_loss 1.7244, val loss 1.7027\n",
      "step 22300 of 100000: train_loss 1.7015, val loss 1.7103\n",
      "step 22400 of 100000: train_loss 1.6962, val loss 1.6940\n",
      "step 22500 of 100000: train_loss 1.7142, val loss 1.7206\n",
      "step 22600 of 100000: train_loss 1.7128, val loss 1.7081\n",
      "step 22700 of 100000: train_loss 1.7253, val loss 1.6742\n",
      "step 22800 of 100000: train_loss 1.6986, val loss 1.7199\n",
      "step 22900 of 100000: train_loss 1.7417, val loss 1.7295\n",
      "step 23000 of 100000: train_loss 1.7402, val loss 1.6672\n",
      "step 23100 of 100000: train_loss 1.7159, val loss 1.7233\n",
      "step 23200 of 100000: train_loss 1.7545, val loss 1.6986\n",
      "step 23300 of 100000: train_loss 1.7159, val loss 1.7140\n",
      "step 23400 of 100000: train_loss 1.6765, val loss 1.7081\n",
      "step 23500 of 100000: train_loss 1.7344, val loss 1.7146\n",
      "step 23600 of 100000: train_loss 1.7087, val loss 1.7035\n",
      "step 23700 of 100000: train_loss 1.7106, val loss 1.7171\n",
      "step 23800 of 100000: train_loss 1.6882, val loss 1.6829\n",
      "step 23900 of 100000: train_loss 1.7340, val loss 1.7495\n",
      "step 24000 of 100000: train_loss 1.7035, val loss 1.6417\n",
      "step 24100 of 100000: train_loss 1.7341, val loss 1.7104\n",
      "step 24200 of 100000: train_loss 1.7342, val loss 1.7240\n",
      "step 24300 of 100000: train_loss 1.7300, val loss 1.6788\n",
      "step 24400 of 100000: train_loss 1.6926, val loss 1.6775\n",
      "step 24500 of 100000: train_loss 1.7304, val loss 1.6961\n",
      "step 24600 of 100000: train_loss 1.7315, val loss 1.7147\n",
      "step 24700 of 100000: train_loss 1.7494, val loss 1.6969\n",
      "step 24800 of 100000: train_loss 1.7119, val loss 1.7047\n",
      "step 24900 of 100000: train_loss 1.6884, val loss 1.7060\n",
      "step 25000 of 100000: train_loss 1.6831, val loss 1.6932\n",
      "step 25100 of 100000: train_loss 1.7137, val loss 1.6800\n",
      "step 25200 of 100000: train_loss 1.7116, val loss 1.7341\n",
      "step 25300 of 100000: train_loss 1.7030, val loss 1.7139\n",
      "step 25400 of 100000: train_loss 1.7255, val loss 1.6927\n",
      "step 25500 of 100000: train_loss 1.7058, val loss 1.6729\n",
      "step 25600 of 100000: train_loss 1.7411, val loss 1.6899\n",
      "step 25700 of 100000: train_loss 1.7069, val loss 1.7245\n",
      "step 25800 of 100000: train_loss 1.6925, val loss 1.6966\n",
      "step 25900 of 100000: train_loss 1.7207, val loss 1.7083\n",
      "step 26000 of 100000: train_loss 1.7111, val loss 1.6877\n",
      "step 26100 of 100000: train_loss 1.7003, val loss 1.6923\n",
      "step 26200 of 100000: train_loss 1.7203, val loss 1.6959\n",
      "step 26300 of 100000: train_loss 1.7248, val loss 1.6895\n",
      "step 26400 of 100000: train_loss 1.7076, val loss 1.7055\n",
      "step 26500 of 100000: train_loss 1.6906, val loss 1.7174\n",
      "step 26600 of 100000: train_loss 1.7012, val loss 1.7083\n",
      "step 26700 of 100000: train_loss 1.6761, val loss 1.7000\n",
      "step 26800 of 100000: train_loss 1.7033, val loss 1.7179\n",
      "step 26900 of 100000: train_loss 1.7130, val loss 1.7085\n",
      "step 27000 of 100000: train_loss 1.6998, val loss 1.7256\n",
      "step 27100 of 100000: train_loss 1.7200, val loss 1.7039\n",
      "step 27200 of 100000: train_loss 1.7310, val loss 1.7455\n",
      "step 27300 of 100000: train_loss 1.7173, val loss 1.7122\n",
      "step 27400 of 100000: train_loss 1.7386, val loss 1.7057\n",
      "step 27500 of 100000: train_loss 1.6947, val loss 1.7154\n",
      "step 27600 of 100000: train_loss 1.7271, val loss 1.6819\n",
      "step 27700 of 100000: train_loss 1.7083, val loss 1.7024\n",
      "step 27800 of 100000: train_loss 1.7012, val loss 1.7061\n",
      "step 27900 of 100000: train_loss 1.6849, val loss 1.7291\n",
      "step 28000 of 100000: train_loss 1.7328, val loss 1.7303\n",
      "step 28100 of 100000: train_loss 1.7314, val loss 1.7449\n",
      "step 28200 of 100000: train_loss 1.7429, val loss 1.7096\n",
      "step 28300 of 100000: train_loss 1.7072, val loss 1.7062\n",
      "step 28400 of 100000: train_loss 1.7379, val loss 1.6923\n",
      "step 28500 of 100000: train_loss 1.7230, val loss 1.7093\n",
      "step 28600 of 100000: train_loss 1.6987, val loss 1.7121\n",
      "step 28700 of 100000: train_loss 1.7309, val loss 1.6970\n",
      "step 28800 of 100000: train_loss 1.7216, val loss 1.7048\n",
      "step 28900 of 100000: train_loss 1.7183, val loss 1.7146\n",
      "step 29000 of 100000: train_loss 1.7193, val loss 1.7036\n",
      "step 29100 of 100000: train_loss 1.7053, val loss 1.7322\n",
      "step 29200 of 100000: train_loss 1.6992, val loss 1.7180\n",
      "step 29300 of 100000: train_loss 1.7395, val loss 1.7504\n",
      "step 29400 of 100000: train_loss 1.7617, val loss 1.6766\n",
      "step 29500 of 100000: train_loss 1.7047, val loss 1.6952\n",
      "step 29600 of 100000: train_loss 1.6875, val loss 1.7142\n",
      "step 29700 of 100000: train_loss 1.7025, val loss 1.7243\n",
      "step 29800 of 100000: train_loss 1.7135, val loss 1.6800\n",
      "step 29900 of 100000: train_loss 1.7007, val loss 1.6899\n",
      "step 30000 of 100000: train_loss 1.7148, val loss 1.7148\n",
      "step 30100 of 100000: train_loss 1.7189, val loss 1.7118\n",
      "step 30200 of 100000: train_loss 1.6957, val loss 1.6985\n",
      "step 30300 of 100000: train_loss 1.7072, val loss 1.7045\n",
      "step 30400 of 100000: train_loss 1.7162, val loss 1.7260\n",
      "step 30500 of 100000: train_loss 1.7126, val loss 1.7038\n",
      "step 30600 of 100000: train_loss 1.7085, val loss 1.7403\n",
      "step 30700 of 100000: train_loss 1.7406, val loss 1.7063\n",
      "step 30800 of 100000: train_loss 1.7193, val loss 1.6831\n",
      "step 30900 of 100000: train_loss 1.6830, val loss 1.7358\n",
      "step 31000 of 100000: train_loss 1.7260, val loss 1.7060\n",
      "step 31100 of 100000: train_loss 1.6963, val loss 1.6980\n",
      "step 31200 of 100000: train_loss 1.6898, val loss 1.6870\n",
      "step 31300 of 100000: train_loss 1.7445, val loss 1.7178\n",
      "step 31400 of 100000: train_loss 1.7092, val loss 1.6955\n",
      "step 31500 of 100000: train_loss 1.7291, val loss 1.7324\n",
      "step 31600 of 100000: train_loss 1.7092, val loss 1.7142\n",
      "step 31700 of 100000: train_loss 1.7218, val loss 1.6858\n",
      "step 31800 of 100000: train_loss 1.7334, val loss 1.7001\n",
      "step 31900 of 100000: train_loss 1.7284, val loss 1.7234\n",
      "step 32000 of 100000: train_loss 1.7170, val loss 1.6851\n",
      "step 32100 of 100000: train_loss 1.7265, val loss 1.6801\n",
      "step 32200 of 100000: train_loss 1.7279, val loss 1.7290\n",
      "step 32300 of 100000: train_loss 1.7151, val loss 1.6984\n",
      "step 32400 of 100000: train_loss 1.7045, val loss 1.7137\n",
      "step 32500 of 100000: train_loss 1.7491, val loss 1.7038\n",
      "step 32600 of 100000: train_loss 1.7271, val loss 1.6948\n",
      "step 32700 of 100000: train_loss 1.7317, val loss 1.6906\n",
      "step 32800 of 100000: train_loss 1.7112, val loss 1.7040\n",
      "step 32900 of 100000: train_loss 1.6962, val loss 1.7185\n",
      "step 33000 of 100000: train_loss 1.7362, val loss 1.7185\n",
      "step 33100 of 100000: train_loss 1.7287, val loss 1.6835\n",
      "step 33200 of 100000: train_loss 1.7003, val loss 1.7076\n",
      "step 33300 of 100000: train_loss 1.7283, val loss 1.7203\n",
      "step 33400 of 100000: train_loss 1.7160, val loss 1.7192\n",
      "step 33500 of 100000: train_loss 1.7178, val loss 1.6914\n",
      "step 33600 of 100000: train_loss 1.7104, val loss 1.7117\n",
      "step 33700 of 100000: train_loss 1.7314, val loss 1.6924\n",
      "step 33800 of 100000: train_loss 1.7397, val loss 1.7189\n",
      "step 33900 of 100000: train_loss 1.6912, val loss 1.7080\n",
      "step 34000 of 100000: train_loss 1.6964, val loss 1.7274\n",
      "step 34100 of 100000: train_loss 1.6779, val loss 1.7155\n",
      "step 34200 of 100000: train_loss 1.6875, val loss 1.6859\n",
      "step 34300 of 100000: train_loss 1.7050, val loss 1.6961\n",
      "step 34400 of 100000: train_loss 1.7203, val loss 1.6896\n",
      "step 34500 of 100000: train_loss 1.7109, val loss 1.6978\n",
      "step 34600 of 100000: train_loss 1.7290, val loss 1.6886\n",
      "step 34700 of 100000: train_loss 1.7263, val loss 1.7081\n",
      "step 34800 of 100000: train_loss 1.6825, val loss 1.7306\n",
      "step 34900 of 100000: train_loss 1.6914, val loss 1.6966\n",
      "step 35000 of 100000: train_loss 1.6915, val loss 1.7048\n",
      "step 35100 of 100000: train_loss 1.7084, val loss 1.7344\n",
      "step 35200 of 100000: train_loss 1.7511, val loss 1.7218\n",
      "step 35300 of 100000: train_loss 1.7204, val loss 1.7232\n",
      "step 35400 of 100000: train_loss 1.6997, val loss 1.6860\n",
      "step 35500 of 100000: train_loss 1.6872, val loss 1.6799\n",
      "step 35600 of 100000: train_loss 1.7072, val loss 1.7067\n",
      "step 35700 of 100000: train_loss 1.7264, val loss 1.7022\n",
      "step 35800 of 100000: train_loss 1.6950, val loss 1.6922\n",
      "step 35900 of 100000: train_loss 1.7200, val loss 1.6992\n",
      "step 36000 of 100000: train_loss 1.7382, val loss 1.7048\n",
      "step 36100 of 100000: train_loss 1.7023, val loss 1.7018\n",
      "step 36200 of 100000: train_loss 1.6978, val loss 1.7148\n",
      "step 36300 of 100000: train_loss 1.7285, val loss 1.7016\n",
      "step 36400 of 100000: train_loss 1.7286, val loss 1.6916\n",
      "step 36500 of 100000: train_loss 1.7247, val loss 1.6919\n",
      "step 36600 of 100000: train_loss 1.7145, val loss 1.7162\n",
      "step 36700 of 100000: train_loss 1.7131, val loss 1.7116\n",
      "step 36800 of 100000: train_loss 1.7194, val loss 1.7394\n",
      "step 36900 of 100000: train_loss 1.7167, val loss 1.7177\n",
      "step 37000 of 100000: train_loss 1.7066, val loss 1.7091\n",
      "step 37100 of 100000: train_loss 1.7410, val loss 1.7172\n",
      "step 37200 of 100000: train_loss 1.7360, val loss 1.6940\n",
      "step 37300 of 100000: train_loss 1.6941, val loss 1.6643\n",
      "step 37400 of 100000: train_loss 1.7235, val loss 1.6872\n",
      "step 37500 of 100000: train_loss 1.7315, val loss 1.7171\n",
      "step 37600 of 100000: train_loss 1.6796, val loss 1.6914\n",
      "step 37700 of 100000: train_loss 1.6883, val loss 1.6980\n",
      "step 37800 of 100000: train_loss 1.6783, val loss 1.7191\n",
      "step 37900 of 100000: train_loss 1.7166, val loss 1.7037\n",
      "step 38000 of 100000: train_loss 1.6910, val loss 1.7042\n",
      "step 38100 of 100000: train_loss 1.7069, val loss 1.6933\n",
      "step 38200 of 100000: train_loss 1.7154, val loss 1.7082\n",
      "step 38300 of 100000: train_loss 1.7259, val loss 1.6812\n",
      "step 38400 of 100000: train_loss 1.7116, val loss 1.7003\n",
      "step 38500 of 100000: train_loss 1.7185, val loss 1.6905\n",
      "step 38600 of 100000: train_loss 1.7262, val loss 1.6944\n",
      "step 38700 of 100000: train_loss 1.7034, val loss 1.7186\n",
      "step 38800 of 100000: train_loss 1.7077, val loss 1.6873\n",
      "step 38900 of 100000: train_loss 1.7014, val loss 1.7098\n",
      "step 39000 of 100000: train_loss 1.7203, val loss 1.7234\n",
      "step 39100 of 100000: train_loss 1.7051, val loss 1.7050\n",
      "step 39200 of 100000: train_loss 1.7405, val loss 1.7009\n",
      "step 39300 of 100000: train_loss 1.7192, val loss 1.7086\n",
      "step 39400 of 100000: train_loss 1.6749, val loss 1.7116\n",
      "step 39500 of 100000: train_loss 1.7218, val loss 1.6897\n",
      "step 39600 of 100000: train_loss 1.7445, val loss 1.7078\n",
      "step 39700 of 100000: train_loss 1.7161, val loss 1.7025\n",
      "step 39800 of 100000: train_loss 1.6750, val loss 1.6812\n",
      "step 39900 of 100000: train_loss 1.7172, val loss 1.7088\n",
      "step 40000 of 100000: train_loss 1.7007, val loss 1.6893\n",
      "step 40100 of 100000: train_loss 1.7274, val loss 1.7432\n",
      "step 40200 of 100000: train_loss 1.7073, val loss 1.6624\n",
      "step 40300 of 100000: train_loss 1.7188, val loss 1.7180\n",
      "step 40400 of 100000: train_loss 1.7144, val loss 1.7319\n",
      "step 40500 of 100000: train_loss 1.7535, val loss 1.7033\n",
      "step 40600 of 100000: train_loss 1.6987, val loss 1.6785\n",
      "step 40700 of 100000: train_loss 1.6840, val loss 1.7337\n",
      "step 40800 of 100000: train_loss 1.7415, val loss 1.7009\n",
      "step 40900 of 100000: train_loss 1.7051, val loss 1.7061\n",
      "step 41000 of 100000: train_loss 1.7145, val loss 1.7138\n",
      "step 41100 of 100000: train_loss 1.7148, val loss 1.7002\n",
      "step 41200 of 100000: train_loss 1.7106, val loss 1.6898\n",
      "step 41300 of 100000: train_loss 1.6871, val loss 1.7022\n",
      "step 41400 of 100000: train_loss 1.7224, val loss 1.7173\n",
      "step 41500 of 100000: train_loss 1.7279, val loss 1.7019\n",
      "step 41600 of 100000: train_loss 1.7198, val loss 1.7160\n",
      "step 41700 of 100000: train_loss 1.6960, val loss 1.6875\n",
      "step 41800 of 100000: train_loss 1.7237, val loss 1.7210\n",
      "step 41900 of 100000: train_loss 1.7094, val loss 1.7006\n",
      "step 42000 of 100000: train_loss 1.7061, val loss 1.6865\n",
      "step 42100 of 100000: train_loss 1.7338, val loss 1.7117\n",
      "step 42200 of 100000: train_loss 1.7207, val loss 1.7224\n",
      "step 42300 of 100000: train_loss 1.6966, val loss 1.6942\n",
      "step 42400 of 100000: train_loss 1.7260, val loss 1.7206\n",
      "step 42500 of 100000: train_loss 1.6827, val loss 1.7089\n",
      "step 42600 of 100000: train_loss 1.6963, val loss 1.7170\n",
      "step 42700 of 100000: train_loss 1.7062, val loss 1.7230\n",
      "step 42800 of 100000: train_loss 1.7396, val loss 1.6961\n",
      "step 42900 of 100000: train_loss 1.7148, val loss 1.7237\n",
      "step 43000 of 100000: train_loss 1.7129, val loss 1.7240\n",
      "step 43100 of 100000: train_loss 1.6736, val loss 1.7223\n",
      "step 43200 of 100000: train_loss 1.7288, val loss 1.7139\n",
      "step 43300 of 100000: train_loss 1.7270, val loss 1.7093\n",
      "step 43400 of 100000: train_loss 1.7183, val loss 1.7018\n",
      "step 43500 of 100000: train_loss 1.7150, val loss 1.6661\n",
      "step 43600 of 100000: train_loss 1.6925, val loss 1.7292\n",
      "step 43700 of 100000: train_loss 1.7093, val loss 1.6849\n",
      "step 43800 of 100000: train_loss 1.7139, val loss 1.6747\n",
      "step 43900 of 100000: train_loss 1.7230, val loss 1.7114\n",
      "step 44000 of 100000: train_loss 1.7160, val loss 1.7082\n",
      "step 44100 of 100000: train_loss 1.6976, val loss 1.7005\n",
      "step 44200 of 100000: train_loss 1.6737, val loss 1.6956\n",
      "step 44300 of 100000: train_loss 1.7426, val loss 1.7084\n",
      "step 44400 of 100000: train_loss 1.6951, val loss 1.6954\n",
      "step 44500 of 100000: train_loss 1.7297, val loss 1.6458\n",
      "step 44600 of 100000: train_loss 1.7026, val loss 1.6754\n",
      "step 44700 of 100000: train_loss 1.7221, val loss 1.6767\n",
      "step 44800 of 100000: train_loss 1.7252, val loss 1.7065\n",
      "step 44900 of 100000: train_loss 1.7468, val loss 1.7121\n",
      "step 45000 of 100000: train_loss 1.7185, val loss 1.7051\n",
      "step 45100 of 100000: train_loss 1.7148, val loss 1.7503\n",
      "step 45200 of 100000: train_loss 1.7145, val loss 1.7150\n",
      "step 45300 of 100000: train_loss 1.6950, val loss 1.6921\n",
      "step 45400 of 100000: train_loss 1.7143, val loss 1.7015\n",
      "step 45500 of 100000: train_loss 1.7077, val loss 1.6910\n",
      "step 45600 of 100000: train_loss 1.7083, val loss 1.6892\n",
      "step 45700 of 100000: train_loss 1.7044, val loss 1.7027\n",
      "step 45800 of 100000: train_loss 1.7148, val loss 1.7629\n",
      "step 45900 of 100000: train_loss 1.7047, val loss 1.7113\n",
      "step 46000 of 100000: train_loss 1.7298, val loss 1.6878\n",
      "step 46100 of 100000: train_loss 1.7154, val loss 1.7248\n",
      "step 46200 of 100000: train_loss 1.7093, val loss 1.7209\n",
      "step 46300 of 100000: train_loss 1.7172, val loss 1.7106\n",
      "step 46400 of 100000: train_loss 1.6967, val loss 1.6760\n",
      "step 46500 of 100000: train_loss 1.7042, val loss 1.6825\n",
      "step 46600 of 100000: train_loss 1.7110, val loss 1.7115\n",
      "step 46700 of 100000: train_loss 1.7055, val loss 1.7218\n",
      "step 46800 of 100000: train_loss 1.7090, val loss 1.6942\n",
      "step 46900 of 100000: train_loss 1.7095, val loss 1.6958\n",
      "step 47000 of 100000: train_loss 1.6905, val loss 1.7029\n",
      "step 47100 of 100000: train_loss 1.7145, val loss 1.6957\n",
      "step 47200 of 100000: train_loss 1.6980, val loss 1.7314\n",
      "step 47300 of 100000: train_loss 1.7123, val loss 1.6795\n",
      "step 47400 of 100000: train_loss 1.7167, val loss 1.7185\n",
      "step 47500 of 100000: train_loss 1.7094, val loss 1.7091\n",
      "step 47600 of 100000: train_loss 1.6994, val loss 1.7274\n",
      "step 47700 of 100000: train_loss 1.7287, val loss 1.7066\n",
      "step 47800 of 100000: train_loss 1.7012, val loss 1.7123\n",
      "step 47900 of 100000: train_loss 1.7391, val loss 1.7182\n",
      "step 48000 of 100000: train_loss 1.7005, val loss 1.7027\n",
      "step 48100 of 100000: train_loss 1.7347, val loss 1.7082\n",
      "step 48200 of 100000: train_loss 1.7177, val loss 1.6933\n",
      "step 48300 of 100000: train_loss 1.7045, val loss 1.6641\n",
      "step 48400 of 100000: train_loss 1.6950, val loss 1.7138\n",
      "step 48500 of 100000: train_loss 1.6915, val loss 1.7525\n",
      "step 48600 of 100000: train_loss 1.7017, val loss 1.6977\n",
      "step 48700 of 100000: train_loss 1.7137, val loss 1.7072\n",
      "step 48800 of 100000: train_loss 1.7409, val loss 1.7176\n",
      "step 48900 of 100000: train_loss 1.7285, val loss 1.7289\n",
      "step 49000 of 100000: train_loss 1.7241, val loss 1.6859\n",
      "step 49100 of 100000: train_loss 1.7434, val loss 1.7182\n",
      "step 49200 of 100000: train_loss 1.7366, val loss 1.7116\n",
      "step 49300 of 100000: train_loss 1.7281, val loss 1.7005\n",
      "step 49400 of 100000: train_loss 1.7114, val loss 1.6731\n",
      "step 49500 of 100000: train_loss 1.6950, val loss 1.7178\n",
      "step 49600 of 100000: train_loss 1.6893, val loss 1.6995\n",
      "step 49700 of 100000: train_loss 1.7278, val loss 1.6814\n",
      "step 49800 of 100000: train_loss 1.7352, val loss 1.7108\n",
      "step 49900 of 100000: train_loss 1.7320, val loss 1.7038\n",
      "step 50000 of 100000: train_loss 1.7293, val loss 1.7291\n",
      "step 50100 of 100000: train_loss 1.6754, val loss 1.6895\n",
      "step 50200 of 100000: train_loss 1.6925, val loss 1.7306\n",
      "step 50300 of 100000: train_loss 1.7090, val loss 1.6853\n",
      "step 50400 of 100000: train_loss 1.6781, val loss 1.7038\n",
      "step 50500 of 100000: train_loss 1.7033, val loss 1.7023\n",
      "step 50600 of 100000: train_loss 1.7259, val loss 1.7216\n",
      "step 50700 of 100000: train_loss 1.7366, val loss 1.6739\n",
      "step 50800 of 100000: train_loss 1.7090, val loss 1.7152\n",
      "step 50900 of 100000: train_loss 1.7045, val loss 1.6948\n",
      "step 51000 of 100000: train_loss 1.7180, val loss 1.7325\n",
      "step 51100 of 100000: train_loss 1.6951, val loss 1.7148\n",
      "step 51200 of 100000: train_loss 1.6794, val loss 1.7106\n",
      "step 51300 of 100000: train_loss 1.7435, val loss 1.6962\n",
      "step 51400 of 100000: train_loss 1.7464, val loss 1.7172\n",
      "step 51500 of 100000: train_loss 1.7081, val loss 1.7043\n",
      "step 51600 of 100000: train_loss 1.7003, val loss 1.6719\n",
      "step 51700 of 100000: train_loss 1.7020, val loss 1.7015\n",
      "step 51800 of 100000: train_loss 1.7218, val loss 1.7161\n",
      "step 51900 of 100000: train_loss 1.7213, val loss 1.6945\n",
      "step 52000 of 100000: train_loss 1.7155, val loss 1.7037\n",
      "step 52100 of 100000: train_loss 1.6945, val loss 1.7252\n",
      "step 52200 of 100000: train_loss 1.7004, val loss 1.7360\n",
      "step 52300 of 100000: train_loss 1.7199, val loss 1.7150\n",
      "step 52400 of 100000: train_loss 1.7301, val loss 1.6889\n",
      "step 52500 of 100000: train_loss 1.7148, val loss 1.7171\n",
      "step 52600 of 100000: train_loss 1.7096, val loss 1.6882\n",
      "step 52700 of 100000: train_loss 1.7026, val loss 1.7346\n",
      "step 52800 of 100000: train_loss 1.7127, val loss 1.7275\n",
      "step 52900 of 100000: train_loss 1.7044, val loss 1.6745\n",
      "step 53000 of 100000: train_loss 1.7197, val loss 1.7042\n",
      "step 53100 of 100000: train_loss 1.7294, val loss 1.6851\n",
      "step 53200 of 100000: train_loss 1.6898, val loss 1.6955\n",
      "step 53300 of 100000: train_loss 1.7298, val loss 1.6950\n",
      "step 53400 of 100000: train_loss 1.7383, val loss 1.7005\n",
      "step 53500 of 100000: train_loss 1.7084, val loss 1.7104\n",
      "step 53600 of 100000: train_loss 1.7172, val loss 1.7143\n",
      "step 53700 of 100000: train_loss 1.7171, val loss 1.7343\n",
      "step 53800 of 100000: train_loss 1.7467, val loss 1.7220\n",
      "step 53900 of 100000: train_loss 1.7114, val loss 1.7127\n",
      "step 54000 of 100000: train_loss 1.7138, val loss 1.7165\n",
      "step 54100 of 100000: train_loss 1.7241, val loss 1.7089\n",
      "step 54200 of 100000: train_loss 1.7104, val loss 1.6791\n",
      "step 54300 of 100000: train_loss 1.6947, val loss 1.7239\n",
      "step 54400 of 100000: train_loss 1.6798, val loss 1.6959\n",
      "step 54500 of 100000: train_loss 1.6936, val loss 1.7070\n",
      "step 54600 of 100000: train_loss 1.7342, val loss 1.7287\n",
      "step 54700 of 100000: train_loss 1.7404, val loss 1.6897\n",
      "step 54800 of 100000: train_loss 1.6980, val loss 1.7030\n",
      "step 54900 of 100000: train_loss 1.7154, val loss 1.7030\n",
      "step 55000 of 100000: train_loss 1.7349, val loss 1.7442\n",
      "step 55100 of 100000: train_loss 1.7222, val loss 1.6947\n",
      "step 55200 of 100000: train_loss 1.7252, val loss 1.6850\n",
      "step 55300 of 100000: train_loss 1.6926, val loss 1.7147\n",
      "step 55400 of 100000: train_loss 1.7494, val loss 1.7609\n",
      "step 55500 of 100000: train_loss 1.7259, val loss 1.7237\n",
      "step 55600 of 100000: train_loss 1.7315, val loss 1.7470\n",
      "step 55700 of 100000: train_loss 1.6852, val loss 1.6986\n",
      "step 55800 of 100000: train_loss 1.7188, val loss 1.7241\n",
      "step 55900 of 100000: train_loss 1.7283, val loss 1.7254\n",
      "step 56000 of 100000: train_loss 1.7156, val loss 1.6965\n",
      "step 56100 of 100000: train_loss 1.7045, val loss 1.6987\n",
      "step 56200 of 100000: train_loss 1.7292, val loss 1.7251\n",
      "step 56300 of 100000: train_loss 1.6965, val loss 1.6935\n",
      "step 56400 of 100000: train_loss 1.7013, val loss 1.6887\n",
      "step 56500 of 100000: train_loss 1.6971, val loss 1.6910\n",
      "step 56600 of 100000: train_loss 1.6945, val loss 1.7283\n",
      "step 56700 of 100000: train_loss 1.6817, val loss 1.6648\n",
      "step 56800 of 100000: train_loss 1.7225, val loss 1.7608\n",
      "step 56900 of 100000: train_loss 1.6985, val loss 1.7005\n",
      "step 57000 of 100000: train_loss 1.6958, val loss 1.6968\n",
      "step 57100 of 100000: train_loss 1.7176, val loss 1.6761\n",
      "step 57200 of 100000: train_loss 1.7293, val loss 1.6888\n",
      "step 57300 of 100000: train_loss 1.7218, val loss 1.6977\n",
      "step 57400 of 100000: train_loss 1.6741, val loss 1.6943\n",
      "step 57500 of 100000: train_loss 1.7102, val loss 1.7241\n",
      "step 57600 of 100000: train_loss 1.7158, val loss 1.7355\n",
      "step 57700 of 100000: train_loss 1.7256, val loss 1.7150\n",
      "step 57800 of 100000: train_loss 1.7350, val loss 1.7185\n",
      "step 57900 of 100000: train_loss 1.7174, val loss 1.6698\n",
      "step 58000 of 100000: train_loss 1.7110, val loss 1.7042\n",
      "step 58100 of 100000: train_loss 1.7332, val loss 1.7113\n",
      "step 58200 of 100000: train_loss 1.7006, val loss 1.7184\n",
      "step 58300 of 100000: train_loss 1.7289, val loss 1.6748\n",
      "step 58400 of 100000: train_loss 1.6971, val loss 1.7043\n",
      "step 58500 of 100000: train_loss 1.7314, val loss 1.6989\n",
      "step 58600 of 100000: train_loss 1.6941, val loss 1.6840\n",
      "step 58700 of 100000: train_loss 1.7084, val loss 1.7007\n",
      "step 58800 of 100000: train_loss 1.6976, val loss 1.6931\n",
      "step 58900 of 100000: train_loss 1.7054, val loss 1.7193\n",
      "step 59000 of 100000: train_loss 1.7051, val loss 1.7302\n",
      "step 59100 of 100000: train_loss 1.7224, val loss 1.7254\n",
      "step 59200 of 100000: train_loss 1.7466, val loss 1.6977\n",
      "step 59300 of 100000: train_loss 1.6930, val loss 1.6950\n",
      "step 59400 of 100000: train_loss 1.7093, val loss 1.7091\n",
      "step 59500 of 100000: train_loss 1.7071, val loss 1.6767\n",
      "step 59600 of 100000: train_loss 1.6948, val loss 1.7053\n",
      "step 59700 of 100000: train_loss 1.6923, val loss 1.7170\n",
      "step 59800 of 100000: train_loss 1.6822, val loss 1.6885\n",
      "step 59900 of 100000: train_loss 1.6984, val loss 1.7002\n",
      "step 60000 of 100000: train_loss 1.7226, val loss 1.7191\n",
      "step 60100 of 100000: train_loss 1.7310, val loss 1.7007\n",
      "step 60200 of 100000: train_loss 1.6931, val loss 1.7131\n",
      "step 60300 of 100000: train_loss 1.7086, val loss 1.6918\n",
      "step 60400 of 100000: train_loss 1.6921, val loss 1.7278\n",
      "step 60500 of 100000: train_loss 1.7287, val loss 1.7262\n",
      "step 60600 of 100000: train_loss 1.6688, val loss 1.6766\n",
      "step 60700 of 100000: train_loss 1.7117, val loss 1.7088\n",
      "step 60800 of 100000: train_loss 1.7185, val loss 1.7215\n",
      "step 60900 of 100000: train_loss 1.7449, val loss 1.7349\n",
      "step 61000 of 100000: train_loss 1.7122, val loss 1.7150\n",
      "step 61100 of 100000: train_loss 1.7303, val loss 1.7286\n",
      "step 61200 of 100000: train_loss 1.7060, val loss 1.6663\n",
      "step 61300 of 100000: train_loss 1.7263, val loss 1.7006\n",
      "step 61400 of 100000: train_loss 1.7417, val loss 1.7286\n",
      "step 61500 of 100000: train_loss 1.7367, val loss 1.7265\n",
      "step 61600 of 100000: train_loss 1.7245, val loss 1.6890\n",
      "step 61700 of 100000: train_loss 1.7227, val loss 1.6994\n",
      "step 61800 of 100000: train_loss 1.7241, val loss 1.7023\n",
      "step 61900 of 100000: train_loss 1.6947, val loss 1.7176\n",
      "step 62000 of 100000: train_loss 1.7334, val loss 1.7253\n",
      "step 62100 of 100000: train_loss 1.7207, val loss 1.6959\n",
      "step 62200 of 100000: train_loss 1.7124, val loss 1.7230\n",
      "step 62300 of 100000: train_loss 1.7077, val loss 1.7094\n",
      "step 62400 of 100000: train_loss 1.7134, val loss 1.7140\n",
      "step 62500 of 100000: train_loss 1.7255, val loss 1.7515\n",
      "step 62600 of 100000: train_loss 1.7275, val loss 1.7354\n",
      "step 62700 of 100000: train_loss 1.7300, val loss 1.6969\n",
      "step 62800 of 100000: train_loss 1.7143, val loss 1.7253\n",
      "step 62900 of 100000: train_loss 1.7362, val loss 1.7150\n",
      "step 63000 of 100000: train_loss 1.7518, val loss 1.6949\n",
      "step 63100 of 100000: train_loss 1.7315, val loss 1.7025\n",
      "step 63200 of 100000: train_loss 1.7145, val loss 1.7002\n",
      "step 63300 of 100000: train_loss 1.7104, val loss 1.7236\n",
      "step 63400 of 100000: train_loss 1.7300, val loss 1.7047\n",
      "step 63500 of 100000: train_loss 1.6866, val loss 1.7229\n",
      "step 63600 of 100000: train_loss 1.7202, val loss 1.7072\n",
      "step 63700 of 100000: train_loss 1.7167, val loss 1.7301\n",
      "step 63800 of 100000: train_loss 1.7434, val loss 1.7184\n",
      "step 63900 of 100000: train_loss 1.6974, val loss 1.7382\n",
      "step 64000 of 100000: train_loss 1.7371, val loss 1.7157\n",
      "step 64100 of 100000: train_loss 1.6919, val loss 1.7094\n",
      "step 64200 of 100000: train_loss 1.7196, val loss 1.7228\n",
      "step 64300 of 100000: train_loss 1.7205, val loss 1.7204\n",
      "step 64400 of 100000: train_loss 1.7037, val loss 1.7081\n",
      "step 64500 of 100000: train_loss 1.7040, val loss 1.7142\n",
      "step 64600 of 100000: train_loss 1.6980, val loss 1.6810\n",
      "step 64700 of 100000: train_loss 1.7181, val loss 1.7459\n",
      "step 64800 of 100000: train_loss 1.7326, val loss 1.6979\n",
      "step 64900 of 100000: train_loss 1.7059, val loss 1.6857\n",
      "step 65000 of 100000: train_loss 1.7210, val loss 1.6949\n",
      "step 65100 of 100000: train_loss 1.7353, val loss 1.7105\n",
      "step 65200 of 100000: train_loss 1.7084, val loss 1.6982\n",
      "step 65300 of 100000: train_loss 1.7140, val loss 1.6736\n",
      "step 65400 of 100000: train_loss 1.7052, val loss 1.7088\n",
      "step 65500 of 100000: train_loss 1.6958, val loss 1.6998\n",
      "step 65600 of 100000: train_loss 1.7380, val loss 1.7012\n",
      "step 65700 of 100000: train_loss 1.7162, val loss 1.7158\n",
      "step 65800 of 100000: train_loss 1.7209, val loss 1.6889\n",
      "step 65900 of 100000: train_loss 1.6906, val loss 1.6994\n",
      "step 66000 of 100000: train_loss 1.6930, val loss 1.6926\n",
      "step 66100 of 100000: train_loss 1.7148, val loss 1.6774\n",
      "step 66200 of 100000: train_loss 1.7078, val loss 1.7239\n",
      "step 66300 of 100000: train_loss 1.7135, val loss 1.6925\n",
      "step 66400 of 100000: train_loss 1.6941, val loss 1.7167\n",
      "step 66500 of 100000: train_loss 1.7164, val loss 1.7146\n",
      "step 66600 of 100000: train_loss 1.7368, val loss 1.7103\n",
      "step 66700 of 100000: train_loss 1.6967, val loss 1.7384\n",
      "step 66800 of 100000: train_loss 1.7055, val loss 1.7026\n",
      "step 66900 of 100000: train_loss 1.7219, val loss 1.6994\n",
      "step 67000 of 100000: train_loss 1.7055, val loss 1.7202\n",
      "step 67100 of 100000: train_loss 1.7176, val loss 1.7311\n",
      "step 67200 of 100000: train_loss 1.7297, val loss 1.7052\n",
      "step 67300 of 100000: train_loss 1.6993, val loss 1.7320\n",
      "step 67400 of 100000: train_loss 1.7223, val loss 1.7200\n",
      "step 67500 of 100000: train_loss 1.7430, val loss 1.7329\n",
      "step 67600 of 100000: train_loss 1.6914, val loss 1.7426\n",
      "step 67700 of 100000: train_loss 1.7222, val loss 1.7113\n",
      "step 67800 of 100000: train_loss 1.6948, val loss 1.6884\n",
      "step 67900 of 100000: train_loss 1.7058, val loss 1.6736\n",
      "step 68000 of 100000: train_loss 1.7246, val loss 1.7366\n",
      "step 68100 of 100000: train_loss 1.7162, val loss 1.6987\n",
      "step 68200 of 100000: train_loss 1.7203, val loss 1.6930\n",
      "step 68300 of 100000: train_loss 1.6894, val loss 1.7058\n",
      "step 68400 of 100000: train_loss 1.6852, val loss 1.7371\n",
      "step 68500 of 100000: train_loss 1.7295, val loss 1.6808\n",
      "step 68600 of 100000: train_loss 1.6974, val loss 1.7096\n",
      "step 68700 of 100000: train_loss 1.7298, val loss 1.6960\n",
      "step 68800 of 100000: train_loss 1.7298, val loss 1.7376\n",
      "step 68900 of 100000: train_loss 1.7060, val loss 1.7006\n",
      "step 69000 of 100000: train_loss 1.7589, val loss 1.6826\n",
      "step 69100 of 100000: train_loss 1.6943, val loss 1.7279\n",
      "step 69200 of 100000: train_loss 1.7089, val loss 1.7171\n",
      "step 69300 of 100000: train_loss 1.7273, val loss 1.7249\n",
      "step 69400 of 100000: train_loss 1.7013, val loss 1.6947\n",
      "step 69500 of 100000: train_loss 1.7197, val loss 1.7388\n",
      "step 69600 of 100000: train_loss 1.7036, val loss 1.7272\n",
      "step 69700 of 100000: train_loss 1.7221, val loss 1.6898\n",
      "step 69800 of 100000: train_loss 1.7176, val loss 1.7109\n",
      "step 69900 of 100000: train_loss 1.7226, val loss 1.6909\n",
      "step 70000 of 100000: train_loss 1.6944, val loss 1.6930\n",
      "step 70100 of 100000: train_loss 1.7363, val loss 1.6730\n",
      "step 70200 of 100000: train_loss 1.7061, val loss 1.7069\n",
      "step 70300 of 100000: train_loss 1.7446, val loss 1.6961\n",
      "step 70400 of 100000: train_loss 1.7168, val loss 1.7119\n",
      "step 70500 of 100000: train_loss 1.7080, val loss 1.7265\n",
      "step 70600 of 100000: train_loss 1.7431, val loss 1.7076\n",
      "step 70700 of 100000: train_loss 1.7367, val loss 1.6958\n",
      "step 70800 of 100000: train_loss 1.7204, val loss 1.7156\n",
      "step 70900 of 100000: train_loss 1.7177, val loss 1.7035\n",
      "step 71000 of 100000: train_loss 1.7184, val loss 1.7126\n",
      "step 71100 of 100000: train_loss 1.7219, val loss 1.7001\n",
      "step 71200 of 100000: train_loss 1.7201, val loss 1.7177\n",
      "step 71300 of 100000: train_loss 1.7119, val loss 1.7040\n",
      "step 71400 of 100000: train_loss 1.7454, val loss 1.7048\n",
      "step 71500 of 100000: train_loss 1.7420, val loss 1.6787\n",
      "step 71600 of 100000: train_loss 1.6970, val loss 1.7141\n",
      "step 71700 of 100000: train_loss 1.7011, val loss 1.7218\n",
      "step 71800 of 100000: train_loss 1.7049, val loss 1.7315\n",
      "step 71900 of 100000: train_loss 1.7032, val loss 1.7021\n",
      "step 72000 of 100000: train_loss 1.6923, val loss 1.6733\n",
      "step 72100 of 100000: train_loss 1.7166, val loss 1.7271\n",
      "step 72200 of 100000: train_loss 1.7281, val loss 1.6892\n",
      "step 72300 of 100000: train_loss 1.7451, val loss 1.7364\n",
      "step 72400 of 100000: train_loss 1.7296, val loss 1.7146\n",
      "step 72500 of 100000: train_loss 1.7150, val loss 1.6867\n",
      "step 72600 of 100000: train_loss 1.7117, val loss 1.6511\n",
      "step 72700 of 100000: train_loss 1.7168, val loss 1.7185\n",
      "step 72800 of 100000: train_loss 1.7121, val loss 1.6904\n",
      "step 72900 of 100000: train_loss 1.6780, val loss 1.6918\n",
      "step 73000 of 100000: train_loss 1.7329, val loss 1.7218\n",
      "step 73100 of 100000: train_loss 1.6989, val loss 1.7076\n",
      "step 73200 of 100000: train_loss 1.7001, val loss 1.7119\n",
      "step 73300 of 100000: train_loss 1.7458, val loss 1.7002\n",
      "step 73400 of 100000: train_loss 1.7143, val loss 1.6900\n",
      "step 73500 of 100000: train_loss 1.7065, val loss 1.7328\n",
      "step 73600 of 100000: train_loss 1.7367, val loss 1.6979\n",
      "step 73700 of 100000: train_loss 1.6966, val loss 1.7445\n",
      "step 73800 of 100000: train_loss 1.7126, val loss 1.7119\n",
      "step 73900 of 100000: train_loss 1.7029, val loss 1.7016\n",
      "step 74000 of 100000: train_loss 1.7186, val loss 1.7038\n",
      "step 74100 of 100000: train_loss 1.7018, val loss 1.7180\n",
      "step 74200 of 100000: train_loss 1.7159, val loss 1.6915\n",
      "step 74300 of 100000: train_loss 1.6966, val loss 1.7037\n",
      "step 74400 of 100000: train_loss 1.7064, val loss 1.6739\n",
      "step 74500 of 100000: train_loss 1.7181, val loss 1.6986\n",
      "step 74600 of 100000: train_loss 1.6795, val loss 1.7102\n",
      "step 74700 of 100000: train_loss 1.7177, val loss 1.7140\n",
      "step 74800 of 100000: train_loss 1.7204, val loss 1.7296\n",
      "step 74900 of 100000: train_loss 1.7108, val loss 1.7112\n",
      "step 75000 of 100000: train_loss 1.7140, val loss 1.7362\n",
      "step 75100 of 100000: train_loss 1.6726, val loss 1.7249\n",
      "step 75200 of 100000: train_loss 1.7153, val loss 1.7142\n",
      "step 75300 of 100000: train_loss 1.7188, val loss 1.7055\n",
      "step 75400 of 100000: train_loss 1.7423, val loss 1.7116\n",
      "step 75500 of 100000: train_loss 1.7203, val loss 1.6720\n",
      "step 75600 of 100000: train_loss 1.7026, val loss 1.7112\n",
      "step 75700 of 100000: train_loss 1.6933, val loss 1.7222\n",
      "step 75800 of 100000: train_loss 1.7155, val loss 1.7207\n",
      "step 75900 of 100000: train_loss 1.7455, val loss 1.7049\n",
      "step 76000 of 100000: train_loss 1.7034, val loss 1.7027\n",
      "step 76100 of 100000: train_loss 1.6901, val loss 1.7512\n",
      "step 76200 of 100000: train_loss 1.7096, val loss 1.6845\n",
      "step 76300 of 100000: train_loss 1.7143, val loss 1.6676\n",
      "step 76400 of 100000: train_loss 1.6975, val loss 1.7046\n",
      "step 76500 of 100000: train_loss 1.7266, val loss 1.7071\n",
      "step 76600 of 100000: train_loss 1.7066, val loss 1.7250\n",
      "step 76700 of 100000: train_loss 1.7180, val loss 1.7078\n",
      "step 76800 of 100000: train_loss 1.7152, val loss 1.6756\n",
      "step 76900 of 100000: train_loss 1.6984, val loss 1.6899\n",
      "step 77000 of 100000: train_loss 1.7285, val loss 1.7015\n",
      "step 77100 of 100000: train_loss 1.6996, val loss 1.7070\n",
      "step 77200 of 100000: train_loss 1.6843, val loss 1.6779\n",
      "step 77300 of 100000: train_loss 1.6946, val loss 1.6928\n",
      "step 77400 of 100000: train_loss 1.7153, val loss 1.6834\n",
      "step 77500 of 100000: train_loss 1.7344, val loss 1.7239\n",
      "step 77600 of 100000: train_loss 1.6864, val loss 1.7183\n",
      "step 77700 of 100000: train_loss 1.6853, val loss 1.7262\n",
      "step 77800 of 100000: train_loss 1.7104, val loss 1.7025\n",
      "step 77900 of 100000: train_loss 1.7270, val loss 1.7105\n",
      "step 78000 of 100000: train_loss 1.6986, val loss 1.6903\n",
      "step 78100 of 100000: train_loss 1.7133, val loss 1.7687\n",
      "step 78200 of 100000: train_loss 1.6942, val loss 1.6955\n",
      "step 78300 of 100000: train_loss 1.7343, val loss 1.7088\n",
      "step 78400 of 100000: train_loss 1.7216, val loss 1.7127\n",
      "step 78500 of 100000: train_loss 1.6817, val loss 1.7430\n",
      "step 78600 of 100000: train_loss 1.7171, val loss 1.7125\n",
      "step 78700 of 100000: train_loss 1.6928, val loss 1.7318\n",
      "step 78800 of 100000: train_loss 1.7264, val loss 1.7163\n",
      "step 78900 of 100000: train_loss 1.7142, val loss 1.7133\n",
      "step 79000 of 100000: train_loss 1.6965, val loss 1.6921\n",
      "step 79100 of 100000: train_loss 1.7235, val loss 1.7121\n",
      "step 79200 of 100000: train_loss 1.7360, val loss 1.7003\n",
      "step 79300 of 100000: train_loss 1.6896, val loss 1.6951\n",
      "step 79400 of 100000: train_loss 1.6830, val loss 1.7123\n",
      "step 79500 of 100000: train_loss 1.7014, val loss 1.7092\n",
      "step 79600 of 100000: train_loss 1.7279, val loss 1.7292\n",
      "step 79700 of 100000: train_loss 1.7268, val loss 1.6971\n",
      "step 79800 of 100000: train_loss 1.7234, val loss 1.6719\n",
      "step 79900 of 100000: train_loss 1.7107, val loss 1.6896\n",
      "step 80000 of 100000: train_loss 1.7093, val loss 1.7139\n",
      "step 80100 of 100000: train_loss 1.7032, val loss 1.7252\n",
      "step 80200 of 100000: train_loss 1.7123, val loss 1.7216\n",
      "step 80300 of 100000: train_loss 1.7207, val loss 1.7008\n",
      "step 80400 of 100000: train_loss 1.6782, val loss 1.7002\n",
      "step 80500 of 100000: train_loss 1.7379, val loss 1.7186\n",
      "step 80600 of 100000: train_loss 1.7032, val loss 1.7387\n",
      "step 80700 of 100000: train_loss 1.7389, val loss 1.7335\n",
      "step 80800 of 100000: train_loss 1.7531, val loss 1.6947\n",
      "step 80900 of 100000: train_loss 1.7223, val loss 1.7021\n",
      "step 81000 of 100000: train_loss 1.7136, val loss 1.6844\n",
      "step 81100 of 100000: train_loss 1.7187, val loss 1.7516\n",
      "step 81200 of 100000: train_loss 1.7170, val loss 1.6618\n",
      "step 81300 of 100000: train_loss 1.7032, val loss 1.7196\n",
      "step 81400 of 100000: train_loss 1.6976, val loss 1.6851\n",
      "step 81500 of 100000: train_loss 1.7081, val loss 1.7067\n",
      "step 81600 of 100000: train_loss 1.7154, val loss 1.7158\n",
      "step 81700 of 100000: train_loss 1.7272, val loss 1.7116\n",
      "step 81800 of 100000: train_loss 1.7358, val loss 1.7233\n",
      "step 81900 of 100000: train_loss 1.7243, val loss 1.6934\n",
      "step 82000 of 100000: train_loss 1.7026, val loss 1.7209\n",
      "step 82100 of 100000: train_loss 1.6807, val loss 1.6837\n",
      "step 82200 of 100000: train_loss 1.7145, val loss 1.7388\n",
      "step 82300 of 100000: train_loss 1.7547, val loss 1.7358\n",
      "step 82400 of 100000: train_loss 1.7127, val loss 1.6873\n",
      "step 82500 of 100000: train_loss 1.6879, val loss 1.7026\n",
      "step 82600 of 100000: train_loss 1.7360, val loss 1.7214\n",
      "step 82700 of 100000: train_loss 1.7448, val loss 1.7526\n",
      "step 82800 of 100000: train_loss 1.7141, val loss 1.6863\n",
      "step 82900 of 100000: train_loss 1.7004, val loss 1.7225\n",
      "step 83000 of 100000: train_loss 1.7141, val loss 1.7448\n",
      "step 83100 of 100000: train_loss 1.7097, val loss 1.7391\n",
      "step 83200 of 100000: train_loss 1.7307, val loss 1.6935\n",
      "step 83300 of 100000: train_loss 1.7241, val loss 1.6803\n",
      "step 83400 of 100000: train_loss 1.7163, val loss 1.7169\n",
      "step 83500 of 100000: train_loss 1.7312, val loss 1.7101\n",
      "step 83600 of 100000: train_loss 1.7139, val loss 1.7140\n",
      "step 83700 of 100000: train_loss 1.6991, val loss 1.7173\n",
      "step 83800 of 100000: train_loss 1.7016, val loss 1.7024\n",
      "step 83900 of 100000: train_loss 1.7460, val loss 1.7076\n",
      "step 84000 of 100000: train_loss 1.7459, val loss 1.7135\n",
      "step 84100 of 100000: train_loss 1.7224, val loss 1.6967\n",
      "step 84200 of 100000: train_loss 1.7159, val loss 1.7066\n",
      "step 84300 of 100000: train_loss 1.7100, val loss 1.7361\n",
      "step 84400 of 100000: train_loss 1.7001, val loss 1.7122\n",
      "step 84500 of 100000: train_loss 1.6957, val loss 1.7087\n",
      "step 84600 of 100000: train_loss 1.7325, val loss 1.6746\n",
      "step 84700 of 100000: train_loss 1.6895, val loss 1.7136\n",
      "step 84800 of 100000: train_loss 1.7337, val loss 1.7132\n",
      "step 84900 of 100000: train_loss 1.7269, val loss 1.6963\n",
      "step 85000 of 100000: train_loss 1.7013, val loss 1.6913\n",
      "step 85100 of 100000: train_loss 1.6762, val loss 1.7234\n",
      "step 85200 of 100000: train_loss 1.7254, val loss 1.7150\n",
      "step 85300 of 100000: train_loss 1.7026, val loss 1.7174\n",
      "step 85400 of 100000: train_loss 1.7301, val loss 1.6788\n",
      "step 85500 of 100000: train_loss 1.7369, val loss 1.7129\n",
      "step 85600 of 100000: train_loss 1.7070, val loss 1.7128\n",
      "step 85700 of 100000: train_loss 1.6896, val loss 1.6978\n",
      "step 85800 of 100000: train_loss 1.7429, val loss 1.7465\n",
      "step 85900 of 100000: train_loss 1.7252, val loss 1.7184\n",
      "step 86000 of 100000: train_loss 1.6828, val loss 1.7098\n",
      "step 86100 of 100000: train_loss 1.6757, val loss 1.7232\n",
      "step 86200 of 100000: train_loss 1.7182, val loss 1.7046\n",
      "step 86300 of 100000: train_loss 1.7322, val loss 1.7260\n",
      "step 86400 of 100000: train_loss 1.7446, val loss 1.6849\n",
      "step 86500 of 100000: train_loss 1.7399, val loss 1.6879\n",
      "step 86600 of 100000: train_loss 1.7080, val loss 1.7035\n",
      "step 86700 of 100000: train_loss 1.7233, val loss 1.6943\n",
      "step 86800 of 100000: train_loss 1.7322, val loss 1.6958\n",
      "step 86900 of 100000: train_loss 1.7139, val loss 1.7098\n",
      "step 87000 of 100000: train_loss 1.7098, val loss 1.6895\n",
      "step 87100 of 100000: train_loss 1.7126, val loss 1.6706\n",
      "step 87200 of 100000: train_loss 1.7070, val loss 1.6894\n",
      "step 87300 of 100000: train_loss 1.7447, val loss 1.7097\n",
      "step 87400 of 100000: train_loss 1.7043, val loss 1.6973\n",
      "step 87500 of 100000: train_loss 1.7212, val loss 1.7349\n",
      "step 87600 of 100000: train_loss 1.7319, val loss 1.6974\n",
      "step 87700 of 100000: train_loss 1.7006, val loss 1.6936\n",
      "step 87800 of 100000: train_loss 1.7153, val loss 1.6909\n",
      "step 87900 of 100000: train_loss 1.7270, val loss 1.6925\n",
      "step 88000 of 100000: train_loss 1.7145, val loss 1.7206\n",
      "step 88100 of 100000: train_loss 1.7285, val loss 1.7221\n",
      "step 88200 of 100000: train_loss 1.7122, val loss 1.7270\n",
      "step 88300 of 100000: train_loss 1.7290, val loss 1.7174\n",
      "step 88400 of 100000: train_loss 1.7149, val loss 1.7134\n",
      "step 88500 of 100000: train_loss 1.7054, val loss 1.7320\n",
      "step 88600 of 100000: train_loss 1.7251, val loss 1.6912\n",
      "step 88700 of 100000: train_loss 1.7377, val loss 1.6965\n",
      "step 88800 of 100000: train_loss 1.7137, val loss 1.6724\n",
      "step 88900 of 100000: train_loss 1.7223, val loss 1.7349\n",
      "step 89000 of 100000: train_loss 1.7184, val loss 1.7366\n",
      "step 89100 of 100000: train_loss 1.7223, val loss 1.7030\n",
      "step 89200 of 100000: train_loss 1.7184, val loss 1.7043\n",
      "step 89300 of 100000: train_loss 1.7177, val loss 1.7163\n",
      "step 89400 of 100000: train_loss 1.7258, val loss 1.6982\n",
      "step 89500 of 100000: train_loss 1.7124, val loss 1.6922\n",
      "step 89600 of 100000: train_loss 1.7011, val loss 1.7178\n",
      "step 89700 of 100000: train_loss 1.6954, val loss 1.7129\n",
      "step 89800 of 100000: train_loss 1.7122, val loss 1.7101\n",
      "step 89900 of 100000: train_loss 1.7443, val loss 1.7080\n",
      "step 90000 of 100000: train_loss 1.7308, val loss 1.7150\n",
      "step 90100 of 100000: train_loss 1.7289, val loss 1.7119\n",
      "step 90200 of 100000: train_loss 1.7334, val loss 1.7231\n",
      "step 90300 of 100000: train_loss 1.7408, val loss 1.7206\n",
      "step 90400 of 100000: train_loss 1.6833, val loss 1.7074\n",
      "step 90500 of 100000: train_loss 1.6942, val loss 1.7354\n",
      "step 90600 of 100000: train_loss 1.7198, val loss 1.7126\n",
      "step 90700 of 100000: train_loss 1.7085, val loss 1.7132\n",
      "step 90800 of 100000: train_loss 1.6978, val loss 1.7059\n",
      "step 90900 of 100000: train_loss 1.7138, val loss 1.7102\n",
      "step 91000 of 100000: train_loss 1.6944, val loss 1.6948\n",
      "step 91100 of 100000: train_loss 1.7099, val loss 1.6823\n",
      "step 91200 of 100000: train_loss 1.7010, val loss 1.7246\n",
      "step 91300 of 100000: train_loss 1.6973, val loss 1.7154\n",
      "step 91400 of 100000: train_loss 1.6896, val loss 1.6819\n",
      "step 91500 of 100000: train_loss 1.7175, val loss 1.7172\n",
      "step 91600 of 100000: train_loss 1.7050, val loss 1.7156\n",
      "step 91700 of 100000: train_loss 1.7148, val loss 1.6817\n",
      "step 91800 of 100000: train_loss 1.7317, val loss 1.7408\n",
      "step 91900 of 100000: train_loss 1.7346, val loss 1.7175\n",
      "step 92000 of 100000: train_loss 1.6966, val loss 1.7179\n",
      "step 92100 of 100000: train_loss 1.7325, val loss 1.7217\n",
      "step 92200 of 100000: train_loss 1.6889, val loss 1.7114\n",
      "step 92300 of 100000: train_loss 1.7167, val loss 1.6947\n",
      "step 92400 of 100000: train_loss 1.7196, val loss 1.7244\n",
      "step 92500 of 100000: train_loss 1.7543, val loss 1.7039\n",
      "step 92600 of 100000: train_loss 1.7374, val loss 1.6922\n",
      "step 92700 of 100000: train_loss 1.7107, val loss 1.7024\n",
      "step 92800 of 100000: train_loss 1.7201, val loss 1.7315\n",
      "step 92900 of 100000: train_loss 1.7371, val loss 1.7026\n",
      "step 93000 of 100000: train_loss 1.7263, val loss 1.6799\n",
      "step 93100 of 100000: train_loss 1.7295, val loss 1.6853\n",
      "step 93200 of 100000: train_loss 1.7250, val loss 1.7113\n",
      "step 93300 of 100000: train_loss 1.7090, val loss 1.7356\n",
      "step 93400 of 100000: train_loss 1.7485, val loss 1.7396\n",
      "step 93500 of 100000: train_loss 1.7262, val loss 1.7109\n",
      "step 93600 of 100000: train_loss 1.7037, val loss 1.6974\n",
      "step 93700 of 100000: train_loss 1.7116, val loss 1.7089\n",
      "step 93800 of 100000: train_loss 1.7190, val loss 1.7054\n",
      "step 93900 of 100000: train_loss 1.7136, val loss 1.7029\n",
      "step 94000 of 100000: train_loss 1.7152, val loss 1.7398\n",
      "step 94100 of 100000: train_loss 1.7213, val loss 1.6910\n",
      "step 94200 of 100000: train_loss 1.7607, val loss 1.6759\n",
      "step 94300 of 100000: train_loss 1.7250, val loss 1.6828\n",
      "step 94400 of 100000: train_loss 1.7281, val loss 1.7145\n",
      "step 94500 of 100000: train_loss 1.7238, val loss 1.7260\n",
      "step 94600 of 100000: train_loss 1.6866, val loss 1.6853\n",
      "step 94700 of 100000: train_loss 1.7335, val loss 1.7120\n",
      "step 94800 of 100000: train_loss 1.7362, val loss 1.7100\n",
      "step 94900 of 100000: train_loss 1.7284, val loss 1.7277\n",
      "step 95000 of 100000: train_loss 1.7435, val loss 1.6850\n",
      "step 95100 of 100000: train_loss 1.7273, val loss 1.6939\n",
      "step 95200 of 100000: train_loss 1.6875, val loss 1.7227\n",
      "step 95300 of 100000: train_loss 1.7235, val loss 1.7115\n",
      "step 95400 of 100000: train_loss 1.7224, val loss 1.7327\n",
      "step 95500 of 100000: train_loss 1.7066, val loss 1.6989\n",
      "step 95600 of 100000: train_loss 1.7099, val loss 1.7004\n",
      "step 95700 of 100000: train_loss 1.6604, val loss 1.7346\n",
      "step 95800 of 100000: train_loss 1.7297, val loss 1.7018\n",
      "step 95900 of 100000: train_loss 1.7102, val loss 1.7048\n",
      "step 96000 of 100000: train_loss 1.6939, val loss 1.7166\n",
      "step 96100 of 100000: train_loss 1.7368, val loss 1.7206\n",
      "step 96200 of 100000: train_loss 1.7673, val loss 1.6932\n",
      "step 96300 of 100000: train_loss 1.7220, val loss 1.6909\n",
      "step 96400 of 100000: train_loss 1.7057, val loss 1.7400\n",
      "step 96500 of 100000: train_loss 1.7043, val loss 1.7095\n",
      "step 96600 of 100000: train_loss 1.6955, val loss 1.7070\n",
      "step 96700 of 100000: train_loss 1.6974, val loss 1.6952\n",
      "step 96800 of 100000: train_loss 1.7232, val loss 1.7318\n",
      "step 96900 of 100000: train_loss 1.6963, val loss 1.6959\n",
      "step 97000 of 100000: train_loss 1.7104, val loss 1.7165\n",
      "step 97100 of 100000: train_loss 1.6940, val loss 1.7239\n",
      "step 97200 of 100000: train_loss 1.7298, val loss 1.6842\n",
      "step 97300 of 100000: train_loss 1.7448, val loss 1.7052\n",
      "step 97400 of 100000: train_loss 1.7346, val loss 1.7185\n",
      "step 97500 of 100000: train_loss 1.7349, val loss 1.6919\n",
      "step 97600 of 100000: train_loss 1.7029, val loss 1.6830\n",
      "step 97700 of 100000: train_loss 1.7126, val loss 1.7317\n",
      "step 97800 of 100000: train_loss 1.6817, val loss 1.7174\n",
      "step 97900 of 100000: train_loss 1.6984, val loss 1.6723\n",
      "step 98000 of 100000: train_loss 1.7040, val loss 1.7026\n",
      "step 98100 of 100000: train_loss 1.7200, val loss 1.6666\n",
      "step 98200 of 100000: train_loss 1.7155, val loss 1.7277\n",
      "step 98300 of 100000: train_loss 1.7129, val loss 1.7068\n",
      "step 98400 of 100000: train_loss 1.7023, val loss 1.7178\n",
      "step 98500 of 100000: train_loss 1.7033, val loss 1.7670\n",
      "step 98600 of 100000: train_loss 1.7210, val loss 1.7149\n",
      "step 98700 of 100000: train_loss 1.7220, val loss 1.7157\n",
      "step 98800 of 100000: train_loss 1.7187, val loss 1.6830\n",
      "step 98900 of 100000: train_loss 1.7452, val loss 1.7053\n",
      "step 99000 of 100000: train_loss 1.6623, val loss 1.7430\n",
      "step 99100 of 100000: train_loss 1.7256, val loss 1.6901\n",
      "step 99200 of 100000: train_loss 1.7363, val loss 1.6794\n",
      "step 99300 of 100000: train_loss 1.6771, val loss 1.7059\n",
      "step 99400 of 100000: train_loss 1.7465, val loss 1.6975\n",
      "step 99500 of 100000: train_loss 1.7363, val loss 1.7047\n",
      "step 99600 of 100000: train_loss 1.7029, val loss 1.6999\n",
      "step 99700 of 100000: train_loss 1.7410, val loss 1.7030\n",
      "step 99800 of 100000: train_loss 1.7159, val loss 1.7454\n",
      "step 99900 of 100000: train_loss 1.7258, val loss 1.7169\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(0, len(all_data2))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data2[train_inds]\n",
    "val_data = all_data2[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "\n",
    "input_dim = all_data2.shape[2]\n",
    "\n",
    "model2 = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx2,\n",
    "                causal_ordering=causal_ordering2,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types=var_types2,\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model2.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model2(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model2.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model2(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model2.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f7acec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [1.36] est ATE: [0.77096986] error: [0.58903014]\n",
      "Mean Squared Error Across All Vars: tensor(1.7172)\n",
      "Mean Squared Error Across Outcome: tensor(0.6801)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "   \n",
    "model2.eval()\n",
    "inf = inference.CausalInference(model=model2, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X1':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X1':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names2.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data2, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data2, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model2(train_data.to(device))\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1].detach().cpu().numpy())\n",
    "print('Mean Squared Error Across All Vars:', ((train_data - preds.detach().cpu())**2).mean())\n",
    "print('Mean Squared Error Across Outcome:', ((train_data[:,effect_index,:] - preds[:,effect_index,:].detach().cpu())**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b0cde",
   "metadata": {},
   "source": [
    "## Correct DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8a281b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 100000: train_loss 9.0737, val loss 8.5158\n",
      "step 100 of 100000: train_loss 3.9570, val loss 3.8544\n",
      "step 200 of 100000: train_loss 3.5596, val loss 3.5411\n",
      "step 300 of 100000: train_loss 3.4454, val loss 3.4267\n",
      "step 400 of 100000: train_loss 3.3095, val loss 3.2436\n",
      "step 500 of 100000: train_loss 3.1468, val loss 3.0873\n",
      "step 600 of 100000: train_loss 3.1445, val loss 3.1013\n",
      "step 700 of 100000: train_loss 3.0283, val loss 2.9770\n",
      "step 800 of 100000: train_loss 3.0610, val loss 3.0133\n",
      "step 900 of 100000: train_loss 2.9700, val loss 2.9667\n",
      "step 1000 of 100000: train_loss 2.9918, val loss 2.9122\n",
      "step 1100 of 100000: train_loss 2.9154, val loss 2.8948\n",
      "step 1200 of 100000: train_loss 2.8971, val loss 2.8909\n",
      "step 1300 of 100000: train_loss 2.9564, val loss 2.8773\n",
      "step 1400 of 100000: train_loss 2.8839, val loss 2.8754\n",
      "step 1500 of 100000: train_loss 2.9200, val loss 2.9182\n",
      "step 1600 of 100000: train_loss 2.9383, val loss 2.8307\n",
      "step 1700 of 100000: train_loss 2.8398, val loss 2.8423\n",
      "step 1800 of 100000: train_loss 2.9181, val loss 2.8859\n",
      "step 1900 of 100000: train_loss 2.8598, val loss 2.8617\n",
      "step 2000 of 100000: train_loss 2.9042, val loss 2.8216\n",
      "step 2100 of 100000: train_loss 2.9093, val loss 2.8645\n",
      "step 2200 of 100000: train_loss 2.9146, val loss 2.8878\n",
      "step 2300 of 100000: train_loss 2.8901, val loss 2.8651\n",
      "step 2400 of 100000: train_loss 2.9410, val loss 2.8568\n",
      "step 2500 of 100000: train_loss 2.8458, val loss 2.8309\n",
      "step 2600 of 100000: train_loss 2.8842, val loss 2.8617\n",
      "step 2700 of 100000: train_loss 2.8798, val loss 2.8762\n",
      "step 2800 of 100000: train_loss 2.8540, val loss 2.8263\n",
      "step 2900 of 100000: train_loss 2.9049, val loss 2.8522\n",
      "step 3000 of 100000: train_loss 2.9109, val loss 2.8441\n",
      "step 3100 of 100000: train_loss 2.9343, val loss 2.8085\n",
      "step 3200 of 100000: train_loss 2.8617, val loss 2.8636\n",
      "step 3300 of 100000: train_loss 2.9186, val loss 2.8871\n",
      "step 3400 of 100000: train_loss 2.9036, val loss 2.8396\n",
      "step 3500 of 100000: train_loss 2.8492, val loss 2.8485\n",
      "step 3600 of 100000: train_loss 2.9158, val loss 2.8849\n",
      "step 3700 of 100000: train_loss 2.8512, val loss 2.8519\n",
      "step 3800 of 100000: train_loss 2.9115, val loss 2.8407\n",
      "step 3900 of 100000: train_loss 2.8559, val loss 2.8214\n",
      "step 4000 of 100000: train_loss 2.9051, val loss 2.8502\n",
      "step 4100 of 100000: train_loss 2.8786, val loss 2.7788\n",
      "step 4200 of 100000: train_loss 2.8788, val loss 2.8319\n",
      "step 4300 of 100000: train_loss 2.8416, val loss 2.8885\n",
      "step 4400 of 100000: train_loss 2.8725, val loss 2.8125\n",
      "step 4500 of 100000: train_loss 2.8872, val loss 2.8897\n",
      "step 4600 of 100000: train_loss 2.8506, val loss 2.7996\n",
      "step 4700 of 100000: train_loss 2.8736, val loss 2.8270\n",
      "step 4800 of 100000: train_loss 2.8316, val loss 2.7820\n",
      "step 4900 of 100000: train_loss 2.9136, val loss 2.8405\n",
      "step 5000 of 100000: train_loss 2.8865, val loss 2.7971\n",
      "step 5100 of 100000: train_loss 2.8563, val loss 2.7970\n",
      "step 5200 of 100000: train_loss 2.8892, val loss 2.8464\n",
      "step 5300 of 100000: train_loss 2.8794, val loss 2.8084\n",
      "step 5400 of 100000: train_loss 2.8528, val loss 2.7947\n",
      "step 5500 of 100000: train_loss 2.8915, val loss 2.8553\n",
      "step 5600 of 100000: train_loss 2.8653, val loss 2.7928\n",
      "step 5700 of 100000: train_loss 2.8739, val loss 2.8017\n",
      "step 5800 of 100000: train_loss 2.8250, val loss 2.8040\n",
      "step 5900 of 100000: train_loss 2.8742, val loss 2.8143\n",
      "step 6000 of 100000: train_loss 2.9513, val loss 2.8172\n",
      "step 6100 of 100000: train_loss 2.8255, val loss 2.7856\n",
      "step 6200 of 100000: train_loss 2.8877, val loss 2.8412\n",
      "step 6300 of 100000: train_loss 2.8727, val loss 2.8007\n",
      "step 6400 of 100000: train_loss 2.8690, val loss 2.8211\n",
      "step 6500 of 100000: train_loss 2.8288, val loss 2.8391\n",
      "step 6600 of 100000: train_loss 2.8794, val loss 2.8203\n",
      "step 6700 of 100000: train_loss 2.8900, val loss 2.8002\n",
      "step 6800 of 100000: train_loss 2.8696, val loss 2.8067\n",
      "step 6900 of 100000: train_loss 2.9053, val loss 2.8635\n",
      "step 7000 of 100000: train_loss 2.8246, val loss 2.7526\n",
      "step 7100 of 100000: train_loss 2.8850, val loss 2.8354\n",
      "step 7200 of 100000: train_loss 2.8795, val loss 2.8425\n",
      "step 7300 of 100000: train_loss 2.8789, val loss 2.8525\n",
      "step 7400 of 100000: train_loss 2.8722, val loss 2.7829\n",
      "step 7500 of 100000: train_loss 2.8369, val loss 2.8138\n",
      "step 7600 of 100000: train_loss 2.8970, val loss 2.8281\n",
      "step 7700 of 100000: train_loss 2.8802, val loss 2.8252\n",
      "step 7800 of 100000: train_loss 2.8665, val loss 2.8409\n",
      "step 7900 of 100000: train_loss 2.8444, val loss 2.8012\n",
      "step 8000 of 100000: train_loss 2.9143, val loss 2.8304\n",
      "step 8100 of 100000: train_loss 2.8663, val loss 2.8487\n",
      "step 8200 of 100000: train_loss 2.8497, val loss 2.8311\n",
      "step 8300 of 100000: train_loss 2.8643, val loss 2.8244\n",
      "step 8400 of 100000: train_loss 2.8440, val loss 2.7921\n",
      "step 8500 of 100000: train_loss 2.8320, val loss 2.8094\n",
      "step 8600 of 100000: train_loss 2.8437, val loss 2.8428\n",
      "step 8700 of 100000: train_loss 2.9177, val loss 2.8536\n",
      "step 8800 of 100000: train_loss 2.8604, val loss 2.7971\n",
      "step 8900 of 100000: train_loss 2.8919, val loss 2.7893\n",
      "step 9000 of 100000: train_loss 2.8753, val loss 2.8472\n",
      "step 9100 of 100000: train_loss 2.8568, val loss 2.8548\n",
      "step 9200 of 100000: train_loss 2.9112, val loss 2.8273\n",
      "step 9300 of 100000: train_loss 2.8590, val loss 2.8151\n",
      "step 9400 of 100000: train_loss 2.8419, val loss 2.8449\n",
      "step 9500 of 100000: train_loss 2.8509, val loss 2.8244\n",
      "step 9600 of 100000: train_loss 2.8403, val loss 2.8241\n",
      "step 9700 of 100000: train_loss 2.8537, val loss 2.8596\n",
      "step 9800 of 100000: train_loss 2.8601, val loss 2.7794\n",
      "step 9900 of 100000: train_loss 2.8852, val loss 2.8191\n",
      "step 10000 of 100000: train_loss 2.8628, val loss 2.8175\n",
      "step 10100 of 100000: train_loss 2.8917, val loss 2.8704\n",
      "step 10200 of 100000: train_loss 2.8757, val loss 2.7825\n",
      "step 10300 of 100000: train_loss 2.9283, val loss 2.8579\n",
      "step 10400 of 100000: train_loss 2.9053, val loss 2.8213\n",
      "step 10500 of 100000: train_loss 2.8764, val loss 2.8052\n",
      "step 10600 of 100000: train_loss 2.8950, val loss 2.8369\n",
      "step 10700 of 100000: train_loss 2.8438, val loss 2.8311\n",
      "step 10800 of 100000: train_loss 2.8496, val loss 2.8450\n",
      "step 10900 of 100000: train_loss 2.9443, val loss 2.8228\n",
      "step 11000 of 100000: train_loss 2.8504, val loss 2.7940\n",
      "step 11100 of 100000: train_loss 2.8992, val loss 2.7697\n",
      "step 11200 of 100000: train_loss 2.8875, val loss 2.8196\n",
      "step 11300 of 100000: train_loss 2.8373, val loss 2.8522\n",
      "step 11400 of 100000: train_loss 2.8714, val loss 2.8551\n",
      "step 11500 of 100000: train_loss 2.8395, val loss 2.8277\n",
      "step 11600 of 100000: train_loss 2.8361, val loss 2.8364\n",
      "step 11700 of 100000: train_loss 2.8855, val loss 2.8280\n",
      "step 11800 of 100000: train_loss 2.8276, val loss 2.7905\n",
      "step 11900 of 100000: train_loss 2.8895, val loss 2.8545\n",
      "step 12000 of 100000: train_loss 2.8582, val loss 2.8126\n",
      "step 12100 of 100000: train_loss 2.8137, val loss 2.8614\n",
      "step 12200 of 100000: train_loss 2.8371, val loss 2.7789\n",
      "step 12300 of 100000: train_loss 2.8172, val loss 2.8383\n",
      "step 12400 of 100000: train_loss 2.9094, val loss 2.7940\n",
      "step 12500 of 100000: train_loss 2.8547, val loss 2.8668\n",
      "step 12600 of 100000: train_loss 2.9258, val loss 2.8176\n",
      "step 12700 of 100000: train_loss 2.8845, val loss 2.7963\n",
      "step 12800 of 100000: train_loss 2.8728, val loss 2.8228\n",
      "step 12900 of 100000: train_loss 2.8695, val loss 2.8037\n",
      "step 13000 of 100000: train_loss 2.8998, val loss 2.8207\n",
      "step 13100 of 100000: train_loss 2.8556, val loss 2.8423\n",
      "step 13200 of 100000: train_loss 2.8425, val loss 2.8007\n",
      "step 13300 of 100000: train_loss 2.8752, val loss 2.8340\n",
      "step 13400 of 100000: train_loss 2.8770, val loss 2.8807\n",
      "step 13500 of 100000: train_loss 2.8359, val loss 2.7837\n",
      "step 13600 of 100000: train_loss 2.8453, val loss 2.8808\n",
      "step 13700 of 100000: train_loss 2.8716, val loss 2.8453\n",
      "step 13800 of 100000: train_loss 2.8742, val loss 2.8220\n",
      "step 13900 of 100000: train_loss 2.9009, val loss 2.8610\n",
      "step 14000 of 100000: train_loss 2.8347, val loss 2.8196\n",
      "step 14100 of 100000: train_loss 2.8729, val loss 2.7943\n",
      "step 14200 of 100000: train_loss 2.8483, val loss 2.8737\n",
      "step 14300 of 100000: train_loss 2.8547, val loss 2.8315\n",
      "step 14400 of 100000: train_loss 2.8972, val loss 2.8247\n",
      "step 14500 of 100000: train_loss 2.8677, val loss 2.7823\n",
      "step 14600 of 100000: train_loss 2.8762, val loss 2.8214\n",
      "step 14700 of 100000: train_loss 2.8705, val loss 2.8610\n",
      "step 14800 of 100000: train_loss 2.8703, val loss 2.8018\n",
      "step 14900 of 100000: train_loss 2.8758, val loss 2.7914\n",
      "step 15000 of 100000: train_loss 2.8462, val loss 2.8382\n",
      "step 15100 of 100000: train_loss 2.8796, val loss 2.8061\n",
      "step 15200 of 100000: train_loss 2.8557, val loss 2.7882\n",
      "step 15300 of 100000: train_loss 2.8699, val loss 2.7886\n",
      "step 15400 of 100000: train_loss 2.8746, val loss 2.8440\n",
      "step 15500 of 100000: train_loss 2.8490, val loss 2.8216\n",
      "step 15600 of 100000: train_loss 2.8561, val loss 2.8866\n",
      "step 15700 of 100000: train_loss 2.8334, val loss 2.8029\n",
      "step 15800 of 100000: train_loss 2.8695, val loss 2.7866\n",
      "step 15900 of 100000: train_loss 2.9346, val loss 2.8714\n",
      "step 16000 of 100000: train_loss 2.8921, val loss 2.8768\n",
      "step 16100 of 100000: train_loss 2.8396, val loss 2.8376\n",
      "step 16200 of 100000: train_loss 2.8771, val loss 2.8041\n",
      "step 16300 of 100000: train_loss 2.9035, val loss 2.7943\n",
      "step 16400 of 100000: train_loss 2.8497, val loss 2.8102\n",
      "step 16500 of 100000: train_loss 2.8783, val loss 2.7957\n",
      "step 16600 of 100000: train_loss 2.8550, val loss 2.8358\n",
      "step 16700 of 100000: train_loss 2.8286, val loss 2.8192\n",
      "step 16800 of 100000: train_loss 2.8742, val loss 2.8239\n",
      "step 16900 of 100000: train_loss 2.8643, val loss 2.8166\n",
      "step 17000 of 100000: train_loss 2.8543, val loss 2.8243\n",
      "step 17100 of 100000: train_loss 2.8773, val loss 2.8956\n",
      "step 17200 of 100000: train_loss 2.9062, val loss 2.8213\n",
      "step 17300 of 100000: train_loss 2.9006, val loss 2.8464\n",
      "step 17400 of 100000: train_loss 2.8647, val loss 2.8198\n",
      "step 17500 of 100000: train_loss 2.9233, val loss 2.8376\n",
      "step 17600 of 100000: train_loss 2.8643, val loss 2.8128\n",
      "step 17700 of 100000: train_loss 2.8974, val loss 2.7850\n",
      "step 17800 of 100000: train_loss 2.8272, val loss 2.8372\n",
      "step 17900 of 100000: train_loss 2.8650, val loss 2.8080\n",
      "step 18000 of 100000: train_loss 2.8331, val loss 2.8086\n",
      "step 18100 of 100000: train_loss 2.9141, val loss 2.8229\n",
      "step 18200 of 100000: train_loss 2.8408, val loss 2.8390\n",
      "step 18300 of 100000: train_loss 2.8466, val loss 2.8533\n",
      "step 18400 of 100000: train_loss 2.9024, val loss 2.8231\n",
      "step 18500 of 100000: train_loss 2.8453, val loss 2.8487\n",
      "step 18600 of 100000: train_loss 2.8483, val loss 2.8234\n",
      "step 18700 of 100000: train_loss 2.8760, val loss 2.8052\n",
      "step 18800 of 100000: train_loss 2.8840, val loss 2.8300\n",
      "step 18900 of 100000: train_loss 2.8652, val loss 2.8043\n",
      "step 19000 of 100000: train_loss 2.8278, val loss 2.8319\n",
      "step 19100 of 100000: train_loss 2.8677, val loss 2.8257\n",
      "step 19200 of 100000: train_loss 2.8381, val loss 2.8286\n",
      "step 19300 of 100000: train_loss 2.8963, val loss 2.8042\n",
      "step 19400 of 100000: train_loss 2.8935, val loss 2.8197\n",
      "step 19500 of 100000: train_loss 2.8207, val loss 2.8313\n",
      "step 19600 of 100000: train_loss 2.8611, val loss 2.8372\n",
      "step 19700 of 100000: train_loss 2.8155, val loss 2.7988\n",
      "step 19800 of 100000: train_loss 2.8741, val loss 2.8159\n",
      "step 19900 of 100000: train_loss 2.8884, val loss 2.7970\n",
      "step 20000 of 100000: train_loss 2.8479, val loss 2.8310\n",
      "step 20100 of 100000: train_loss 2.8534, val loss 2.7796\n",
      "step 20200 of 100000: train_loss 2.8332, val loss 2.8432\n",
      "step 20300 of 100000: train_loss 2.8697, val loss 2.8298\n",
      "step 20400 of 100000: train_loss 2.8077, val loss 2.8055\n",
      "step 20500 of 100000: train_loss 2.9007, val loss 2.8719\n",
      "step 20600 of 100000: train_loss 2.9017, val loss 2.8481\n",
      "step 20700 of 100000: train_loss 2.8483, val loss 2.8094\n",
      "step 20800 of 100000: train_loss 2.8243, val loss 2.7810\n",
      "step 20900 of 100000: train_loss 2.8839, val loss 2.7841\n",
      "step 21000 of 100000: train_loss 2.8306, val loss 2.8367\n",
      "step 21100 of 100000: train_loss 2.8640, val loss 2.8223\n",
      "step 21200 of 100000: train_loss 2.8778, val loss 2.8154\n",
      "step 21300 of 100000: train_loss 2.8418, val loss 2.7843\n",
      "step 21400 of 100000: train_loss 2.8739, val loss 2.8150\n",
      "step 21500 of 100000: train_loss 2.8964, val loss 2.7756\n",
      "step 21600 of 100000: train_loss 2.8547, val loss 2.8166\n",
      "step 21700 of 100000: train_loss 2.8760, val loss 2.8267\n",
      "step 21800 of 100000: train_loss 2.8648, val loss 2.8345\n",
      "step 21900 of 100000: train_loss 2.8894, val loss 2.7756\n",
      "step 22000 of 100000: train_loss 2.8474, val loss 2.8252\n",
      "step 22100 of 100000: train_loss 2.8214, val loss 2.7996\n",
      "step 22200 of 100000: train_loss 2.8276, val loss 2.8086\n",
      "step 22300 of 100000: train_loss 2.8683, val loss 2.8159\n",
      "step 22400 of 100000: train_loss 2.8615, val loss 2.8696\n",
      "step 22500 of 100000: train_loss 2.8259, val loss 2.8480\n",
      "step 22600 of 100000: train_loss 2.8241, val loss 2.7825\n",
      "step 22700 of 100000: train_loss 2.8515, val loss 2.8091\n",
      "step 22800 of 100000: train_loss 2.8989, val loss 2.8083\n",
      "step 22900 of 100000: train_loss 2.8481, val loss 2.7978\n",
      "step 23000 of 100000: train_loss 2.8707, val loss 2.8072\n",
      "step 23100 of 100000: train_loss 2.8760, val loss 2.8060\n",
      "step 23200 of 100000: train_loss 2.8577, val loss 2.8042\n",
      "step 23300 of 100000: train_loss 2.8395, val loss 2.7739\n",
      "step 23400 of 100000: train_loss 2.8326, val loss 2.7931\n",
      "step 23500 of 100000: train_loss 2.8640, val loss 2.8087\n",
      "step 23600 of 100000: train_loss 2.8747, val loss 2.8116\n",
      "step 23700 of 100000: train_loss 2.8700, val loss 2.8563\n",
      "step 23800 of 100000: train_loss 2.8808, val loss 2.8240\n",
      "step 23900 of 100000: train_loss 2.8471, val loss 2.7690\n",
      "step 24000 of 100000: train_loss 2.8321, val loss 2.8403\n",
      "step 24100 of 100000: train_loss 2.8039, val loss 2.8553\n",
      "step 24200 of 100000: train_loss 2.8421, val loss 2.7974\n",
      "step 24300 of 100000: train_loss 2.8951, val loss 2.8290\n",
      "step 24400 of 100000: train_loss 2.8923, val loss 2.8375\n",
      "step 24500 of 100000: train_loss 2.8698, val loss 2.7873\n",
      "step 24600 of 100000: train_loss 2.8741, val loss 2.8359\n",
      "step 24700 of 100000: train_loss 2.8690, val loss 2.7974\n",
      "step 24800 of 100000: train_loss 2.8337, val loss 2.8109\n",
      "step 24900 of 100000: train_loss 2.8914, val loss 2.8069\n",
      "step 25000 of 100000: train_loss 2.8383, val loss 2.8359\n",
      "step 25100 of 100000: train_loss 2.8538, val loss 2.7841\n",
      "step 25200 of 100000: train_loss 2.8636, val loss 2.7862\n",
      "step 25300 of 100000: train_loss 2.7907, val loss 2.7938\n",
      "step 25400 of 100000: train_loss 2.8775, val loss 2.7860\n",
      "step 25500 of 100000: train_loss 2.8625, val loss 2.8120\n",
      "step 25600 of 100000: train_loss 2.8551, val loss 2.8232\n",
      "step 25700 of 100000: train_loss 2.8644, val loss 2.8185\n",
      "step 25800 of 100000: train_loss 2.8599, val loss 2.7898\n",
      "step 25900 of 100000: train_loss 2.8636, val loss 2.8178\n",
      "step 26000 of 100000: train_loss 2.8200, val loss 2.8092\n",
      "step 26100 of 100000: train_loss 2.8604, val loss 2.8334\n",
      "step 26200 of 100000: train_loss 2.8511, val loss 2.8198\n",
      "step 26300 of 100000: train_loss 2.8685, val loss 2.7777\n",
      "step 26400 of 100000: train_loss 2.8668, val loss 2.7998\n",
      "step 26500 of 100000: train_loss 2.8548, val loss 2.8223\n",
      "step 26600 of 100000: train_loss 2.8275, val loss 2.8195\n",
      "step 26700 of 100000: train_loss 2.8516, val loss 2.8136\n",
      "step 26800 of 100000: train_loss 2.8673, val loss 2.8320\n",
      "step 26900 of 100000: train_loss 2.8492, val loss 2.7834\n",
      "step 27000 of 100000: train_loss 2.8699, val loss 2.8228\n",
      "step 27100 of 100000: train_loss 2.8805, val loss 2.8426\n",
      "step 27200 of 100000: train_loss 2.8650, val loss 2.8084\n",
      "step 27300 of 100000: train_loss 2.8424, val loss 2.8463\n",
      "step 27400 of 100000: train_loss 2.8598, val loss 2.8436\n",
      "step 27500 of 100000: train_loss 2.8998, val loss 2.8063\n",
      "step 27600 of 100000: train_loss 2.8414, val loss 2.7651\n",
      "step 27700 of 100000: train_loss 2.8962, val loss 2.8184\n",
      "step 27800 of 100000: train_loss 2.8481, val loss 2.7762\n",
      "step 27900 of 100000: train_loss 2.8369, val loss 2.8469\n",
      "step 28000 of 100000: train_loss 2.8368, val loss 2.8159\n",
      "step 28100 of 100000: train_loss 2.8561, val loss 2.8375\n",
      "step 28200 of 100000: train_loss 2.8534, val loss 2.7653\n",
      "step 28300 of 100000: train_loss 2.8853, val loss 2.8057\n",
      "step 28400 of 100000: train_loss 2.8547, val loss 2.8516\n",
      "step 28500 of 100000: train_loss 2.8815, val loss 2.8674\n",
      "step 28600 of 100000: train_loss 2.8883, val loss 2.8490\n",
      "step 28700 of 100000: train_loss 2.8173, val loss 2.8169\n",
      "step 28800 of 100000: train_loss 2.8522, val loss 2.7967\n",
      "step 28900 of 100000: train_loss 2.8359, val loss 2.8166\n",
      "step 29000 of 100000: train_loss 2.8262, val loss 2.8419\n",
      "step 29100 of 100000: train_loss 2.8509, val loss 2.8063\n",
      "step 29200 of 100000: train_loss 2.8214, val loss 2.7860\n",
      "step 29300 of 100000: train_loss 2.8223, val loss 2.8296\n",
      "step 29400 of 100000: train_loss 2.8790, val loss 2.8328\n",
      "step 29500 of 100000: train_loss 2.8648, val loss 2.8298\n",
      "step 29600 of 100000: train_loss 2.8799, val loss 2.8350\n",
      "step 29700 of 100000: train_loss 2.8537, val loss 2.8209\n",
      "step 29800 of 100000: train_loss 2.8275, val loss 2.8399\n",
      "step 29900 of 100000: train_loss 2.8895, val loss 2.8222\n",
      "step 30000 of 100000: train_loss 2.8379, val loss 2.8218\n",
      "step 30100 of 100000: train_loss 2.8702, val loss 2.8341\n",
      "step 30200 of 100000: train_loss 2.8456, val loss 2.8235\n",
      "step 30300 of 100000: train_loss 2.8594, val loss 2.8073\n",
      "step 30400 of 100000: train_loss 2.8537, val loss 2.8283\n",
      "step 30500 of 100000: train_loss 2.8389, val loss 2.8216\n",
      "step 30600 of 100000: train_loss 2.8470, val loss 2.8552\n",
      "step 30700 of 100000: train_loss 2.8866, val loss 2.8072\n",
      "step 30800 of 100000: train_loss 2.8965, val loss 2.7684\n",
      "step 30900 of 100000: train_loss 2.8287, val loss 2.8338\n",
      "step 31000 of 100000: train_loss 2.8556, val loss 2.8064\n",
      "step 31100 of 100000: train_loss 2.8660, val loss 2.8430\n",
      "step 31200 of 100000: train_loss 2.8750, val loss 2.7585\n",
      "step 31300 of 100000: train_loss 2.8309, val loss 2.8239\n",
      "step 31400 of 100000: train_loss 2.8475, val loss 2.8113\n",
      "step 31500 of 100000: train_loss 2.8291, val loss 2.7701\n",
      "step 31600 of 100000: train_loss 2.8973, val loss 2.8039\n",
      "step 31700 of 100000: train_loss 2.8645, val loss 2.7892\n",
      "step 31800 of 100000: train_loss 2.8705, val loss 2.7814\n",
      "step 31900 of 100000: train_loss 2.8611, val loss 2.8229\n",
      "step 32000 of 100000: train_loss 2.8288, val loss 2.8225\n",
      "step 32100 of 100000: train_loss 2.8547, val loss 2.8120\n",
      "step 32200 of 100000: train_loss 2.8605, val loss 2.7963\n",
      "step 32300 of 100000: train_loss 2.8953, val loss 2.8433\n",
      "step 32400 of 100000: train_loss 2.8134, val loss 2.8659\n",
      "step 32500 of 100000: train_loss 2.8668, val loss 2.7940\n",
      "step 32600 of 100000: train_loss 2.8390, val loss 2.7779\n",
      "step 32700 of 100000: train_loss 2.8577, val loss 2.8293\n",
      "step 32800 of 100000: train_loss 2.8353, val loss 2.8091\n",
      "step 32900 of 100000: train_loss 2.8575, val loss 2.8195\n",
      "step 33000 of 100000: train_loss 2.8576, val loss 2.8197\n",
      "step 33100 of 100000: train_loss 2.8471, val loss 2.8182\n",
      "step 33200 of 100000: train_loss 2.8409, val loss 2.7994\n",
      "step 33300 of 100000: train_loss 2.8848, val loss 2.7850\n",
      "step 33400 of 100000: train_loss 2.8528, val loss 2.7912\n",
      "step 33500 of 100000: train_loss 2.8523, val loss 2.8418\n",
      "step 33600 of 100000: train_loss 2.8864, val loss 2.8218\n",
      "step 33700 of 100000: train_loss 2.8624, val loss 2.8156\n",
      "step 33800 of 100000: train_loss 2.8392, val loss 2.8150\n",
      "step 33900 of 100000: train_loss 2.8771, val loss 2.7903\n",
      "step 34000 of 100000: train_loss 2.7937, val loss 2.8101\n",
      "step 34100 of 100000: train_loss 2.8789, val loss 2.8142\n",
      "step 34200 of 100000: train_loss 2.8208, val loss 2.8019\n",
      "step 34300 of 100000: train_loss 2.8443, val loss 2.8301\n",
      "step 34400 of 100000: train_loss 2.8435, val loss 2.8008\n",
      "step 34500 of 100000: train_loss 2.8513, val loss 2.7885\n",
      "step 34600 of 100000: train_loss 2.9049, val loss 2.7856\n",
      "step 34700 of 100000: train_loss 2.8942, val loss 2.8280\n",
      "step 34800 of 100000: train_loss 2.7989, val loss 2.7837\n",
      "step 34900 of 100000: train_loss 2.8207, val loss 2.7947\n",
      "step 35000 of 100000: train_loss 2.8965, val loss 2.8559\n",
      "step 35100 of 100000: train_loss 2.8715, val loss 2.8125\n",
      "step 35200 of 100000: train_loss 2.8824, val loss 2.7790\n",
      "step 35300 of 100000: train_loss 2.8420, val loss 2.7985\n",
      "step 35400 of 100000: train_loss 2.8502, val loss 2.8775\n",
      "step 35500 of 100000: train_loss 2.8323, val loss 2.8132\n",
      "step 35600 of 100000: train_loss 2.8587, val loss 2.8008\n",
      "step 35700 of 100000: train_loss 2.8317, val loss 2.8080\n",
      "step 35800 of 100000: train_loss 2.8676, val loss 2.8428\n",
      "step 35900 of 100000: train_loss 2.8696, val loss 2.7842\n",
      "step 36000 of 100000: train_loss 2.8690, val loss 2.8267\n",
      "step 36100 of 100000: train_loss 2.8929, val loss 2.8200\n",
      "step 36200 of 100000: train_loss 2.8338, val loss 2.8413\n",
      "step 36300 of 100000: train_loss 2.8374, val loss 2.8406\n",
      "step 36400 of 100000: train_loss 2.8863, val loss 2.8333\n",
      "step 36500 of 100000: train_loss 2.8512, val loss 2.8279\n",
      "step 36600 of 100000: train_loss 2.7951, val loss 2.8240\n",
      "step 36700 of 100000: train_loss 2.8269, val loss 2.8387\n",
      "step 36800 of 100000: train_loss 2.8349, val loss 2.8097\n",
      "step 36900 of 100000: train_loss 2.8442, val loss 2.8061\n",
      "step 37000 of 100000: train_loss 2.8427, val loss 2.7915\n",
      "step 37100 of 100000: train_loss 2.8484, val loss 2.7921\n",
      "step 37200 of 100000: train_loss 2.9023, val loss 2.8364\n",
      "step 37300 of 100000: train_loss 2.8549, val loss 2.8924\n",
      "step 37400 of 100000: train_loss 2.8553, val loss 2.8160\n",
      "step 37500 of 100000: train_loss 2.8190, val loss 2.7890\n",
      "step 37600 of 100000: train_loss 2.8729, val loss 2.8203\n",
      "step 37700 of 100000: train_loss 2.8730, val loss 2.8169\n",
      "step 37800 of 100000: train_loss 2.8486, val loss 2.7802\n",
      "step 37900 of 100000: train_loss 2.8416, val loss 2.8154\n",
      "step 38000 of 100000: train_loss 2.8692, val loss 2.8088\n",
      "step 38100 of 100000: train_loss 2.8868, val loss 2.8142\n",
      "step 38200 of 100000: train_loss 2.8299, val loss 2.8134\n",
      "step 38300 of 100000: train_loss 2.8879, val loss 2.8474\n",
      "step 38400 of 100000: train_loss 2.8797, val loss 2.8009\n",
      "step 38500 of 100000: train_loss 2.8362, val loss 2.7755\n",
      "step 38600 of 100000: train_loss 2.8616, val loss 2.8260\n",
      "step 38700 of 100000: train_loss 2.8531, val loss 2.8507\n",
      "step 38800 of 100000: train_loss 2.8619, val loss 2.8135\n",
      "step 38900 of 100000: train_loss 2.8423, val loss 2.7947\n",
      "step 39000 of 100000: train_loss 2.8665, val loss 2.8294\n",
      "step 39100 of 100000: train_loss 2.8247, val loss 2.8039\n",
      "step 39200 of 100000: train_loss 2.8417, val loss 2.8161\n",
      "step 39300 of 100000: train_loss 2.8611, val loss 2.8197\n",
      "step 39400 of 100000: train_loss 2.8295, val loss 2.8643\n",
      "step 39500 of 100000: train_loss 2.9169, val loss 2.8072\n",
      "step 39600 of 100000: train_loss 2.8266, val loss 2.8438\n",
      "step 39700 of 100000: train_loss 2.8553, val loss 2.8185\n",
      "step 39800 of 100000: train_loss 2.8811, val loss 2.8038\n",
      "step 39900 of 100000: train_loss 2.8561, val loss 2.7896\n",
      "step 40000 of 100000: train_loss 2.8781, val loss 2.8028\n",
      "step 40100 of 100000: train_loss 2.8324, val loss 2.8188\n",
      "step 40200 of 100000: train_loss 2.8539, val loss 2.8163\n",
      "step 40300 of 100000: train_loss 2.8816, val loss 2.8005\n",
      "step 40400 of 100000: train_loss 2.8182, val loss 2.8492\n",
      "step 40500 of 100000: train_loss 2.8293, val loss 2.8392\n",
      "step 40600 of 100000: train_loss 2.8945, val loss 2.8568\n",
      "step 40700 of 100000: train_loss 2.8399, val loss 2.8281\n",
      "step 40800 of 100000: train_loss 2.8470, val loss 2.8372\n",
      "step 40900 of 100000: train_loss 2.8652, val loss 2.8094\n",
      "step 41000 of 100000: train_loss 2.8576, val loss 2.8332\n",
      "step 41100 of 100000: train_loss 2.8536, val loss 2.7887\n",
      "step 41200 of 100000: train_loss 2.8748, val loss 2.8065\n",
      "step 41300 of 100000: train_loss 2.8681, val loss 2.8077\n",
      "step 41400 of 100000: train_loss 2.8659, val loss 2.7958\n",
      "step 41500 of 100000: train_loss 2.8644, val loss 2.8266\n",
      "step 41600 of 100000: train_loss 2.8593, val loss 2.8387\n",
      "step 41700 of 100000: train_loss 2.8772, val loss 2.8113\n",
      "step 41800 of 100000: train_loss 2.8512, val loss 2.8103\n",
      "step 41900 of 100000: train_loss 2.8075, val loss 2.8019\n",
      "step 42000 of 100000: train_loss 2.8674, val loss 2.7993\n",
      "step 42100 of 100000: train_loss 2.8199, val loss 2.7940\n",
      "step 42200 of 100000: train_loss 2.9032, val loss 2.8111\n",
      "step 42300 of 100000: train_loss 2.8688, val loss 2.8217\n",
      "step 42400 of 100000: train_loss 2.8472, val loss 2.7971\n",
      "step 42500 of 100000: train_loss 2.8350, val loss 2.8246\n",
      "step 42600 of 100000: train_loss 2.8234, val loss 2.8188\n",
      "step 42700 of 100000: train_loss 2.8529, val loss 2.8082\n",
      "step 42800 of 100000: train_loss 2.8423, val loss 2.8283\n",
      "step 42900 of 100000: train_loss 2.8831, val loss 2.7868\n",
      "step 43000 of 100000: train_loss 2.8440, val loss 2.8360\n",
      "step 43100 of 100000: train_loss 2.8502, val loss 2.8658\n",
      "step 43200 of 100000: train_loss 2.8650, val loss 2.8098\n",
      "step 43300 of 100000: train_loss 2.8360, val loss 2.7804\n",
      "step 43400 of 100000: train_loss 2.8310, val loss 2.8081\n",
      "step 43500 of 100000: train_loss 2.8827, val loss 2.8582\n",
      "step 43600 of 100000: train_loss 2.8615, val loss 2.7720\n",
      "step 43700 of 100000: train_loss 2.8667, val loss 2.8427\n",
      "step 43800 of 100000: train_loss 2.8789, val loss 2.7942\n",
      "step 43900 of 100000: train_loss 2.8782, val loss 2.8213\n",
      "step 44000 of 100000: train_loss 2.8298, val loss 2.8419\n",
      "step 44100 of 100000: train_loss 2.8574, val loss 2.8375\n",
      "step 44200 of 100000: train_loss 2.8799, val loss 2.7953\n",
      "step 44300 of 100000: train_loss 2.8370, val loss 2.7826\n",
      "step 44400 of 100000: train_loss 2.8018, val loss 2.8408\n",
      "step 44500 of 100000: train_loss 2.8491, val loss 2.8714\n",
      "step 44600 of 100000: train_loss 2.8671, val loss 2.8479\n",
      "step 44700 of 100000: train_loss 2.8780, val loss 2.7905\n",
      "step 44800 of 100000: train_loss 2.8500, val loss 2.8329\n",
      "step 44900 of 100000: train_loss 2.8227, val loss 2.7985\n",
      "step 45000 of 100000: train_loss 2.8467, val loss 2.8421\n",
      "step 45100 of 100000: train_loss 2.8428, val loss 2.8139\n",
      "step 45200 of 100000: train_loss 2.8601, val loss 2.8306\n",
      "step 45300 of 100000: train_loss 2.8860, val loss 2.8246\n",
      "step 45400 of 100000: train_loss 2.8631, val loss 2.8021\n",
      "step 45500 of 100000: train_loss 2.8897, val loss 2.8351\n",
      "step 45600 of 100000: train_loss 2.8799, val loss 2.8018\n",
      "step 45700 of 100000: train_loss 2.8882, val loss 2.8303\n",
      "step 45800 of 100000: train_loss 2.8661, val loss 2.8406\n",
      "step 45900 of 100000: train_loss 2.8530, val loss 2.8408\n",
      "step 46000 of 100000: train_loss 2.8669, val loss 2.8223\n",
      "step 46100 of 100000: train_loss 2.8070, val loss 2.8240\n",
      "step 46200 of 100000: train_loss 2.8810, val loss 2.8461\n",
      "step 46300 of 100000: train_loss 2.8861, val loss 2.7941\n",
      "step 46400 of 100000: train_loss 2.8457, val loss 2.8188\n",
      "step 46500 of 100000: train_loss 2.8216, val loss 2.8055\n",
      "step 46600 of 100000: train_loss 2.8612, val loss 2.8159\n",
      "step 46700 of 100000: train_loss 2.8744, val loss 2.8457\n",
      "step 46800 of 100000: train_loss 2.8593, val loss 2.8041\n",
      "step 46900 of 100000: train_loss 2.8408, val loss 2.7860\n",
      "step 47000 of 100000: train_loss 2.8609, val loss 2.8267\n",
      "step 47100 of 100000: train_loss 2.8500, val loss 2.8281\n",
      "step 47200 of 100000: train_loss 2.8647, val loss 2.8358\n",
      "step 47300 of 100000: train_loss 2.8631, val loss 2.7619\n",
      "step 47400 of 100000: train_loss 2.8130, val loss 2.7963\n",
      "step 47500 of 100000: train_loss 2.8181, val loss 2.7994\n",
      "step 47600 of 100000: train_loss 2.8652, val loss 2.8292\n",
      "step 47700 of 100000: train_loss 2.8351, val loss 2.8362\n",
      "step 47800 of 100000: train_loss 2.8651, val loss 2.7943\n",
      "step 47900 of 100000: train_loss 2.8797, val loss 2.7971\n",
      "step 48000 of 100000: train_loss 2.8240, val loss 2.8106\n",
      "step 48100 of 100000: train_loss 2.8238, val loss 2.8307\n",
      "step 48200 of 100000: train_loss 2.8716, val loss 2.7704\n",
      "step 48300 of 100000: train_loss 2.8296, val loss 2.8256\n",
      "step 48400 of 100000: train_loss 2.8224, val loss 2.8090\n",
      "step 48500 of 100000: train_loss 2.8688, val loss 2.7937\n",
      "step 48600 of 100000: train_loss 2.8386, val loss 2.8279\n",
      "step 48700 of 100000: train_loss 2.8858, val loss 2.8045\n",
      "step 48800 of 100000: train_loss 2.8602, val loss 2.8338\n",
      "step 48900 of 100000: train_loss 2.8807, val loss 2.8130\n",
      "step 49000 of 100000: train_loss 2.8501, val loss 2.8394\n",
      "step 49100 of 100000: train_loss 2.8260, val loss 2.7420\n",
      "step 49200 of 100000: train_loss 2.8401, val loss 2.8331\n",
      "step 49300 of 100000: train_loss 2.8654, val loss 2.8272\n",
      "step 49400 of 100000: train_loss 2.8658, val loss 2.7883\n",
      "step 49500 of 100000: train_loss 2.8419, val loss 2.7599\n",
      "step 49600 of 100000: train_loss 2.8581, val loss 2.8415\n",
      "step 49700 of 100000: train_loss 2.8735, val loss 2.8027\n",
      "step 49800 of 100000: train_loss 2.8566, val loss 2.8364\n",
      "step 49900 of 100000: train_loss 2.8539, val loss 2.8076\n",
      "step 50000 of 100000: train_loss 2.8489, val loss 2.8251\n",
      "step 50100 of 100000: train_loss 2.8687, val loss 2.7568\n",
      "step 50200 of 100000: train_loss 2.8443, val loss 2.8317\n",
      "step 50300 of 100000: train_loss 2.8172, val loss 2.8484\n",
      "step 50400 of 100000: train_loss 2.8554, val loss 2.8080\n",
      "step 50500 of 100000: train_loss 2.8744, val loss 2.8162\n",
      "step 50600 of 100000: train_loss 2.8234, val loss 2.7906\n",
      "step 50700 of 100000: train_loss 2.8122, val loss 2.7774\n",
      "step 50800 of 100000: train_loss 2.8447, val loss 2.8139\n",
      "step 50900 of 100000: train_loss 2.8321, val loss 2.7944\n",
      "step 51000 of 100000: train_loss 2.8419, val loss 2.8334\n",
      "step 51100 of 100000: train_loss 2.8447, val loss 2.8561\n",
      "step 51200 of 100000: train_loss 2.8173, val loss 2.7904\n",
      "step 51300 of 100000: train_loss 2.9019, val loss 2.7876\n",
      "step 51400 of 100000: train_loss 2.8671, val loss 2.8978\n",
      "step 51500 of 100000: train_loss 2.8081, val loss 2.8141\n",
      "step 51600 of 100000: train_loss 2.8311, val loss 2.7829\n",
      "step 51700 of 100000: train_loss 2.8673, val loss 2.7940\n",
      "step 51800 of 100000: train_loss 2.8811, val loss 2.8376\n",
      "step 51900 of 100000: train_loss 2.8165, val loss 2.8248\n",
      "step 52000 of 100000: train_loss 2.8673, val loss 2.8241\n",
      "step 52100 of 100000: train_loss 2.8628, val loss 2.8024\n",
      "step 52200 of 100000: train_loss 2.8588, val loss 2.7969\n",
      "step 52300 of 100000: train_loss 2.8617, val loss 2.8070\n",
      "step 52400 of 100000: train_loss 2.8318, val loss 2.7805\n",
      "step 52500 of 100000: train_loss 2.8663, val loss 2.8346\n",
      "step 52600 of 100000: train_loss 2.8443, val loss 2.8558\n",
      "step 52700 of 100000: train_loss 2.8431, val loss 2.8202\n",
      "step 52800 of 100000: train_loss 2.8982, val loss 2.7692\n",
      "step 52900 of 100000: train_loss 2.8904, val loss 2.8105\n",
      "step 53000 of 100000: train_loss 2.8438, val loss 2.8078\n",
      "step 53100 of 100000: train_loss 2.8472, val loss 2.8117\n",
      "step 53200 of 100000: train_loss 2.8443, val loss 2.8068\n",
      "step 53300 of 100000: train_loss 2.8691, val loss 2.8219\n",
      "step 53400 of 100000: train_loss 2.8480, val loss 2.8170\n",
      "step 53500 of 100000: train_loss 2.8680, val loss 2.8698\n",
      "step 53600 of 100000: train_loss 2.8726, val loss 2.8125\n",
      "step 53700 of 100000: train_loss 2.8714, val loss 2.8179\n",
      "step 53800 of 100000: train_loss 2.8453, val loss 2.8117\n",
      "step 53900 of 100000: train_loss 2.8513, val loss 2.8167\n",
      "step 54000 of 100000: train_loss 2.9096, val loss 2.8318\n",
      "step 54100 of 100000: train_loss 2.8514, val loss 2.8482\n",
      "step 54200 of 100000: train_loss 2.8440, val loss 2.8169\n",
      "step 54300 of 100000: train_loss 2.8271, val loss 2.8273\n",
      "step 54400 of 100000: train_loss 2.8800, val loss 2.8406\n",
      "step 54500 of 100000: train_loss 2.9014, val loss 2.8422\n",
      "step 54600 of 100000: train_loss 2.8168, val loss 2.8060\n",
      "step 54700 of 100000: train_loss 2.8605, val loss 2.7911\n",
      "step 54800 of 100000: train_loss 2.8529, val loss 2.8482\n",
      "step 54900 of 100000: train_loss 2.8471, val loss 2.8118\n",
      "step 55000 of 100000: train_loss 2.8295, val loss 2.7992\n",
      "step 55100 of 100000: train_loss 2.8508, val loss 2.7504\n",
      "step 55200 of 100000: train_loss 2.8081, val loss 2.7790\n",
      "step 55300 of 100000: train_loss 2.9126, val loss 2.7797\n",
      "step 55400 of 100000: train_loss 2.8491, val loss 2.7892\n",
      "step 55500 of 100000: train_loss 2.8131, val loss 2.8231\n",
      "step 55600 of 100000: train_loss 2.8795, val loss 2.8074\n",
      "step 55700 of 100000: train_loss 2.8794, val loss 2.8294\n",
      "step 55800 of 100000: train_loss 2.8433, val loss 2.8425\n",
      "step 55900 of 100000: train_loss 2.8782, val loss 2.7895\n",
      "step 56000 of 100000: train_loss 2.8392, val loss 2.8040\n",
      "step 56100 of 100000: train_loss 2.8467, val loss 2.7569\n",
      "step 56200 of 100000: train_loss 2.8966, val loss 2.8116\n",
      "step 56300 of 100000: train_loss 2.8399, val loss 2.8093\n",
      "step 56400 of 100000: train_loss 2.7955, val loss 2.8301\n",
      "step 56500 of 100000: train_loss 2.8317, val loss 2.8213\n",
      "step 56600 of 100000: train_loss 2.8289, val loss 2.8414\n",
      "step 56700 of 100000: train_loss 2.8489, val loss 2.8284\n",
      "step 56800 of 100000: train_loss 2.8236, val loss 2.7880\n",
      "step 56900 of 100000: train_loss 2.8632, val loss 2.8216\n",
      "step 57000 of 100000: train_loss 2.8508, val loss 2.8261\n",
      "step 57100 of 100000: train_loss 2.8782, val loss 2.8034\n",
      "step 57200 of 100000: train_loss 2.8666, val loss 2.8226\n",
      "step 57300 of 100000: train_loss 2.8417, val loss 2.8237\n",
      "step 57400 of 100000: train_loss 2.8824, val loss 2.7891\n",
      "step 57500 of 100000: train_loss 2.8886, val loss 2.8196\n",
      "step 57600 of 100000: train_loss 2.8743, val loss 2.7994\n",
      "step 57700 of 100000: train_loss 2.8514, val loss 2.8302\n",
      "step 57800 of 100000: train_loss 2.8555, val loss 2.8320\n",
      "step 57900 of 100000: train_loss 2.8562, val loss 2.8477\n",
      "step 58000 of 100000: train_loss 2.8591, val loss 2.8101\n",
      "step 58100 of 100000: train_loss 2.8100, val loss 2.8320\n",
      "step 58200 of 100000: train_loss 2.8739, val loss 2.8153\n",
      "step 58300 of 100000: train_loss 2.8609, val loss 2.7987\n",
      "step 58400 of 100000: train_loss 2.8637, val loss 2.7732\n",
      "step 58500 of 100000: train_loss 2.8411, val loss 2.8316\n",
      "step 58600 of 100000: train_loss 2.8531, val loss 2.8186\n",
      "step 58700 of 100000: train_loss 2.8387, val loss 2.8102\n",
      "step 58800 of 100000: train_loss 2.8529, val loss 2.8035\n",
      "step 58900 of 100000: train_loss 2.8297, val loss 2.8297\n",
      "step 59000 of 100000: train_loss 2.8557, val loss 2.8242\n",
      "step 59100 of 100000: train_loss 2.8772, val loss 2.8129\n",
      "step 59200 of 100000: train_loss 2.8419, val loss 2.8414\n",
      "step 59300 of 100000: train_loss 2.8429, val loss 2.8345\n",
      "step 59400 of 100000: train_loss 2.8324, val loss 2.8071\n",
      "step 59500 of 100000: train_loss 2.8220, val loss 2.8286\n",
      "step 59600 of 100000: train_loss 2.8475, val loss 2.8208\n",
      "step 59700 of 100000: train_loss 2.8864, val loss 2.8469\n",
      "step 59800 of 100000: train_loss 2.8568, val loss 2.8160\n",
      "step 59900 of 100000: train_loss 2.8088, val loss 2.8101\n",
      "step 60000 of 100000: train_loss 2.8451, val loss 2.8479\n",
      "step 60100 of 100000: train_loss 2.8664, val loss 2.8551\n",
      "step 60200 of 100000: train_loss 2.8893, val loss 2.8164\n",
      "step 60300 of 100000: train_loss 2.8659, val loss 2.7955\n",
      "step 60400 of 100000: train_loss 2.8745, val loss 2.7982\n",
      "step 60500 of 100000: train_loss 2.8378, val loss 2.7989\n",
      "step 60600 of 100000: train_loss 2.8198, val loss 2.8069\n",
      "step 60700 of 100000: train_loss 2.8525, val loss 2.8274\n",
      "step 60800 of 100000: train_loss 2.8822, val loss 2.8129\n",
      "step 60900 of 100000: train_loss 2.8734, val loss 2.8108\n",
      "step 61000 of 100000: train_loss 2.8467, val loss 2.8203\n",
      "step 61100 of 100000: train_loss 2.8843, val loss 2.8336\n",
      "step 61200 of 100000: train_loss 2.8484, val loss 2.7873\n",
      "step 61300 of 100000: train_loss 2.8505, val loss 2.7871\n",
      "step 61400 of 100000: train_loss 2.9070, val loss 2.8004\n",
      "step 61500 of 100000: train_loss 2.8390, val loss 2.7925\n",
      "step 61600 of 100000: train_loss 2.8433, val loss 2.8456\n",
      "step 61700 of 100000: train_loss 2.8619, val loss 2.8176\n",
      "step 61800 of 100000: train_loss 2.8405, val loss 2.8114\n",
      "step 61900 of 100000: train_loss 2.8686, val loss 2.8474\n",
      "step 62000 of 100000: train_loss 2.8811, val loss 2.8048\n",
      "step 62100 of 100000: train_loss 2.8240, val loss 2.8331\n",
      "step 62200 of 100000: train_loss 2.8344, val loss 2.8114\n",
      "step 62300 of 100000: train_loss 2.8537, val loss 2.7795\n",
      "step 62400 of 100000: train_loss 2.8606, val loss 2.8002\n",
      "step 62500 of 100000: train_loss 2.8457, val loss 2.7867\n",
      "step 62600 of 100000: train_loss 2.8062, val loss 2.8074\n",
      "step 62700 of 100000: train_loss 2.8469, val loss 2.8310\n",
      "step 62800 of 100000: train_loss 2.8792, val loss 2.7692\n",
      "step 62900 of 100000: train_loss 2.8315, val loss 2.8110\n",
      "step 63000 of 100000: train_loss 2.8579, val loss 2.7766\n",
      "step 63100 of 100000: train_loss 2.8065, val loss 2.8035\n",
      "step 63200 of 100000: train_loss 2.8717, val loss 2.8320\n",
      "step 63300 of 100000: train_loss 2.8379, val loss 2.7861\n",
      "step 63400 of 100000: train_loss 2.8975, val loss 2.8298\n",
      "step 63500 of 100000: train_loss 2.8500, val loss 2.8237\n",
      "step 63600 of 100000: train_loss 2.8812, val loss 2.8209\n",
      "step 63700 of 100000: train_loss 2.8386, val loss 2.8152\n",
      "step 63800 of 100000: train_loss 2.8172, val loss 2.8644\n",
      "step 63900 of 100000: train_loss 2.8228, val loss 2.8139\n",
      "step 64000 of 100000: train_loss 2.8588, val loss 2.8304\n",
      "step 64100 of 100000: train_loss 2.8709, val loss 2.8271\n",
      "step 64200 of 100000: train_loss 2.8424, val loss 2.8380\n",
      "step 64300 of 100000: train_loss 2.8902, val loss 2.8627\n",
      "step 64400 of 100000: train_loss 2.8648, val loss 2.7864\n",
      "step 64500 of 100000: train_loss 2.8586, val loss 2.8586\n",
      "step 64600 of 100000: train_loss 2.8308, val loss 2.8505\n",
      "step 64700 of 100000: train_loss 2.8523, val loss 2.8038\n",
      "step 64800 of 100000: train_loss 2.8530, val loss 2.7774\n",
      "step 64900 of 100000: train_loss 2.8401, val loss 2.7780\n",
      "step 65000 of 100000: train_loss 2.8608, val loss 2.8084\n",
      "step 65100 of 100000: train_loss 2.8325, val loss 2.8219\n",
      "step 65200 of 100000: train_loss 2.8618, val loss 2.8167\n",
      "step 65300 of 100000: train_loss 2.8661, val loss 2.8066\n",
      "step 65400 of 100000: train_loss 2.8530, val loss 2.7941\n",
      "step 65500 of 100000: train_loss 2.8495, val loss 2.8078\n",
      "step 65600 of 100000: train_loss 2.8657, val loss 2.8013\n",
      "step 65700 of 100000: train_loss 2.8495, val loss 2.8174\n",
      "step 65800 of 100000: train_loss 2.8244, val loss 2.8178\n",
      "step 65900 of 100000: train_loss 2.8125, val loss 2.8000\n",
      "step 66000 of 100000: train_loss 2.8304, val loss 2.7982\n",
      "step 66100 of 100000: train_loss 2.8659, val loss 2.8249\n",
      "step 66200 of 100000: train_loss 2.8139, val loss 2.8731\n",
      "step 66300 of 100000: train_loss 2.8643, val loss 2.7696\n",
      "step 66400 of 100000: train_loss 2.8434, val loss 2.7533\n",
      "step 66500 of 100000: train_loss 2.8864, val loss 2.8041\n",
      "step 66600 of 100000: train_loss 2.8099, val loss 2.8347\n",
      "step 66700 of 100000: train_loss 2.8791, val loss 2.7868\n",
      "step 66800 of 100000: train_loss 2.8719, val loss 2.7849\n",
      "step 66900 of 100000: train_loss 2.8316, val loss 2.8097\n",
      "step 67000 of 100000: train_loss 2.8772, val loss 2.8122\n",
      "step 67100 of 100000: train_loss 2.8558, val loss 2.7880\n",
      "step 67200 of 100000: train_loss 2.8404, val loss 2.8063\n",
      "step 67300 of 100000: train_loss 2.8150, val loss 2.8156\n",
      "step 67400 of 100000: train_loss 2.8244, val loss 2.8128\n",
      "step 67500 of 100000: train_loss 2.8603, val loss 2.8200\n",
      "step 67600 of 100000: train_loss 2.8381, val loss 2.8268\n",
      "step 67700 of 100000: train_loss 2.8235, val loss 2.8012\n",
      "step 67800 of 100000: train_loss 2.8371, val loss 2.8179\n",
      "step 67900 of 100000: train_loss 2.8582, val loss 2.7627\n",
      "step 68000 of 100000: train_loss 2.8712, val loss 2.8198\n",
      "step 68100 of 100000: train_loss 2.8196, val loss 2.7962\n",
      "step 68200 of 100000: train_loss 2.8325, val loss 2.7612\n",
      "step 68300 of 100000: train_loss 2.8362, val loss 2.7966\n",
      "step 68400 of 100000: train_loss 2.8532, val loss 2.8400\n",
      "step 68500 of 100000: train_loss 2.8266, val loss 2.7759\n",
      "step 68600 of 100000: train_loss 2.8675, val loss 2.8064\n",
      "step 68700 of 100000: train_loss 2.8660, val loss 2.8143\n",
      "step 68800 of 100000: train_loss 2.8208, val loss 2.8049\n",
      "step 68900 of 100000: train_loss 2.8390, val loss 2.8400\n",
      "step 69000 of 100000: train_loss 2.8567, val loss 2.8575\n",
      "step 69100 of 100000: train_loss 2.8533, val loss 2.7966\n",
      "step 69200 of 100000: train_loss 2.8720, val loss 2.8286\n",
      "step 69300 of 100000: train_loss 2.8648, val loss 2.8272\n",
      "step 69400 of 100000: train_loss 2.8496, val loss 2.8176\n",
      "step 69500 of 100000: train_loss 2.8438, val loss 2.8027\n",
      "step 69600 of 100000: train_loss 2.9294, val loss 2.8306\n",
      "step 69700 of 100000: train_loss 2.8626, val loss 2.7989\n",
      "step 69800 of 100000: train_loss 2.8680, val loss 2.8676\n",
      "step 69900 of 100000: train_loss 2.8563, val loss 2.8228\n",
      "step 70000 of 100000: train_loss 2.8826, val loss 2.8056\n",
      "step 70100 of 100000: train_loss 2.8894, val loss 2.8471\n",
      "step 70200 of 100000: train_loss 2.8622, val loss 2.8364\n",
      "step 70300 of 100000: train_loss 2.8536, val loss 2.8089\n",
      "step 70400 of 100000: train_loss 2.8297, val loss 2.8107\n",
      "step 70500 of 100000: train_loss 2.8492, val loss 2.8222\n",
      "step 70600 of 100000: train_loss 2.8569, val loss 2.8229\n",
      "step 70700 of 100000: train_loss 2.8872, val loss 2.7888\n",
      "step 70800 of 100000: train_loss 2.8644, val loss 2.8344\n",
      "step 70900 of 100000: train_loss 2.8212, val loss 2.8516\n",
      "step 71000 of 100000: train_loss 2.8115, val loss 2.8187\n",
      "step 71100 of 100000: train_loss 2.8545, val loss 2.7986\n",
      "step 71200 of 100000: train_loss 2.8339, val loss 2.8325\n",
      "step 71300 of 100000: train_loss 2.8398, val loss 2.8068\n",
      "step 71400 of 100000: train_loss 2.8305, val loss 2.8232\n",
      "step 71500 of 100000: train_loss 2.8784, val loss 2.8350\n",
      "step 71600 of 100000: train_loss 2.8854, val loss 2.8733\n",
      "step 71700 of 100000: train_loss 2.8351, val loss 2.8426\n",
      "step 71800 of 100000: train_loss 2.8746, val loss 2.7690\n",
      "step 71900 of 100000: train_loss 2.8579, val loss 2.8260\n",
      "step 72000 of 100000: train_loss 2.8597, val loss 2.8053\n",
      "step 72100 of 100000: train_loss 2.8177, val loss 2.8225\n",
      "step 72200 of 100000: train_loss 2.8737, val loss 2.8102\n",
      "step 72300 of 100000: train_loss 2.8429, val loss 2.7889\n",
      "step 72400 of 100000: train_loss 2.8427, val loss 2.8099\n",
      "step 72500 of 100000: train_loss 2.8342, val loss 2.8355\n",
      "step 72600 of 100000: train_loss 2.8393, val loss 2.8193\n",
      "step 72700 of 100000: train_loss 2.8153, val loss 2.8099\n",
      "step 72800 of 100000: train_loss 2.8571, val loss 2.8305\n",
      "step 72900 of 100000: train_loss 2.8649, val loss 2.8295\n",
      "step 73000 of 100000: train_loss 2.8548, val loss 2.8327\n",
      "step 73100 of 100000: train_loss 2.8943, val loss 2.8202\n",
      "step 73200 of 100000: train_loss 2.9018, val loss 2.8059\n",
      "step 73300 of 100000: train_loss 2.8260, val loss 2.7802\n",
      "step 73400 of 100000: train_loss 2.7803, val loss 2.8165\n",
      "step 73500 of 100000: train_loss 2.8954, val loss 2.8371\n",
      "step 73600 of 100000: train_loss 2.9001, val loss 2.8291\n",
      "step 73700 of 100000: train_loss 2.8643, val loss 2.7865\n",
      "step 73800 of 100000: train_loss 2.8990, val loss 2.7682\n",
      "step 73900 of 100000: train_loss 2.8686, val loss 2.7800\n",
      "step 74000 of 100000: train_loss 2.8775, val loss 2.8362\n",
      "step 74100 of 100000: train_loss 2.8747, val loss 2.8221\n",
      "step 74200 of 100000: train_loss 2.8124, val loss 2.8162\n",
      "step 74300 of 100000: train_loss 2.8622, val loss 2.8216\n",
      "step 74400 of 100000: train_loss 2.8803, val loss 2.8069\n",
      "step 74500 of 100000: train_loss 2.8985, val loss 2.8043\n",
      "step 74600 of 100000: train_loss 2.8630, val loss 2.8054\n",
      "step 74700 of 100000: train_loss 2.8553, val loss 2.7958\n",
      "step 74800 of 100000: train_loss 2.8784, val loss 2.8275\n",
      "step 74900 of 100000: train_loss 2.8411, val loss 2.8056\n",
      "step 75000 of 100000: train_loss 2.8428, val loss 2.8571\n",
      "step 75100 of 100000: train_loss 2.8308, val loss 2.8241\n",
      "step 75200 of 100000: train_loss 2.8721, val loss 2.8158\n",
      "step 75300 of 100000: train_loss 2.8337, val loss 2.8235\n",
      "step 75400 of 100000: train_loss 2.8626, val loss 2.8106\n",
      "step 75500 of 100000: train_loss 2.8552, val loss 2.8088\n",
      "step 75600 of 100000: train_loss 2.8430, val loss 2.8046\n",
      "step 75700 of 100000: train_loss 2.8736, val loss 2.8266\n",
      "step 75800 of 100000: train_loss 2.8193, val loss 2.8133\n",
      "step 75900 of 100000: train_loss 2.8411, val loss 2.8540\n",
      "step 76000 of 100000: train_loss 2.8731, val loss 2.7921\n",
      "step 76100 of 100000: train_loss 2.8615, val loss 2.8764\n",
      "step 76200 of 100000: train_loss 2.8398, val loss 2.8190\n",
      "step 76300 of 100000: train_loss 2.8041, val loss 2.8552\n",
      "step 76400 of 100000: train_loss 2.8695, val loss 2.8471\n",
      "step 76500 of 100000: train_loss 2.8688, val loss 2.8050\n",
      "step 76600 of 100000: train_loss 2.7834, val loss 2.8326\n",
      "step 76700 of 100000: train_loss 2.8888, val loss 2.7828\n",
      "step 76800 of 100000: train_loss 2.7933, val loss 2.7907\n",
      "step 76900 of 100000: train_loss 2.9079, val loss 2.8402\n",
      "step 77000 of 100000: train_loss 2.8694, val loss 2.8506\n",
      "step 77100 of 100000: train_loss 2.8627, val loss 2.8085\n",
      "step 77200 of 100000: train_loss 2.8658, val loss 2.8249\n",
      "step 77300 of 100000: train_loss 2.9063, val loss 2.8445\n",
      "step 77400 of 100000: train_loss 2.7865, val loss 2.7905\n",
      "step 77500 of 100000: train_loss 2.8319, val loss 2.8036\n",
      "step 77600 of 100000: train_loss 2.9287, val loss 2.8390\n",
      "step 77700 of 100000: train_loss 2.8495, val loss 2.8222\n",
      "step 77800 of 100000: train_loss 2.8757, val loss 2.7902\n",
      "step 77900 of 100000: train_loss 2.8839, val loss 2.8409\n",
      "step 78000 of 100000: train_loss 2.8640, val loss 2.7925\n",
      "step 78100 of 100000: train_loss 2.8334, val loss 2.8400\n",
      "step 78200 of 100000: train_loss 2.8491, val loss 2.8134\n",
      "step 78300 of 100000: train_loss 2.8509, val loss 2.7992\n",
      "step 78400 of 100000: train_loss 2.8739, val loss 2.8200\n",
      "step 78500 of 100000: train_loss 2.8744, val loss 2.7970\n",
      "step 78600 of 100000: train_loss 2.8566, val loss 2.7929\n",
      "step 78700 of 100000: train_loss 2.8698, val loss 2.8329\n",
      "step 78800 of 100000: train_loss 2.8455, val loss 2.8178\n",
      "step 78900 of 100000: train_loss 2.8838, val loss 2.8281\n",
      "step 79000 of 100000: train_loss 2.8390, val loss 2.8478\n",
      "step 79100 of 100000: train_loss 2.8197, val loss 2.7773\n",
      "step 79200 of 100000: train_loss 2.8419, val loss 2.7849\n",
      "step 79300 of 100000: train_loss 2.8430, val loss 2.8508\n",
      "step 79400 of 100000: train_loss 2.8652, val loss 2.8082\n",
      "step 79500 of 100000: train_loss 2.8214, val loss 2.8511\n",
      "step 79600 of 100000: train_loss 2.8466, val loss 2.8185\n",
      "step 79700 of 100000: train_loss 2.8436, val loss 2.8053\n",
      "step 79800 of 100000: train_loss 2.8686, val loss 2.8494\n",
      "step 79900 of 100000: train_loss 2.8644, val loss 2.8202\n",
      "step 80000 of 100000: train_loss 2.8230, val loss 2.8359\n",
      "step 80100 of 100000: train_loss 2.8447, val loss 2.8267\n",
      "step 80200 of 100000: train_loss 2.8606, val loss 2.8025\n",
      "step 80300 of 100000: train_loss 2.8839, val loss 2.8186\n",
      "step 80400 of 100000: train_loss 2.8879, val loss 2.8578\n",
      "step 80500 of 100000: train_loss 2.8215, val loss 2.7962\n",
      "step 80600 of 100000: train_loss 2.8880, val loss 2.8107\n",
      "step 80700 of 100000: train_loss 2.8149, val loss 2.7885\n",
      "step 80800 of 100000: train_loss 2.8637, val loss 2.8382\n",
      "step 80900 of 100000: train_loss 2.8554, val loss 2.8219\n",
      "step 81000 of 100000: train_loss 2.8974, val loss 2.8374\n",
      "step 81100 of 100000: train_loss 2.8477, val loss 2.7835\n",
      "step 81200 of 100000: train_loss 2.8475, val loss 2.8272\n",
      "step 81300 of 100000: train_loss 2.8080, val loss 2.8172\n",
      "step 81400 of 100000: train_loss 2.8034, val loss 2.7857\n",
      "step 81500 of 100000: train_loss 2.8431, val loss 2.7962\n",
      "step 81600 of 100000: train_loss 2.8513, val loss 2.7673\n",
      "step 81700 of 100000: train_loss 2.8247, val loss 2.8494\n",
      "step 81800 of 100000: train_loss 2.8430, val loss 2.8368\n",
      "step 81900 of 100000: train_loss 2.8587, val loss 2.8258\n",
      "step 82000 of 100000: train_loss 2.8469, val loss 2.7947\n",
      "step 82100 of 100000: train_loss 2.8163, val loss 2.7849\n",
      "step 82200 of 100000: train_loss 2.8649, val loss 2.8208\n",
      "step 82300 of 100000: train_loss 2.8291, val loss 2.8027\n",
      "step 82400 of 100000: train_loss 2.8792, val loss 2.7998\n",
      "step 82500 of 100000: train_loss 2.8642, val loss 2.8178\n",
      "step 82600 of 100000: train_loss 2.8424, val loss 2.8411\n",
      "step 82700 of 100000: train_loss 2.8602, val loss 2.7852\n",
      "step 82800 of 100000: train_loss 2.8495, val loss 2.8755\n",
      "step 82900 of 100000: train_loss 2.8555, val loss 2.8186\n",
      "step 83000 of 100000: train_loss 2.8747, val loss 2.7988\n",
      "step 83100 of 100000: train_loss 2.8531, val loss 2.8611\n",
      "step 83200 of 100000: train_loss 2.8049, val loss 2.7789\n",
      "step 83300 of 100000: train_loss 2.8527, val loss 2.7840\n",
      "step 83400 of 100000: train_loss 2.8332, val loss 2.7597\n",
      "step 83500 of 100000: train_loss 2.8703, val loss 2.8274\n",
      "step 83600 of 100000: train_loss 2.8598, val loss 2.8169\n",
      "step 83700 of 100000: train_loss 2.8906, val loss 2.7770\n",
      "step 83800 of 100000: train_loss 2.8848, val loss 2.8562\n",
      "step 83900 of 100000: train_loss 2.8402, val loss 2.8509\n",
      "step 84000 of 100000: train_loss 2.8682, val loss 2.7722\n",
      "step 84100 of 100000: train_loss 2.8928, val loss 2.8263\n",
      "step 84200 of 100000: train_loss 2.8513, val loss 2.8093\n",
      "step 84300 of 100000: train_loss 2.8669, val loss 2.8172\n",
      "step 84400 of 100000: train_loss 2.8593, val loss 2.8202\n",
      "step 84500 of 100000: train_loss 2.8433, val loss 2.8662\n",
      "step 84600 of 100000: train_loss 2.8715, val loss 2.8368\n",
      "step 84700 of 100000: train_loss 2.8534, val loss 2.7993\n",
      "step 84800 of 100000: train_loss 2.8460, val loss 2.7983\n",
      "step 84900 of 100000: train_loss 2.8599, val loss 2.8101\n",
      "step 85000 of 100000: train_loss 2.8778, val loss 2.8184\n",
      "step 85100 of 100000: train_loss 2.8149, val loss 2.8560\n",
      "step 85200 of 100000: train_loss 2.8751, val loss 2.7829\n",
      "step 85300 of 100000: train_loss 2.8565, val loss 2.7986\n",
      "step 85400 of 100000: train_loss 2.8313, val loss 2.8356\n",
      "step 85500 of 100000: train_loss 2.8638, val loss 2.8433\n",
      "step 85600 of 100000: train_loss 2.8436, val loss 2.8323\n",
      "step 85700 of 100000: train_loss 2.8384, val loss 2.7690\n",
      "step 85800 of 100000: train_loss 2.8721, val loss 2.8267\n",
      "step 85900 of 100000: train_loss 2.8445, val loss 2.8449\n",
      "step 86000 of 100000: train_loss 2.9017, val loss 2.8665\n",
      "step 86100 of 100000: train_loss 2.8915, val loss 2.7958\n",
      "step 86200 of 100000: train_loss 2.8013, val loss 2.7721\n",
      "step 86300 of 100000: train_loss 2.8357, val loss 2.7905\n",
      "step 86400 of 100000: train_loss 2.8907, val loss 2.8293\n",
      "step 86500 of 100000: train_loss 2.8556, val loss 2.7957\n",
      "step 86600 of 100000: train_loss 2.8398, val loss 2.7800\n",
      "step 86700 of 100000: train_loss 2.8507, val loss 2.8072\n",
      "step 86800 of 100000: train_loss 2.8347, val loss 2.8063\n",
      "step 86900 of 100000: train_loss 2.8633, val loss 2.7947\n",
      "step 87000 of 100000: train_loss 2.8503, val loss 2.8537\n",
      "step 87100 of 100000: train_loss 2.8559, val loss 2.7808\n",
      "step 87200 of 100000: train_loss 2.8599, val loss 2.8221\n",
      "step 87300 of 100000: train_loss 2.8702, val loss 2.8407\n",
      "step 87400 of 100000: train_loss 2.8905, val loss 2.8225\n",
      "step 87500 of 100000: train_loss 2.8464, val loss 2.8867\n",
      "step 87600 of 100000: train_loss 2.9070, val loss 2.7933\n",
      "step 87700 of 100000: train_loss 2.8674, val loss 2.8030\n",
      "step 87800 of 100000: train_loss 2.8415, val loss 2.7926\n",
      "step 87900 of 100000: train_loss 2.9074, val loss 2.7793\n",
      "step 88000 of 100000: train_loss 2.8410, val loss 2.8528\n",
      "step 88100 of 100000: train_loss 2.8399, val loss 2.8372\n",
      "step 88200 of 100000: train_loss 2.8902, val loss 2.7880\n",
      "step 88300 of 100000: train_loss 2.8845, val loss 2.7711\n",
      "step 88400 of 100000: train_loss 2.8576, val loss 2.8277\n",
      "step 88500 of 100000: train_loss 2.8661, val loss 2.8179\n",
      "step 88600 of 100000: train_loss 2.8354, val loss 2.7994\n",
      "step 88700 of 100000: train_loss 2.8438, val loss 2.8376\n",
      "step 88800 of 100000: train_loss 2.8625, val loss 2.8398\n",
      "step 88900 of 100000: train_loss 2.8661, val loss 2.8804\n",
      "step 89000 of 100000: train_loss 2.8483, val loss 2.8224\n",
      "step 89100 of 100000: train_loss 2.8685, val loss 2.7726\n",
      "step 89200 of 100000: train_loss 2.8669, val loss 2.7937\n",
      "step 89300 of 100000: train_loss 2.8870, val loss 2.8300\n",
      "step 89400 of 100000: train_loss 2.8526, val loss 2.7769\n",
      "step 89500 of 100000: train_loss 2.8953, val loss 2.8490\n",
      "step 89600 of 100000: train_loss 2.8942, val loss 2.8337\n",
      "step 89700 of 100000: train_loss 2.8195, val loss 2.7710\n",
      "step 89800 of 100000: train_loss 2.8380, val loss 2.8048\n",
      "step 89900 of 100000: train_loss 2.8660, val loss 2.8045\n",
      "step 90000 of 100000: train_loss 2.8559, val loss 2.7952\n",
      "step 90100 of 100000: train_loss 2.8390, val loss 2.8215\n",
      "step 90200 of 100000: train_loss 2.8745, val loss 2.8205\n",
      "step 90300 of 100000: train_loss 2.8884, val loss 2.7850\n",
      "step 90400 of 100000: train_loss 2.8432, val loss 2.8264\n",
      "step 90500 of 100000: train_loss 2.8699, val loss 2.8366\n",
      "step 90600 of 100000: train_loss 2.8390, val loss 2.8120\n",
      "step 90700 of 100000: train_loss 2.8390, val loss 2.8378\n",
      "step 90800 of 100000: train_loss 2.8533, val loss 2.7691\n",
      "step 90900 of 100000: train_loss 2.8508, val loss 2.8293\n",
      "step 91000 of 100000: train_loss 2.8844, val loss 2.7709\n",
      "step 91100 of 100000: train_loss 2.8671, val loss 2.8703\n",
      "step 91200 of 100000: train_loss 2.8346, val loss 2.8411\n",
      "step 91300 of 100000: train_loss 2.8650, val loss 2.8174\n",
      "step 91400 of 100000: train_loss 2.8216, val loss 2.8341\n",
      "step 91500 of 100000: train_loss 2.8687, val loss 2.8608\n",
      "step 91600 of 100000: train_loss 2.8093, val loss 2.7860\n",
      "step 91700 of 100000: train_loss 2.8386, val loss 2.8300\n",
      "step 91800 of 100000: train_loss 2.8296, val loss 2.8037\n",
      "step 91900 of 100000: train_loss 2.8530, val loss 2.8254\n",
      "step 92000 of 100000: train_loss 2.8986, val loss 2.8328\n",
      "step 92100 of 100000: train_loss 2.8300, val loss 2.7931\n",
      "step 92200 of 100000: train_loss 2.8949, val loss 2.8170\n",
      "step 92300 of 100000: train_loss 2.8197, val loss 2.8090\n",
      "step 92400 of 100000: train_loss 2.8490, val loss 2.7909\n",
      "step 92500 of 100000: train_loss 2.8803, val loss 2.8597\n",
      "step 92600 of 100000: train_loss 2.8520, val loss 2.7849\n",
      "step 92700 of 100000: train_loss 2.8946, val loss 2.8101\n",
      "step 92800 of 100000: train_loss 2.8914, val loss 2.8152\n",
      "step 92900 of 100000: train_loss 2.8314, val loss 2.7761\n",
      "step 93000 of 100000: train_loss 2.8644, val loss 2.8374\n",
      "step 93100 of 100000: train_loss 2.8507, val loss 2.8250\n",
      "step 93200 of 100000: train_loss 2.8449, val loss 2.8069\n",
      "step 93300 of 100000: train_loss 2.8494, val loss 2.8143\n",
      "step 93400 of 100000: train_loss 2.8012, val loss 2.8576\n",
      "step 93500 of 100000: train_loss 2.8644, val loss 2.8083\n",
      "step 93600 of 100000: train_loss 2.8270, val loss 2.7893\n",
      "step 93700 of 100000: train_loss 2.7846, val loss 2.8333\n",
      "step 93800 of 100000: train_loss 2.8772, val loss 2.7795\n",
      "step 93900 of 100000: train_loss 2.8597, val loss 2.8349\n",
      "step 94000 of 100000: train_loss 2.8212, val loss 2.8149\n",
      "step 94100 of 100000: train_loss 2.8461, val loss 2.7968\n",
      "step 94200 of 100000: train_loss 2.8095, val loss 2.8398\n",
      "step 94300 of 100000: train_loss 2.8338, val loss 2.8035\n",
      "step 94400 of 100000: train_loss 2.8723, val loss 2.8239\n",
      "step 94500 of 100000: train_loss 2.8577, val loss 2.8258\n",
      "step 94600 of 100000: train_loss 2.8118, val loss 2.8072\n",
      "step 94700 of 100000: train_loss 2.8273, val loss 2.8277\n",
      "step 94800 of 100000: train_loss 2.8514, val loss 2.8251\n",
      "step 94900 of 100000: train_loss 2.8685, val loss 2.7785\n",
      "step 95000 of 100000: train_loss 2.8504, val loss 2.8204\n",
      "step 95100 of 100000: train_loss 2.8213, val loss 2.7729\n",
      "step 95200 of 100000: train_loss 2.9034, val loss 2.7801\n",
      "step 95300 of 100000: train_loss 2.8279, val loss 2.8234\n",
      "step 95400 of 100000: train_loss 2.8575, val loss 2.8367\n",
      "step 95500 of 100000: train_loss 2.9054, val loss 2.8338\n",
      "step 95600 of 100000: train_loss 2.8071, val loss 2.7748\n",
      "step 95700 of 100000: train_loss 2.8480, val loss 2.8194\n",
      "step 95800 of 100000: train_loss 2.8765, val loss 2.8320\n",
      "step 95900 of 100000: train_loss 2.8473, val loss 2.7891\n",
      "step 96000 of 100000: train_loss 2.8823, val loss 2.8065\n",
      "step 96100 of 100000: train_loss 2.8309, val loss 2.8115\n",
      "step 96200 of 100000: train_loss 2.8766, val loss 2.7638\n",
      "step 96300 of 100000: train_loss 2.8700, val loss 2.8294\n",
      "step 96400 of 100000: train_loss 2.8389, val loss 2.8679\n",
      "step 96500 of 100000: train_loss 2.8192, val loss 2.7950\n",
      "step 96600 of 100000: train_loss 2.8086, val loss 2.8206\n",
      "step 96700 of 100000: train_loss 2.8729, val loss 2.8402\n",
      "step 96800 of 100000: train_loss 2.8211, val loss 2.8137\n",
      "step 96900 of 100000: train_loss 2.8447, val loss 2.7932\n",
      "step 97000 of 100000: train_loss 2.8394, val loss 2.8465\n",
      "step 97100 of 100000: train_loss 2.8513, val loss 2.7864\n",
      "step 97200 of 100000: train_loss 2.8482, val loss 2.7757\n",
      "step 97300 of 100000: train_loss 2.8697, val loss 2.7992\n",
      "step 97400 of 100000: train_loss 2.8468, val loss 2.8147\n",
      "step 97500 of 100000: train_loss 2.7892, val loss 2.8106\n",
      "step 97600 of 100000: train_loss 2.8373, val loss 2.8424\n",
      "step 97700 of 100000: train_loss 2.8933, val loss 2.8062\n",
      "step 97800 of 100000: train_loss 2.8838, val loss 2.8301\n",
      "step 97900 of 100000: train_loss 2.7944, val loss 2.8088\n",
      "step 98000 of 100000: train_loss 2.8457, val loss 2.8342\n",
      "step 98100 of 100000: train_loss 2.8878, val loss 2.8390\n",
      "step 98200 of 100000: train_loss 2.8262, val loss 2.7794\n",
      "step 98300 of 100000: train_loss 2.8368, val loss 2.7809\n",
      "step 98400 of 100000: train_loss 2.8493, val loss 2.8180\n",
      "step 98500 of 100000: train_loss 2.8350, val loss 2.7822\n",
      "step 98600 of 100000: train_loss 2.8645, val loss 2.8064\n",
      "step 98700 of 100000: train_loss 2.8724, val loss 2.8483\n",
      "step 98800 of 100000: train_loss 2.8190, val loss 2.7767\n",
      "step 98900 of 100000: train_loss 2.8748, val loss 2.7929\n",
      "step 99000 of 100000: train_loss 2.8400, val loss 2.8036\n",
      "step 99100 of 100000: train_loss 2.8752, val loss 2.8398\n",
      "step 99200 of 100000: train_loss 2.8502, val loss 2.8067\n",
      "step 99300 of 100000: train_loss 2.8205, val loss 2.8223\n",
      "step 99400 of 100000: train_loss 2.8723, val loss 2.7782\n",
      "step 99500 of 100000: train_loss 2.8521, val loss 2.8154\n",
      "step 99600 of 100000: train_loss 2.8344, val loss 2.7802\n",
      "step 99700 of 100000: train_loss 2.8464, val loss 2.8353\n",
      "step 99800 of 100000: train_loss 2.8521, val loss 2.7818\n",
      "step 99900 of 100000: train_loss 2.8459, val loss 2.7995\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(0, len(all_data3))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data3[train_inds]\n",
    "val_data = all_data3[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "\n",
    "input_dim = all_data3.shape[2]\n",
    "\n",
    "model3 = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx3,\n",
    "                causal_ordering=causal_ordering3,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types=var_types3,\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model3.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model3.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model3(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model3.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model3(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model3.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6057349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [1.36] est ATE: [1.14525673] error: [0.21474327]\n",
      "Mean Squared Error Across All Vars: tensor(0.9648)\n",
      "Mean Squared Error Across Outcome: tensor(0.9709)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABLaUlEQVR4nO3deXxU9bk/8M+ZkEwWkiEBwgSNEBaraZBVFoNWMGgqpaK9eqtgldJYaGhx6S2g9QI/tEhd8Ba8iLZF24jYRUUU08viSkGoMUiI7AnSkGFLmIkJmYSZ8/sjnnEms50zc86cWT7v1yuvF5mcOec7JJnz5Pt9vs8jiKIogoiIiEgHBr0HQERERImLgQgRERHphoEIERER6YaBCBEREemGgQgRERHphoEIERER6YaBCBEREemGgQgRERHppofeAwjE6XTi5MmTyMzMhCAIeg+HiIiIZBBFES0tLejfvz8MhsBzHlEdiJw8eRL5+fl6D4OIiIhCcOLECVx66aUBj4nqQCQzMxNA1wvJysrSeTREREQkh81mQ35+vus+HkhUByLSckxWVhYDESIiohgjJ62CyapERESkGwYiREREpBsGIkRERKQbBiJERESkGwYiREREpBsGIkRERKQbBiJERESkGwYiREREpJuoLmhGRERE2nA4Reyua8LplnbkZqZibEEOkgyR7+vGQISIiCjBVNY0YummWjRa212P5ZlSsXhaIUqL8iI6Fi7NEBERJZDKmkbMrajyCEIAwGJtx9yKKlTWNEZ0PAxEiIiIEoTDKWLpplqIPr4mPbZ0Uy0cTl9HaIOBCBERUYLYXdfkNRPiTgTQaG3H7rqmiI2JgQgREVGCON3iPwgJ5Tg1MBAhIiJKELmZqaoepwYGIkRERAlibEEO8kyp8LdJV0DX7pmxBTkRGxMDESIiogSRZBCweFohAHgFI9Lni6cVRrSeCAMRIiKiBFJalIc1M0fBbPJcfjGbUrFm5qiI1xFhQTMiIqIEU1qUhymFZlZWJSIiIn0kGQRMGNxb72FwaYaIiIj0w0CEiIiIdMNAhIiIiHTDQISIiIh0w0CEiIiIdMNAhIiIiHTDQISIiIh0w0CEiIiIdKN5INLQ0ICZM2eid+/eSEtLw7Bhw/Cvf/1L68sSERFRDNC0smpzczOKi4sxadIkvPvuu+jbty8OHz6M7OxsLS9LRETk4nCKUVHKnHzTNBBZsWIF8vPzsW7dOtdjBQUFWl6SiIjIpbKmEUs31aLR2u56LM+UisXTCiPe3I1803Rp5q233sKYMWNw++23Izc3FyNHjsSLL77o93i73Q6bzebxQUREFIrKmkbMrajyCEIAwGJtx9yKKlTWNOo0MnKnaSBy7NgxrFmzBkOHDsU//vEPzJ07F7/4xS/w8ssv+zx++fLlMJlMro/8/Hwth0dERHHK4RSxdFMtRB9fkx5buqkWDqevIyiSBFEUNfsupKSkYMyYMfjnP//peuwXv/gF9uzZg507d3odb7fbYbfbXZ/bbDbk5+fDarUiKytLq2ESEVGc2Xn0HO58cVfQ414tGx8VHWjjjc1mg8lkknX/1nRGJC8vD4WFhR6PXXnllfjyyy99Hm80GpGVleXxQUREpNTplvbgByk4jrSjaSBSXFyMgwcPejx26NAhDBgwQMvLEhFRgsvNTFX1ONKOpoHIAw88gF27duE3v/kNjhw5gvXr1+OFF15AeXm5lpclIqIEN7YgB3mmVPjbpCuga/fM2IKcSA6LfNA0ELn66qvxxhtv4NVXX0VRURGWLVuGZ599FjNmzNDyskRElOCSDAIWT+tKDegejEifL55WyHoiUUDTZNVwKUl2ISIi6o51RPSh5P6taUEzIiIiPZUW5WFKoZmVVaMYAxEiIoprSQaBW3SjGAMRIiIijbDPTXAMRIiIKKrF6s2c+SnyMBAhIqKoFembuVpBj9TnpvtuEKnPzZqZoxiMfI2BCBERRaVI38zVCnqC9bkR0NXnZkqhOSZmdrSmaR0RIiKiUES6aZ2anXp31zV5ncedCKDR2o7ddU2hDjeuMBAhIqKoE8mbudpBD/vcKMNAhIiIoo7Sm7nDKWLn0XPYWN2AnUfPKZopUTvoYZ8bZZgjQkREUUfJzTyc3A6HU8SOI2dlXUtucCT1ubFY233OsggAzH763MTqDqFwMBAhIiIP0XAzDHYzB7qCjebWDpSvDy2h1VcAE4jc4EjqczO3ogoC4DG2QH1uEnW7L3vNEBGRS7TcDB1OEau3H8bKrYf9HmNK6wFBEHC+rdPn16WZh48XTPZ50/e1I0fpeQJR8n/pbzzS1WJtu6+S+zcDESIiAqD/zVCaidlSa8Gb1SfR1NqhynlfLRvvUeLd4RQxccV2WTMh4b52ObNLwcYTaiCkJza9IyIiRfSufaF0mUSJ7rkdwZJT3ZnDnA2S0+dGSbJsPPbMYSBCRES63gyVLJOEIjcz1WNm4vCpFlnPmzdpMB6Y8i3NZyESfbsvAxEiIlL9Zig34TXQTEy4pCWN5tYO2Usx7oqH9PUbhKiZ0Jvo230ZiBARkao3QyVJmkqWSZSQQoLvD8/zuasm2HP9ba8F1E/oDWe7bzxgQTMiInLdDP39TS+g62Yb7Gbor1R6o7UdcyqqsGzTfo+CY1otN5hNqXjurpF4a2+j4iAE+GZ7bfdCaZs/V68UvETa7ut+fX/jiUecESEiopBrX7jruOjEw2/sC3jj/8OOevxhR71rBkHt5YaMlCS8cPcYjB/cO6TZFvfkVF8zHwYBmiT0lhblYc3MUV7XCzdZNhYwECEiiiFaFhvzdzPMzkjGY7cUBbwZVtY04uE3atDU6rumR3fSDMJzd42COcsIi80e9vgBoLXDgX8db0Lx0D6yZ1vmTRqCof16evx/+kugDVQ5Xkro3XXsHIqH9FE89tKiPEwpNOteTC7SGIgQEcWISBQbKy3Kg9MJ/HpjjauOR1NrJ5a98wUMBsHrOl2Fx45g5dZDiq4jzSAse6cWP7w6H89uO6LK+AFg3Y56zJs8VPZsS/GQPl51RsJJoC17+V/46XcGYd7koYqDCDnbfeMNc0SIiGKAmm3qg12nfH2VVzExX9eprGlE8RPbFAchEmkG4Z9Hw++g6+78hU7srmsKmvcCeOe9OJwiXtpRF1YCbVunAyu3Hsbox7ao9n2JZwxEiIiinNpt6tW4jhQYqbGksrte3UAEACy29oBJoK7jrO34beUXALoCq4krtmPZO1+oMobzbZ2qBonxioEIEVGUU7tNfbjX2XXsnGa1P9Sy7O39qKxpdOW9pKck+TxOBLD2wzqU/WmPzxmncIlQJ0iMZwxEiIiiXKQqb8p9/s6j5zSp/SFXao/gt66m1k7MqajC/9u0HxnJPdDW4Qh4/Jba05oFVo3WdqzcctBj2zJ9g4EIEVGUi1TlzfqzrTKP1PdmOuc7g2Uf+8cd9bh73W7dZ29Wv3cUd764CxNXbOdSTTcMRIiIopxaxcYCcThFrP/ky6DH5ZlSMWGQsq2po/JNyE5TZ5Nmdnoyfn7DUDw/cxRyMlJUOWckqZ1cHA8YiBARRblIVN5cvf0wTrUETzwdPSAb4wf3DrobxV3VCSuaL1wMeWzu7r1mIICubcaPTr1SlXPK9ejUK7HqzpEIp6yHmsnF8YKBCBFRDJCSLs0mz+UXsykVa2aOCquOSGVNI1ZuPSzr2Lc/b8SWWkvQ3ShaWbn1MK5+fAs2f34SZlOaqucONuN0b3EB+vQ0BixqJodaycXxggXNiIhihBaVN6Wy7Eo89Ne9+HzxTVgzcxR+9bfPYWtXZ7ZDrqbWTvxs/WeYPbEABiFwtVO5phTmYmvt6aDl7dXsjaNVn51Yw0CEiCiGqFl5U2lZdkmr3YH/2XoIX1hsEQ9C3P3h47qwzyEAuO+6Aiy6udBn5druvV7U7I2jdp+dWMVAhIgoAfnrpSLX77arV5JdT6a0Hhh5WTYAeTNOUuKwxdoe1k6ccJOL4wlzRIiIEky4vVTiyfkLF127WOQ0FJRTrVWO7w/Pi/tmdnIJoihG7c+izWaDyWSC1WpFVlaW3sMhIoop/m6sO4+ew50v7tJ7eFFDAGBKT0ZqjyRYbPIaCvpaxlF6zXCTjKOZkvs3AxEiojgUqFOv/aIT8zdU6ze4GCHNV/gLGKRAb8eRM1j93lHF5zabUvHxgslxOTOi5P7NpRkiojgTrFOv/AqqiS1YzQ8pcXhov8yQzs3S710YiBARxZFgHXRFAC/9sw7mLPkFyRKZnJof4ex+Yel3BiJERHElWAddAGhuu4iv7BchIvIFyWJVoJofwUrwy5HIpd8ZiBARxZH/2y/vRvaVvav+R1pKkpbDiRuBZj3cd9KESprBWvLWfuw4chYbqxsSZsmGdUSIiGJIoC2mmz8/iZd2Hld0vrYOhxbDjCsGAWhuDdyHp7QoD/eXXI6VWw+FfB0RgMVmx4zff+J6LNDOnXjBQISIKEYE2gkDAD9b/5leQ4trThEoX/8Z1hiEgAXPBvZJV/3a0pJNPG/1jdj23SeeeAKLFi3C/Pnz8eyzz8p6DrfvEhF18VcJVcpLMKUn43ybslLtJJ+cWiNa1WeJxa2+Ubd9d8+ePVi7di2uuuqqSFyOiCiuyNkJwyBEW9L/sXsQAngmmaqRtOrv2vHcrVfzQOSrr77CjBkz8OKLLyI7O1vryxERxZ1dR8+FXMGTtOVeawRA2EmrgcRrt17NA5Hy8nJMnToVJSUlWl+KiCiuOJwi/mfrYZT9+V96D4UCcJ+xKC3Kw3N3jUKwFZRQVljqz7aFNL5op2my6oYNG1BVVYU9e/bIOt5ut8Nu/yYz2WazaTU0IqKoVlnTiIWv7+OSSwx59+saIKb0ZATbdesUgUenXok+mUb0yTDiwb9U41RL4J05G/Z8iXmTh8RMnohcmgUiJ06cwPz587FlyxakpsqrOrd8+XIsXbpUqyEREcWEyppGzKmo0nsYpNCfdh7Hn3YeR6+0ZFnH98k04pYRlwAA7hp3GVZuPRzw+EZrO3YdOweDIATsEBxrNNs18+abb+LWW29FUtI3xXIcDgcEQYDBYIDdbvf4GuB7RiQ/P5+7ZogoYXRcdGL88q1oauVMSLx7tWw8JgzuDQDYWN0gqxFhr7RknL/wzc9GtNYZUbJrRrMZkRtuuAH79u3zeGzWrFm44oorsGDBAq8gBACMRiOMRqNWQyIiimqVNY14+I0aBiFxTtqOO7Ygx/WY3H417kEIEB91RjQLRDIzM1FUVOTxWEZGBnr37u31OBFRovNXJ4Tii7SIsnhaoceSirT112JtV/QzIPULWrqpFlMKzTG5TMNeM0REEeRwith59JxHL5FAdUIovphNqT5nL9z71SgNJWK9zkhES7y///77kbwcEVFU8Vei/YdX57NOSJzrlZ6M5+4chfGDe/udtSgtysOamaO8fkZ6yayaG6t1RthrhogoAvwtvVis7UF3S1B0EgRA7naP822dMBiEoEsnpUV5Xv1snE4RM/7wScDnAfLzTKINAxEiIo0FK9FOsSk9OQmtCroXy52xSDIIrt00QNfPT6D8EV/Jr7GEOSJERBrbXdfEpZc41NrhwAMlQ5GTIa9uSKgzFoHyR/wlv8YSBiJERBpwT0rdceSs3sMhjQzsk4Fdi0qQk5Hi9xgBXblA4cxYSPkjZpNnMOMv+TWWcGmGiEhlvpJSKT7Vn21FSg8DfnNrEeZ+XQ3XfflEzRkLX/kj8VBZlYEIEZGKWA8ksazcehjfMmf63fFiVrnyaff8kXjAQISIKAiHU5T1V6jDKWLJW/sZhCQQ92Ji8TpjoTUGIkQUs+QGCOHwV/vD/a/cjotO/HlnPTZWN8BiC9xBleKLezGxCV/XCIm3GQutMRAhopgkJ0BQ4xq+uuBarO2YU1GFHxcPxJmWdrz9uYWzIAkuVouJRQPumiGimCPlYXRPBpUagFXWNIZ9DYdTxMLX9/n8mhR0/HFHPTYxCCHEbjGxaMBAhIhiipziYEs31cLhDC88WL39iKyy2pTY1Niam+gYiBBRTAlWHCyUBmDdG9F1XHTihQ+PqjBaimfxUEwsGjBHhIhiity1eLnH+co1yTAqK91NiSEnIxlNrd/Mkqm9NTdRMRAhopgidy0+0HHSbpsttRb8cUe919db7QxCyFN2ejJ2LSrBp8ebuTVXZQxEiCimjC3ICasBGKueUiia2zqx/cApzn5ogDkiRBRTwmkA5m+3DVEwUuGycJOgyRsDESKKSt0TSN1vAKE0AAu024YomFCSoEkeLs0QUdSRU6xMaTntYLttiORg4TL1MRAhoqjir2mcVKzMfcZDSTlt3kBIDSxcpj4uzRBR1FCjWJm/JZ36s23qD5gSBguXaYczIkQUNZQUK/M1E7L585P49cYaj1oPeaZUPDq1EK/u/lKLIVMCYOEybTEQIaKoEU6xsuWba7H2wzqvxxut7fjZeu/GdUT+9EpP9ijvz8Jl2mIgQkQRJxUU655kGmqxss2fN/oMQoiU6GlMwlO3D1eUBE3hYyBCRKrxF2C4C7QjZkqhGXmm1KC7Wz45ds51bodTxK831mjyeiixLJs+zDXrITcJmsLHQISIVCFny62cHTGPTr0SP1v/WcBrPbvtMDbsOYEl3y9EZmoymlo71H45lIDMWdwRowfumiGisPmrWCoFGJU1jbJ3xJjSU2Rd02Jrx5yKKsxatzu8wRMBMAjA6AHZAY8JVGSPQscZESIKS7AAQyqNfaLpgqwdMf88elbR9TscvBlQ+Jwi8OnxZr9LMnJm/Cg0nBEhorDI3XL7+OYvZJ2vofmCSiMjUsbfri05M34UOgYiRBQWtSuWiiJnOEgfvnZtqVFkjwJjIEJEYVG75PXGvfzrkiLPX9VUJUX2KDQMRIgoLGMLcpBnSgWrLFCsEuBdNVVKTH1X5rILexmFjsmqRBSWJIOAxdMKMbeiCgLgcwqbKFr5SjitrGnEkrdqYbHJDy7YDC90DESIKGylRXlYM3OU164ComiVkZKEF340BuMH9faYCamsacScCvktAQR0lYBnM7zQMRAhShByqp6Go7Qoz6M0dp+eRjz0l2qcstk5S0JRRQDw9B3DUTykj8fjDqeIha/vU3QegM3wwsVAhCgBhFMDQUkAk2QQPOow/Pf3vs2Gc6QLaZmwV1oyzl/w7Mbs7+d+1bZDHs3ugmEzPHUwECGKc3LKqru/kboHHvVnW/Hq7i9hsdldX5cbwFTWNGLZO7VqvhQiv3IyUjxK/ZtNqfj+8DxsrD4JuJWm8bc9vLKmEc9uOyLrWjcW5mJW8SA2w1MJAxGiOCa36umUQjOSDILPmZPu/AUw7vwFP0RayDOl4oP/moRPjze7Zu6aW+0oX/+Z18/gKZvd6+dX+j2R6/J+mWyKpyIGIkRxTEkNBOuFDlnBg68Axp3DKWLJW/sZhFDELJ5WiJQeBldw4HCKmLhiu+wAPNjvSXcTBvUJfhDJxjoiRHFMbm0Di/WC35kTXwIVcVq9/YjHUg5RpEi1P1ZuOaSoCJmSGiC90pMxnrMhquKMCFEck1vboKm1I6Rtt93fwCtrGrFy6yHF5yEKx5K39sPpBJa9o2z7uPTzq6QGyBO3DWNeiMo4I0IUx4JVPRXQtb6e09MY0vnrz7a5/q106yORWiw2O3623rspXTBSACKnOrAA4H/v8p8XRaFjIEIUx6SqpwC83mTdayCYs0KrCvns1kOuzqP/s0XZ1kcivUgBuFSELNDvieS5u0bi5qsYhGhB00Bk+fLluPrqq5GZmYnc3FxMnz4dBw8e1PKSRNSNVPXUbPIMNsymVNfOgXD6xSzdVItlb+/H796Tt/WRSE/+ipD5+z3JM6Xi+ZmjcPNV/SM4ysQiiBr23C4tLcUPf/hDXH311bh48SIefvhh1NTUoLa2FhkZGUGfb7PZYDKZYLVakZWVpdUwiRJCsMJk0pZbgP1iKH75q4Mj/X5YrBfQ1NqBnJ5GmLPUr0CcKJTcvzUNRLo7c+YMcnNz8cEHH+C6664LejwDEaLIklNHhCgWzZs0GMVD+voMLMKpPEy+Kbl/R3TXjNVqBQDk5PhuDmS322G3f7Ptz2azRWRcRNT1F6EpLQW/nHI5qv99Hl+ea8Pu+iZc6HTqPTSigHoak/CV3eHza1JTugemfMvnzIbSysOkvogFIk6nE/fffz+Ki4tRVFTk85jly5dj6dKlkRoSUVxT0iOGMyEUywIFIYD/pnRKKw+TNiIWiJSXl6OmpgYff/yx32MWLVqEBx980PW5zWZDfn5+JIZHFBek4GNLrQVvVp/06L3ha6rZ4RSxevsR1v6guNS9KV334NzpFGUXPmNJd+1EJBCZN28e3n77bXz44Ye49NJL/R5nNBphNIZWz4Ao0QWb1eg+1VxZ04glb+1nFVSKSzkZyfjgvyYhpUfX5lBfvx+90pJlnUtJ5VVSTtNARBRF/PznP8cbb7yB999/HwUFBVpejiiheHbJbcOzWw8F3O3iPtXsdIo+G4IRxYum1k58erwZEwb39psHcv6CvLo3SiqvknKaBiLl5eVYv349Nm7ciMzMTFgsFgCAyWRCWlqalpcmimuh5nRIU82PvFnDIITi3umW9oB5IMFIia5S4TPShqYFzdasWQOr1Yrrr78eeXl5ro/XXntNy8sSxTXpr7twEkubWQGVEkBuZqrizrqSYImupB7Nl2aIKDS+dr0ACPmvO6JEIpVwf/vzk7KO75WW7LFU0z3RlbTD7rtEUchfgaUfXp3PLbZEMkgzGXLzO56bMQoGQZC13Z3UxUCESENKanlIAhVYWrn1sHaDJYoDvdKT8cRtw1wzGVIfJYu13edMopQHMn5QbwYeOmEgQqQRpWWjHU4Ru46dw8K/7/NbYImIfEvpYUD59UMwb/IQj4BC6qw7t6IKAjx/j5gHEh0i2mtGKfaaoVjlb1ZDeqvrXjaalU2JwvPKT8aheEgfv19nP5nIitpeM0SJIFjZaAB4+I19uNDphDkrFc2tdtb0IAqR+9JKIKVFeZhSaFa8VEraYyBCpDI52wWbWjvxwGvVAACDwGUXolAoXVpJMggs1R6FNK0jQpSIlJaDdjIKIZKle6xhNqWyO24c4IwIkcpYDppIfd+7Kg/P3DECnx5v5tJKnGEgQqSyYNsFiUiZKYW5WH3XKADg0koc4tIMkcqk7YLAN2vYRBQaAUBNgw0OrmHGLQYiRBooLcrDmpmjYDZxmYYoHFKjxt11TXoPhTTCQIRII6VFefh4wWQ8UHI5TGnJeg+HKKYpTQKn2MEcESIVuJdy79PTCIjA2VY76s+2siw7kQqYBB6/GIgQhWnz54349cYaNLV26D0UorgjFSyTuk9T/GEgQhSG5ZtrsfbDOr2HQRSX2AsmMTAQIQrR5s9PMggh0pCZvWASAgMRohA4nCJ+vbFG72EQxRUBwP0ll2Ngn3QWLEsgDESIvuaecBrsTXB3XROaWjsjPEKi+MVOuImLgQgRlLcI51ZCovCYs4x4+o4ROPuVnbMfCY6BCCW8yppGzK2o8irHbrG2Y25Flc+mWtxKSBQaKdRY8v1vo3hIH13HQtGBBc0ooTmcIpZuqvXZE0b8+mPJW/vhcIpwOEXsPHoOG6sb4HSKMGcxGCFSih1zqTvOiFBC213X5LEc44vFZsf8DZ/h0+PNHsf2Sme1VKJg7pkwAKVFeeyYS34xEKGEJjfX4+3PG70es7Z1Jav2Sk/G+TYmrhL5UlqUx465FBCXZiihhZPrIS3nCBDxi8lD1BkQURzJY0VUkoGBCCW0sQU5yAuzQ25z20Wseu+ISiMiig8CWBGV5GEgQgktySBg8bTCsM8j+sp2JUoQQrdYI48JqaQAc0Qo4ZUW5eGBkqHskksUoufuHAVTWjJ2HjsLQMCEwb0xfhDzQkgeBiJEAOZNHopXd5+AxcZCZURySUX/AOCXf9vr2lW2+r0jrJRKsnFphghdSzRLvl8IrmZTIstOT8b/3jUSD5QMDXjc7OKBeLVsPD5eMBkAMLeiymsbvFQQsLLGe8cZkTsGIkRfKy3Kw33XFeg9DCLdiKKIm4ryML/kcjw/c5RXIneeKRXPzxyFR6d927UlN1BBQOnrDieTqMg/Ls0Qfc3hFPHWXv71Ronr/IWL+OPHdcjNMiI3MxUf/NckfHq82W8xsmAFAUUAjdZ27K5rYi0R8ouBCNHX5FRZJYp3j2/+wvVvKc/jlhGX+DxWbkFANomkQLg0Q/Q1vlkSeQqW5yG3ICCbRFIgDESIvlZ/tlXvIRBFlWB5HlJBQH9J3gJYXZWCYyBCBKCyppF1RIh8cM/z6M69IGD3YET6nNVVKRgGIpQwHE4RO4+ew8bqBuw8es71F57DKWLJW7U6j44ouvlbuiwtysOamaNg7rbDxszqqiQTk1UpIVTWNGLpplqPZFQpEe+gpYWFzIiCCJTnUVqUhymFZuyua/K7w4bIHwYiFPcqaxoxt6LKq9aBxdqOORVVuoyJKFYI6JrdCJbnkWQQuEWXQsKlGYprDqcYtOASEfnGPA+KBAYiFNdYG4QodGZTKp67axRMaSleuVVEauHSDMUVh1P0WKe2WC/oPSSimNXWcREPv7kP59s6XY+xmR2pLSKByHPPPYcnn3wSFosFw4cPx6pVqzB27NhIXJoSiK+E1JyMFB1HRBTbrBcuej0mFTnjjhhSi+ZLM6+99hoefPBBLF68GFVVVRg+fDhuuukmnD59WutLUwKRElK7L8M0tXboNCKi+MRmdqQ2zQORZ555BmVlZZg1axYKCwvx/PPPIz09HX/84x+1vjTFOH91P3wd5y8hlYjUF6jIGZFSmi7NdHR04NNPP8WiRYtcjxkMBpSUlGDnzp1ex9vtdtjtdtfnNptNy+FRFAtU96P7dDATUon0wf5MpAZNZ0TOnj0Lh8OBfv36eTzer18/WCwWr+OXL18Ok8nk+sjPz9dyeBSl/C2z+GvAxTdDIuXGDsxGdnpyWOdgMztSQ1Rt3120aBGsVqvr48SJE3oPiSJMTt2P7mvTfDMkUm53fTOa2zqRk5GC9JQkRc9lMztSk6ZLM3369EFSUhJOnTrl8fipU6dgNpu9jjcajTAajVoOiaJcsGUW97VpqYqj1AHUYm1nngiRGwHBC/c1t3a4jpFzPIuckdo0nRFJSUnB6NGjsW3bNtdjTqcT27Ztw4QJE7S8NMUoucss7se5dwAlIqDs2gI876MRnS8iuoKL7PRk9Mvy/EOwV3oyenVbvmEzO1Kb5nVEHnzwQdxzzz0YM2YMxo4di2effRatra2YNWuW1pemGCR3maX7caVFebjvugKs/bBOi2ERxZTff1SH0QOy8fGCydhd14QdR85g9XtH/R4vAmhu68QrPxkHgyB4NK4DwGZ2pCnNA5H//M//xJkzZ/Df//3fsFgsGDFiBCorK70SWImA4Mss/hpwOZwi3trb6OMZRIlp6aZaTCk0Y8Lg3rJnGs9+ZcctIy7xepzN7EhLEUlWnTdvHo4fPw673Y5PPvkE48aNi8RlKQa5L7N0/5sr0No0t/ASfaN7nY9QZxqJIiGqds0QAV3LLGt8rG9La9NTCs1ehc64hZfIm/R7Ic00BlpQ4S4Y0gub3lFUKi3Kw5RCs9fa9JZaCyau2O5V6Oz20ZfqOFqi6CTNcEgzjXMrqvzujLnQ6cCWWguTUCniBFEUo3bHo81mg8lkgtVqRVZWlt7DIZ1Jhc6i9geWKEpIuVQfL5jssYxZWdOIha97dtN1fw4A7oghVSi5f3NphmIC+8kQyRMol2pKoRmpPXy/7bOZHemFgQjFBCajEskTqM7H7romWGx2H8/qwmZ2pAfmiFBMYDIqUWA3FvbDrOKCgHU+QikYSKQ1BiIUE7itkCiweyYMDFrvg9t4KRpxaYaiisMpem3NBeRtPyRKVFLTumC5HcF+j9jMjvTAGRGKGpU1jVi6qdZra+7iaYUoLcrD4mmFmFNRpeMIiaJTW4cDM/7wicfviy+BtvGymR3phTMipCtpBuT/bdqPORVVXgmpFms75lZUobKmq3x79wZcRIkkWHjQ/ffFl2AFA7l1lyKNdURIN75mQHwRAJjSk2Ft6/S7fbdXWjLOX/CujUAUD0ypPfC/M0fj6oE52FPfhPJXqvz+vPurIdKdwymymR1pRsn9m0szpAslxclEwGcBJndWBiEUx2YVF6B4SB8AgEEQAgbd7ltwAyWvJhkENrOjqMClGYo4LYqTRe20HpEKXvpnvWu5hVtwKd4wEKGIY3EyImXOX+h05X7I3Vp7tsXutfuMKBpxaYYijn+pEYVm6aZafPBfk5BnSoXF2u53JtAgAMve+cL1ebDdNER64owIRRyLJREpJ+V+fHq8GYunFQLwv4um+wRIo4zdNER6YSBCEcfiZEShO93SjimFZtxfMhSmNM/t7IE2vYhgQzuKTgxESFX+KqO6k4oqAf7/ohvaN0PDURLFrvqzbZi4YjtWbj3s2j3TKy0Z/zHqEq+ZkO7Y0I6iEXNESDW+6oL0SkvGrOKBmDd5qEeNgtKiPDx310j8emMNmlq/2YqYZ0rFo1MLseiNzyM6dqJYsXLrIa/HrBc68beqBlnPt1gvqD0korAwEKGA5BY98lcX5PyFTqzcehjr/lmPJ24bhtKiPHRcdOLh1/dh875GtHU6XMfmZKTg0amFyM5IgfXCRY1fGVH8ULLY0tTaodk4iELBQIT8Ctb7RSKnLsj5tq7thyWFudj6xWn4qufb3NqB8vVVuHvCZSq+CiJyl9PTqPcQiDwwR4R8kmY4gvV+AeTXBREBbKn1HYRIXxcB/Hnnl6EPnIgCMmdx1xpFFwYi5CXQDIf0mHv2vdp1QZjTT6SNPFPX8ipRNGEgQl6CzXC497IAWBeEKFxqbGUX0NWdWvBxPumxxdMK2diOog4DEfKitJeFVBeEiJSbXTwQZhm/P4Kff7t//sRtw7Bm5iiv85lNqVgzcxQrq1JUYrIqeZE7wyEdJ9UFmVNRpeWwiOJSSaEZD08t9Nid1tzagWXveCaKm79OFAfglURu7pZEPqXQLGu3G1E0EETRX+qg/mw2G0wmE6xWK7KysvQeTsJwOEVMXLHdby8LAV1vfB8vmOzx5lZZ04iFf98XsEU5EXXx93skCbR1Xu62eiK9KLl/c0aEvEgzHHMrqiDAM3lUeqvzt9acmmwAWC+JSJZAORtJBgETBvdW/DWiWMMckQQVrBR7aVGe7LVmh1PE/2w9hDkVVbDY7BEZP1Es8DdJkcecDSIXzogkILmFykqL8oKuNVfWNGLJW7Ww2NTdwksUD1bfORLZGUZYrBfQ1NqBnJ5GmLO4lELkjjkiCcZfKXbpLVHJX2n+zkUUb4w9BNgvyv9Jz0hJwtN3DOeMByUs5oiQT8EKlQnoysafUmgO+teanLLuRLHsP0ZdiuKhfWDOSkVNgxWPb/4i6HOK+mdhQekVuGZIH854EMnEQCQBSBn2O46clV2oLFginNyy7kSxJiNFwNN3jPSYzZDbsXb2xAJce3lfrYZGFJcYiMQ5X/kgwcgpaKZ2WXeiaHHPhEFeSypmU5qs58o9joi+wV0zccxf47pg5BQ0qzvTGuqwiKKawce7opzqwezjQhQaBiJxKpQcDgHy3kwdThF/2nU8rPERRasJg/p4PSbV1vGX9cE+LkShYyASp5TmcAQrVNb93E2tHWGMjig69UpPxng/+VFSbZ3uMyOsCUIUHuaIxCmlORw5GSlYdkuRrDdT5odQvHritmEBA3E5tXWISBkGInFKbuM6ybmvm2wZDAgajCg9N1G0M2cZseT735YViLO8OpG6uDQTp6TkOiV/p1ms7ZhbUYXKmkZZ5yaKdYIA/HnWWOxYeAOXVoh0wkAkTknJdQBkByPi1x9LN9V69J7p3pcGQMDEPaJYIYrAodMtXFoh0pFmgUh9fT1mz56NgoICpKWlYfDgwVi8eDE6OpjkGAkOpwhTWgp+XDwQ2RnJip7baG3H6u2HAXRtAZ64YjvufHEX5m+oxp0v7kLxE9tx0PIVZhUPRE5GihbDJ4qY401tAII3giQibWiWI3LgwAE4nU6sXbsWQ4YMQU1NDcrKytDa2oqnnnpKq8smPIdTxOrtR7BuRx3OX+h0PZ6TkYLpI/ojPSUJq987GvQ8K7ceRlvHRbzwYZ3XFmCLrR0rtx5yfZ5hTEKr3aHWSyCKqAE56bIbQRKR+iLa9O7JJ5/EmjVrcOzYMVnHs+mdMpU1jVj4+j6cb+v0+po08Xx/yeUeQUQgAsBeMhTXDALwu/8cgZ9vqFalESQRdVFy/45ojojVakVOjv9iWXa7HTabzeOD5KmsacSciiqfQQjwTUCxYc+XMGcZZZ2TQQjFu9kTC/D4uwf8NoIEvHOmiEhdEQtEjhw5glWrVuGnP/2p32OWL18Ok8nk+sjPz4/U8GKaVEU1GKmp3Z1jL9N+UERRzCAAP72uAJOv6Ce7ESQRaUNxILJw4UIIghDw48CBAx7PaWhoQGlpKW6//XaUlZX5PfeiRYtgtVpdHydOnFD+ihKMwynipR11iqqoDuyTgQdKLtdwVETRZdF3v4VHbr4SP5owAI9OvRIHln0Xi24ulF2cj0X8iLSjOFn1oYcewr333hvwmEGDBrn+ffLkSUyaNAnXXHMNXnjhhYDPMxqNMBrlLRtQaJ11ASAnPQWnU+xISzbgQqdTo9ERRY8fTShAWkqS1+Nyi/OxiB+RdhQHIn379kXfvn1lHdvQ0IBJkyZh9OjRWLduHQy+2lpSSKTOukpXrjOMSfjRH3cz/4MSyvpPjmP2tYO8HpeK81ms7T5/JwQAZnbVJdKUZpFBQ0MDrr/+elx22WV46qmncObMGVgsFlgsFq0umTBC6awrabU7GIRQwpFqhXQXqPCfkkaQRBQ6zeqIbNmyBUeOHMGRI0dw6aWXenwtgjuG45LSzrpAV1dRa1tn0CCEW3YpHg3ISff7NamrbvdlTjPriBBFRETriCjFOiK+baxuwPwN1bKO7ZWWjFnFA5GekoTHNx8I/gSiOGMQgAPLvouUHoEngB1OkV11iVSi5P7N7rsxSG7i3KNTr8S9xQVIMgj47401Go+KSB1qV+otu7YgaBACsKsukV6YPRqDgnXWFdBVnloKQoDAU9NE0eT5GaORkSL/rcnfpIVUK2TRzYUqjYyItMAZkRgkJdjNrajyyunwl2B394SBeHzzF2CBSIpW0g6Va4b0wX3XDcbKrYdlPW/1naOQnZGCk81tqP73eQACBvZOx90TBsqaCSEifTEQiQG+1q6VJtil9DCg7NoCrP2wLtLDJwqqewA9sE+GrOf9uHggbr5K+lnvjR+MYTVmoljDQCTKBesKOqXQLDvBbtHNhTh8+itsP3AmUsMnkqV7AC03D2pKoVnLYRFRBDAQUZmamff+ipY1Wtsxt6LK1RVUSYJd2bWDGYhQ1OiVloznZozC+EG9PX5PWGiMKHEwEFFRsNkLJYIVLRPR1RV0SqFZUaAT7A2eKBKkn9gnfjAMxUP6eH09lDwoIopNzORSiTR70b3QmOXr2YvKmkbXYw6niJ1Hz2FjdQN2Hj3ns8W4nKJloXQFda8kSaQXsynVNaPnj5QHZTZ5LtPIeS4RxQ7OiKgg0OyFiK6/4KTZiy21FlmzJhbrBVnXtlgvKF4OKi3Kw33XMXGVIqunsQfuGHMpphSaZS9ZKs2DIqLYw0BEBcFmL0R0zV6s3n4Yz2497BWwWLrlfABAU2uHrGvvOHIWv/3HQUXLQQ6niLf2Nvr8GpFWWu0XsW5HveJAgoXGiOIbAxEVnG6R1/dl3Y56WbMmSQYBOT2Nss75t6oGr8e6BzYOp4hdR8/hn8fO4mTzBThFUXGvGqJw+fo5JyJiIKICuVsNz1/o9Ps1adZkd10TJgzuDXOWvHP6O5f0hn/xohML39iHr1QsmU0Uqu4/50RETFZVgZyS673Sk2WdS5pdkc4ZKukNf96GagYhFHXkziISUfxjIKIC950o3YMR6fNZ1xTIOpc0uyKdUwhwTqJoM+XKXFnHyZ1FJKL4x0BEJcG2Gs6bPCTgrAnQVdzJKYqu7byBzvlAyVC1XwJR2G6+qr+showsREZEEkEUxaita2Wz2WAymWC1WpGVlaX3cGQJtJVWqjUCIGAxse67XnydEwAmrtjOwmQUVV4tGw/rhQ6fP+dScMIaIETxT8n9m4FIhPmqvtqd3DdsuYENUSTkmVLx8YLJSDIIqlYZJqLYw0AkyknbacvXV/ndSSP10pDe2P3x9YbfOyMF52TWISFSgwDvwFnNvktEFFuU3L+5fVcHSQYBBoOgaDuvP74qT579yo75Gz6Dj8rxRKrzN9PBQmREJAcDEZ3I3b4o5zjpDd/hFLF6+xGs3Hoo3OERyfLo1Ctxb3EBZzqIKGQMRHQid/ui3OMqaxqx5K39sNjs4QyLSBZp6ZBBCBGFi4GITqSCZf52vUhv9NIOGTm7cbgSQ5EghR2LpxUyCCGisDEQ0YlUsGxuRRUEeO96EfHNG32gHQhTCs1+O/8SdWcQoCh3qOzaArz9eaPHz56Zu1+ISEXcNaOzyppGLHx9H863eSau9kpPxhO3DQMAn7Md0t+h95dczpwQkuXRqVfi7gkD8enxZpxuaUf92Vas3HrY7/E/va4Ai24u5O4XIlKMu2ZijLXNe/eMta0TcyuqYEpPDtixd92OOq2HRzGuez6H+06Wb5kzvWbbcjKS8dgtRbj5qv4AuPuFiLTFQERHDqfod1lFeqz7TEn3YwJtAabYlZ6ShLYOdZoVui/zdedr+zdnPIgokhiI6Gh3XVPACquUeIb0zcDSW4oAEZjxh09UOeePiwcGzOfgjAcR6YlN7/xwOEXsPHoOG6sbsPPoOVcjOjXPxVbo1N2RM6345V/3wnqhM2iTRLmmFJpVOAsRkTY4I+KDmn0yfJ0rJyMFj91SxFbo5JPF2o7y9VW477oCvPBhnc9dVXKZs4wenW6ZeEpE0Ya7ZrrxV5MjlM6hwep7lFyZi+0HTrMUO3mREkwfnXollr3zRchLeM+7/byyER0RRYqS+zeXZtzISR5duqlW1jJNoHNJtn7BIIR8k3oNZWcY8fGCyXi1bDzmTRoi+/npKUleQcjciiqvgMZibcfciipU1jSqOXwiItkYiLgJljzq3ogu3HMRyXG6pd2VTDq0X09Zz7np2/2wb8lNriBEzQCbiEhtDETcqNmIjomopAb3PCK5OUX3XuPZ/0XNAJuISG0MRNyo2YhO7URUAUB2erKq5yT9ZKQYYM7yvytGQFf+hnuiqdSfSMlzAHUDbCIitTEQcRPqG72/c+VkpKgyLmk8j08fhl4MRuLCk/8xHEu+XwgAXj9v/prKSf2JlDwHUL/TMxGRmhiIuAn1jd7fuR67pUiVcZlNqVgzcxSyM1ICVlql2FB2bQFuvqo/SovysGbmKJhNngGA9P32tZMllOeoGWATEamN23d9UHOb4+Pv1OLFj+T3g5FqRjxQMhQD+2R41HrYWN2A+RuqFV2fostPigfi19O+7fFYKLU9lD5H2jUDeNYkCWVbOhFRMGx6Fya1+m9s/rwRf69qUPScQC3WOXUe22ZPHIBff+/bXo+HUmJd6XOkmZTuAXagnzciokhgIOJHuP03lm+uxdoPlXXGfaDkcsybPMRvwDO2IAe90pO5PBODyq4twCNTC3UdAxvcEVE0YiCigc2fn1QchAgANuz5EvMm+y9ataXWwiAkxmSl9sATt12Fm6+KjhkHNrgjomjDQERlDqeIX2+sUfw891oOvm4UHRedePgN5ecl/fQ0JuFfv56ClB7MCSci8ofvkCrbXdeEptbQZy181XKorGnE+OVb0dTaEc7QKMLKrh3EIISIKIiIvEva7XaMGDECgiCguro6EpeMOIdTxM6j5/BumD07uiekSrsdwgluKPKy05Mxb/JQvYdBRBT1IrI086tf/Qr9+/fH3r17I3G5iPO13Vcpqdtq95btwRrnUfQRACy/bRiTQImIZNB8RuTdd9/F//3f/+Gpp57S+lK68NfVVAl/xdLYOC/25AUoLEZERN40nRE5deoUysrK8OabbyI9PT3o8Xa7HXa73fW5zWbTcnhhC3XGIiMlCa0dDtfn/mo5sPdHbMgwJuGx6cNgzuJ2WCIipTQLRERRxL333os5c+ZgzJgxqK+vD/qc5cuXY+nSpVoNSXVKZywyjEkom1iAn00aik+PNwet5cACZrHh6duHcwaEiChEipdmFi5cCEEQAn4cOHAAq1atQktLCxYtWiT73IsWLYLVanV9nDhxQunwIkrujEV6ShIAoNXuwLPbjuA7T74H64UO3DLiEkwY3NvvX9DN3CUTVUxpnnG7OcuI57kMQ0QUFsW9Zs6cOYNz584FPGbQoEG44447sGnTJgjCNzdZh8OBpKQkzJgxAy+//HLQa+nVa0aunUfP4c4Xd4X8/P+9a5TfQlcOp4iJK7YzRySKvPKTcTAIAquSEhEFoWmvmb59+6Jv375Bj/vd736Hxx57zPX5yZMncdNNN+G1117DuHHjlF42KjW3dsAgAM4Qt7XMe7UKqzESN1/V3+trTFSNHtKOpvGD/M9eERFRaDTLEbnssss8Pu/ZsycAYPDgwbj00ku1umzEVNY0onx9VVhba50i8LP1n+F5g8BE1Sjlb0cTERGpg2UfQ6B2fY+lm2rh6DatwkTV6GDmdlwiIk1FrNfMwIEDoTAdJWqpvWziq8fM2IIc5JlSYbG2s6CZDuZNGoziIX2ZB0JEpDHOiIRAi2WT7udMMghYPK2rbTxvg5E3tF9mwB1NRESkDgYiIdBi2cTXOUuL8rBm5iiYTVymCZUAoFea8ok/Lo0REUVGxJZm4omayya+esy4Ky3Kw5RCM3bXNeF0SzvOttix7J0vwrxq4njurpG4qSjP9f/XJ8OIh/66F6dsvr93wb4fRESkLs6IhCDQsonw9Uev9GTZSyrBdmQkGQRMGNwbt4y4BPcWFyDPlMrlmiBy0pPx4+KByM4wAoDr/694aB8s+b7/7x3AHTJERJHEQCRE/pZNpF0WT9w2DEDg/A6DANx3XYGiHRnuQRB5S002IDu9B5raOvHHHfW488VdmLhiOyprGl3HBPvecYcMEVHkKK6sGknRXlkV6NrKK037d6+2WVnTiKWbav3usJGClFBufpU1jXj4jX1oau0MZ/gJwd//c6DvHRERhU7J/ZuBiBstbkwdF50Yv3wbmvz0jZFyEj5eMFnxtYKdm74Rzv8zEREpo2mJ93jla/Yiz5SKxdMKw5qq//R4c8BAQYTvOiJypPQw4De3FmFuRZXrXORbOP/PRESkHeaIoCsImVtR5bWEYrG2Y25FlUd+gVJya46EWpuEW3yVYel8IqLokvAzIoHKtYvomtJfuqkWUwrNIU3py61HEU7diu5bfOvPtuHZrYc4Q+ID64MQEUWXhA9EgpVrD3dKX07NEYMANLfaFZ/bnbTFVzI0tyce/Es12i86wzpvrJBT18UgAKMHZEdsTEREFFzCL81YrBdkHRfqlL6c7bZOEShf/1lYS0DuKmsa8fCb+xIiCCnsn4lXy8bjqf8YHnQGyCl25ewQEVH0SOhApLKmUXaV0nCXTp67axQCreyIAB5+Yx/e+KwBO4+e8+rGK5eU73K+LTG29T469duYMLg3zsqcUWKOCBFRdEnYpRnphh3sdq9Wye/sjBQEiy2aWjvxwGvVAELbsRMo3yWQnsYe+Mp+UeGz5MtOT0ZzWycEqLuzJ8/t+xKJXBwiIlJfQs6IKL1hh1ry2+EUsfPoOWysbsCOI2cVPTeUHTvB8l38uSwnPeDXM1KSFJ/T3fLbhuH5MHb2+Cuj7/59kXJx/H2XBHgGLkREFB0SckZE7g07JyMZv7l1WEh1RIJVVQ1GCpKU7NgJddmhttEW8OuZqT3wwo/G4HSLHcve3i+7mmt2ejKW3/bN/5/7zp4+PY146C/VOGWzB2w+9+jUQix7x/P/0exjtkjKxZlbUeU188IeMkRE0SshAxG5N+xHv/ftkIMQOcs+cjRa27F6+2HML7k86LFaLTtYbHYYBAHmrFRZQcjYgTmYXzIU4wf19rjxd9/Zs+T73w4aOJQW5eGmIrOsirdSTZXuAaCvwIWIiKJDQgYicm/Y5izlN/ZQ8zQCWbn1ML5lzgx6I5WzVThUSmZbZoy/DMVD+gQ9Tm7g0D2ACXZO95kX9pAhIopuCRmIBLthh5OgGmqeRjBylmgCLU+ES8lsi5JjtQgclAQuRESkr4RMVnWv7eErERIIPZ9A7szBvEmDsfKO4cjJSJF1vFRULRi1S767J3mOLchBr/RkWccqIQUOt4y4BBMG9+bsBRFRAknIGRFAu3wCubMBxUP6YsLg3khLScKcr5vWBSM3yOk+y/CPmkZsrjkl67nuugdllTWNAeuTiGBCKBERKZOwgQigzbKA0mWf0qI8PFByOVZuPRT03EqWPKRZhsoQgxAAEASg7NoClBbluXJfAslOT8aUQnNI1yIiosSU0IEIoH4+gdJtpA6niDEDsmFK7QFru++iYqHmrMgJHgJxisALH9Zh5GXZMKWlBM19aW7rDLknDxERJaaEzBHRmr88DbMpFWtmjnIt+1TWNGLiiu2Y8YdPAgYhQGhLHmolzi7dVKt5Tx4iIkpMCT8jopVgyz5ya42Ek7OiRlAgdR9uau2QdTxLqBMRkRIMRDTkb9lHTq2RXmnJeG7GKK+iYEqoGRTk9DRqtuWZiIgSF5dmdCBnyeT8hU4YBEGVxFk19rCYs1I12/JMRESJi4GIDuQumYS7tBKoXopc7rVB5Oa+EBERycWlGR1EomW9wylid10T7BeduL9kKF7d/SUsNntI53Kf6WAJdSIiUhMDkQiRAoPTLe3ok2GEOSsVp2za5Fv46vxrzkrFAyWXY2CfdNSfbfs6MAk845LnJ1FWzpZn99fLYIWIiPwRRFFUuz+aamw2G0wmE6xWK7KysvQeTsh8BQa90pNxvq3Tb62RUJc6/O3G6X5ej8CopxEQu5aCmlo7kNOzK1AKNXjw9Xr9BTVERBR/lNy/GYhoLFBgIOKbgEQSzg3b4RQxccV2v4mw0kzLxwsmazY7ITcQIiKi+KXk/s2lGQ0F2qYrouvmnNrDgFd+Mg5nv7KHvYQRbDeOVBNEq+qncl6vnC7CRESUOBiIaEhOYGCx2WEQBNwy4pKwryd3l02w3JBQ6R0IERFR7OH2XQ1Fapsu0DUbcbZF3q6YZW/vR2VNY9jX7C6Sr5eIiOIDAxENRWKbLvBNz5pl73wh6/im1k7MrahSPRiJ1OslIqL4wUBEQ8Eqm7oXCwuVlBwaSnO7pZtq4XCql6sciddLRETxhYGIhgJVNu1eFt3hFLHz6DlsrG7AzqPnZAUIcnrW+OOer6EWJa+XiIgIYLKq5qSy6F4Fxty26YZad0NOz5pg1M7X8Pd6szOSceuIS2BKS4HDKTIYISIiAKwjEjH+Ko2GU3djY3UD5m+oDmtcr5aN12wr7+66JmypteDN6pNoau1wfY3FzYiI4puS+zeXZiJEKot+y4hLMGFwb9dyTKC6G0DgPI5wkz57pSVrlq+RZBBgvdCBdTvqPYIQALBY2zVJliUiotijaSDyzjvvYNy4cUhLS0N2djamT5+u5eVijpK6G74ESw4NZlbxQM2WSMINsoiIKDFoFoj8/e9/x913341Zs2Zh79692LFjB+666y6tLheTwq27ESg5NJjs9GTMmzxU4bPkCzfIIiKixKBJsurFixcxf/58PPnkk5g9e7br8cLCQi0up1i0dIZVo+6Gv+RQf031gK6gZfltwzR9zSxuRkREcmgSiFRVVaGhoQEGgwEjR46ExWLBiBEj8OSTT6KoqMjv8+x2O+z2b6qD2mw21ccWTZ1hpaUVi7Xd5xKG1KQuWB5HaVEephSavYKrLbUW3V4ri5sREZEcmgQix44dAwAsWbIEzzzzDAYOHIinn34a119/PQ4dOoScHN831uXLl2Pp0qVaDAmA/86wUvJkpDvDSksrcyuqvGYulNbdkJJh3fkLUCIx+6NWkEVERPFNUY7IwoULIQhCwI8DBw7A6XQCAB555BH84Ac/wOjRo7Fu3ToIgoC//vWvfs+/aNEiWK1W18eJEyfCe3VuojV5UlpaMZs8ZwbMplRVAiNfu3UigcXNiIhIDkUzIg899BDuvffegMcMGjQIjY1d2zLdc0KMRiMGDRqEL7/80u9zjUYjjEajkiHJFs2dYfWcudCSnGJuRESU2BQFIn379kXfvn2DHjd69GgYjUYcPHgQEydOBAB0dnaivr4eAwYMCG2kYYr25ElfSyvxIF6DLCIiUocmOSJZWVmYM2cOFi9ejPz8fAwYMABPPvkkAOD222/X4pJBMXlSP/EaZBERUfg06zXz5JNPokePHrj77rtx4cIFjBs3Dtu3b0d2drZWlwyIyZNERETRJ6F6zUi7ZgDfO1QivWuGiIgoHrHXjB9a71AhIiIiZTRbmolWTJ4kIiKKHgkXiABMniQiIooWCbU0Q0RERNGFgQgRERHphoEIERER6YaBCBEREemGgQgRERHphoEIERER6YaBCBEREemGgQgRERHphoEIERER6SaqK6tK/fhsNpvOIyEiIiK5pPu2nL66UR2ItLS0AADy8/N1HgkREREp1dLSApPJFPAYQZQTrujE6XTi5MmTyMzMhCDIb0pns9mQn5+PEydOBG0/HMsS4XUmwmsEEuN1JsJrBBLjdSbCawQS43Vq9RpFUURLSwv69+8PgyFwFkhUz4gYDAZceumlIT8/Kysrbn943CXC60yE1wgkxutMhNcIJMbrTITXCCTG69TiNQabCZEwWZWIiIh0w0CEiIiIdBOXgYjRaMTixYthNBr1HoqmEuF1JsJrBBLjdSbCawQS43UmwmsEEuN1RsNrjOpkVSIiIopvcTkjQkRERLGBgQgRERHphoEIERER6YaBCBEREekm7gORQ4cO4ZZbbkGfPn2QlZWFiRMn4r333tN7WJp45513MG7cOKSlpSE7OxvTp0/Xe0iasdvtGDFiBARBQHV1td7DUU19fT1mz56NgoICpKWlYfDgwVi8eDE6Ojr0HlrYnnvuOQwcOBCpqakYN24cdu/erfeQVLN8+XJcffXVyMzMRG5uLqZPn46DBw/qPSxNPfHEExAEAffff7/eQ1FdQ0MDZs6cid69eyMtLQ3Dhg3Dv/71L72HpSqHw4FHH33U471m2bJlsnrDqC3uA5Hvfe97uHjxIrZv345PP/0Uw4cPx/e+9z1YLBa9h6aqv//977j77rsxa9Ys7N27Fzt27MBdd92l97A086tf/Qr9+/fXexiqO3DgAJxOJ9auXYv9+/dj5cqVeP755/Hwww/rPbSwvPbaa3jwwQexePFiVFVVYfjw4bjppptw+vRpvYemig8++ADl5eXYtWsXtmzZgs7OTtx4441obW3Ve2ia2LNnD9auXYurrrpK76Gorrm5GcXFxUhOTsa7776L2tpaPP3008jOztZ7aKpasWIF1qxZg9WrV+OLL77AihUr8Nvf/harVq2K/GDEOHbmzBkRgPjhhx+6HrPZbCIAccuWLTqOTF2dnZ3iJZdcIv7+97/XeygRsXnzZvGKK64Q9+/fLwIQP/vsM72HpKnf/va3YkFBgd7DCMvYsWPF8vJy1+cOh0Ps37+/uHz5ch1HpZ3Tp0+LAMQPPvhA76GorqWlRRw6dKi4ZcsW8Tvf+Y44f/58vYekqgULFogTJ07Uexiamzp1qvjjH//Y47HbbrtNnDFjRsTHEtczIr1798a3vvUt/OlPf0JraysuXryItWvXIjc3F6NHj9Z7eKqpqqpCQ0MDDAYDRo4ciby8PHz3u99FTU2N3kNT3alTp1BWVoY///nPSE9P13s4EWG1WpGTk6P3MELW0dGBTz/9FCUlJa7HDAYDSkpKsHPnTh1Hph2r1QoAMf1986e8vBxTp071+H7Gk7feegtjxozB7bffjtzcXIwcORIvvvii3sNS3TXXXINt27bh0KFDAIC9e/fi448/xne/+92IjyWqm96FSxAEbN26FdOnT0dmZiYMBgNyc3NRWVkZV9Nsx44dAwAsWbIEzzzzDAYOHIinn34a119/PQ4dOhQ3b4aiKOLee+/FnDlzMGbMGNTX1+s9JM0dOXIEq1atwlNPPaX3UEJ29uxZOBwO9OvXz+Pxfv364cCBAzqNSjtOpxP3338/iouLUVRUpPdwVLVhwwZUVVVhz549eg9FM8eOHcOaNWvw4IMP4uGHH8aePXvwi1/8AikpKbjnnnv0Hp5qFi5cCJvNhiuuuAJJSUlwOBx4/PHHMWPGjIiPJSZnRBYuXAhBEAJ+HDhwAKIoory8HLm5ufjoo4+we/duTJ8+HdOmTUNjY6PeLyMoua/T6XQCAB555BH84Ac/wOjRo7Fu3ToIgoC//vWvOr+K4OS+zlWrVqGlpQWLFi3Se8iKyX2N7hoaGlBaWorbb78dZWVlOo2clCovL0dNTQ02bNig91BUdeLECcyfPx+vvPIKUlNT9R6OZpxOJ0aNGoXf/OY3GDlyJO677z6UlZXh+eef13toqvrLX/6CV155BevXr0dVVRVefvllPPXUU3j55ZcjPpaYLPF+5swZnDt3LuAxgwYNwkcffYQbb7wRzc3NHu2Nhw4ditmzZ2PhwoVaDzUscl/njh07MHnyZHz00UeYOHGi62vjxo1DSUkJHn/8ca2HGha5r/OOO+7Apk2bIAiC63GHw4GkpCTMmDFDl18gueS+xpSUFADAyZMncf3112P8+PF46aWXYDDE5N8MALqWZtLT0/G3v/3NYyfXPffcg/Pnz2Pjxo36DU5l8+bNw8aNG/Hhhx+ioKBA7+Go6s0338Stt96KpKQk12MOhwOCIMBgMMBut3t8LVYNGDAAU6ZMwe9//3vXY2vWrMFjjz2GhoYGHUemrvz8fCxcuBDl5eWuxx577DFUVFREfKYyJpdm+vbti759+wY9rq2tDQC83sQNBoNrFiGayX2do0ePhtFoxMGDB12BSGdnJ+rr6zFgwACthxk2ua/zd7/7HR577DHX5ydPnsRNN92E1157DePGjdNyiGGT+xqBrpmQSZMmuWa2YjkIAYCUlBSMHj0a27ZtcwUiTqcT27Ztw7x58/QdnEpEUcTPf/5zvPHGG3j//ffjLggBgBtuuAH79u3zeGzWrFm44oorsGDBgrgIQgCguLjYa+v1oUOHYuK9VIm2tjav95akpCR97o0RT4+NoDNnzoi9e/cWb7vtNrG6ulo8ePCg+Mtf/lJMTk4Wq6ur9R6equbPny9ecskl4j/+8Q/xwIED4uzZs8Xc3FyxqalJ76Fppq6uLu52zfz73/8WhwwZIt5www3iv//9b7GxsdH1Ecs2bNggGo1G8aWXXhJra2vF++67T+zVq5dosVj0Hpoq5s6dK5pMJvH999/3+J61tbXpPTRNxeOumd27d4s9evQQH3/8cfHw4cPiK6+8Iqanp4sVFRV6D01V99xzj3jJJZeIb7/9tlhXVye+/vrrYp8+fcRf/epXER9LXAcioiiKe/bsEW+88UYxJydHzMzMFMePHy9u3rxZ72GprqOjQ3zooYfE3NxcMTMzUywpKRFramr0Hpam4jEQWbdunQjA50esW7VqlXjZZZeJKSkp4tixY8Vdu3bpPSTV+PuerVu3Tu+haSoeAxFRFMVNmzaJRUVFotFoFK+44grxhRde0HtIqrPZbOL8+fPFyy67TExNTRUHDRokPvLII6Ldbo/4WGIyR4SIiIjiQ2wvPhMREVFMYyBCREREumEgQkRERLphIEJERES6YSBCREREumEgQkRERLphIEJERES6YSBCREREumEgQkRERLphIEJERES6YSBCREREumEgQkRERLr5/1TpR9LEDuu9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "   \n",
    "model3.eval()\n",
    "inf = inference.CausalInference(model=model3, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X1':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X1':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names3.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data3, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data3, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model3(train_data.to(device))\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1].detach().cpu().numpy())\n",
    "print('Mean Squared Error Across All Vars:', ((train_data - preds.detach().cpu())**2).mean())\n",
    "print('Mean Squared Error Across Outcome:', ((train_data[:,effect_index,:] - preds[:,effect_index,:].detach().cpu())**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565ba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae93b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cb210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71483b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a705d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89844f5-04ea-41cd-b38b-38a6cae4162b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
