{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75c3bc-21dd-4523-bc43-48758d1d6517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c695a81-04b9-4d14-bc74-11187e97a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0949a58-1135-4b21-9068-ee725ae55a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import CaT\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_full_ordering, reorder_dag\n",
    "\n",
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 50000\n",
    "batch_size = 100\n",
    "max_iters = 80000\n",
    "eval_interval = 200\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.3\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 5e-3\n",
    "ff_n_embed = 6\n",
    "num_heads = 2\n",
    "n_layers = 2\n",
    "head_size = 6\n",
    "d = 5\n",
    "\n",
    "\n",
    "def generate_data_mediation(N, d=5):\n",
    "    DAGnx = nx.DiGraph()\n",
    "    \n",
    "    Ux = np.random.randn(N,d)\n",
    "    X =  Ux\n",
    "\n",
    "    Um = np.random.randn(N,d)\n",
    "    M =  0.2 * X + Um\n",
    "\n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y =  0.7 * M + 0.1 * Uy\n",
    "\n",
    "    M0 = 0.2 * 0 + Um \n",
    "    M1 = 0.2 * 1 + Um\n",
    "\n",
    "    Y0 = 0.7 * M0 + 0.1 * Uy \n",
    "    Y1 = 0.7 * M1 + 0.1 * Uy   # total effect   = 0.7 * 0.2 = 0.14\n",
    "\n",
    "    all_data_dict = {'X': X, 'M': M, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'M': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'M'), ('M', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e1c09c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'M', 'Y'] [0.14 0.14 0.14]\n",
      "(50000, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "d=3\n",
    "_, _, _, _, _, Y0, Y1 = generate_data_mediation(N=1000000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  # multi-dim ATE based off a large sample\n",
    "all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data_mediation(N=sample_size, d=d)\n",
    "print(var_names, ATE)\n",
    "print(all_data.shape)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63c1b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = all_data.shape[2]\n",
    "\n",
    "model = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx,\n",
    "                causal_ordering=causal_ordering,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types=var_types,\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d32adc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 80000: train_loss 1.5687, val loss 1.5536\n",
      "step 200 of 80000: train_loss 1.1992, val loss 1.1714\n",
      "step 400 of 80000: train_loss 1.1583, val loss 1.1489\n",
      "step 600 of 80000: train_loss 1.1226, val loss 1.0958\n",
      "step 800 of 80000: train_loss 1.0927, val loss 1.0807\n",
      "step 1000 of 80000: train_loss 1.0753, val loss 1.0694\n",
      "step 1200 of 80000: train_loss 1.0818, val loss 1.0549\n",
      "step 1400 of 80000: train_loss 1.0483, val loss 1.0440\n",
      "step 1600 of 80000: train_loss 1.0487, val loss 1.0363\n",
      "step 1800 of 80000: train_loss 1.0465, val loss 1.0333\n",
      "step 2000 of 80000: train_loss 1.0414, val loss 1.0325\n",
      "step 2200 of 80000: train_loss 1.0250, val loss 1.0284\n",
      "step 2400 of 80000: train_loss 1.0298, val loss 1.0199\n",
      "step 2600 of 80000: train_loss 1.0150, val loss 1.0008\n",
      "step 2800 of 80000: train_loss 1.0178, val loss 1.0164\n",
      "step 3000 of 80000: train_loss 1.0081, val loss 1.0041\n",
      "step 3200 of 80000: train_loss 1.0072, val loss 1.0069\n",
      "step 3400 of 80000: train_loss 0.9859, val loss 0.9859\n",
      "step 3600 of 80000: train_loss 0.9843, val loss 0.9932\n",
      "step 3800 of 80000: train_loss 0.9924, val loss 0.9870\n",
      "step 4000 of 80000: train_loss 0.9692, val loss 0.9672\n",
      "step 4200 of 80000: train_loss 0.9810, val loss 0.9717\n",
      "step 4400 of 80000: train_loss 0.9364, val loss 0.9331\n",
      "step 4600 of 80000: train_loss 0.9354, val loss 0.9493\n",
      "step 4800 of 80000: train_loss 0.9152, val loss 0.9185\n",
      "step 5000 of 80000: train_loss 0.8895, val loss 0.8935\n",
      "step 5200 of 80000: train_loss 0.8684, val loss 0.8490\n",
      "step 5400 of 80000: train_loss 0.8183, val loss 0.8154\n",
      "step 5600 of 80000: train_loss 0.7766, val loss 0.7769\n",
      "step 5800 of 80000: train_loss 0.7426, val loss 0.7428\n",
      "step 6000 of 80000: train_loss 0.7280, val loss 0.7272\n",
      "step 6200 of 80000: train_loss 0.7121, val loss 0.7129\n",
      "step 6400 of 80000: train_loss 0.6898, val loss 0.6826\n",
      "step 6600 of 80000: train_loss 0.6796, val loss 0.6847\n",
      "step 6800 of 80000: train_loss 0.6678, val loss 0.6702\n",
      "step 7000 of 80000: train_loss 0.6919, val loss 0.6720\n",
      "step 7200 of 80000: train_loss 0.6500, val loss 0.6623\n",
      "step 7400 of 80000: train_loss 0.6612, val loss 0.6540\n",
      "step 7600 of 80000: train_loss 0.6795, val loss 0.6740\n",
      "step 7800 of 80000: train_loss 0.6524, val loss 0.6598\n",
      "step 8000 of 80000: train_loss 0.6433, val loss 0.6397\n",
      "step 8200 of 80000: train_loss 0.6597, val loss 0.6470\n",
      "step 8400 of 80000: train_loss 0.6460, val loss 0.6466\n",
      "step 8600 of 80000: train_loss 0.6466, val loss 0.6228\n",
      "step 8800 of 80000: train_loss 0.6509, val loss 0.6362\n",
      "step 9000 of 80000: train_loss 0.6546, val loss 0.6470\n",
      "step 9200 of 80000: train_loss 0.6316, val loss 0.6479\n",
      "step 9400 of 80000: train_loss 0.6430, val loss 0.6441\n",
      "step 9600 of 80000: train_loss 0.6601, val loss 0.6584\n",
      "step 9800 of 80000: train_loss 0.6731, val loss 0.6772\n",
      "step 10000 of 80000: train_loss 0.6379, val loss 0.6451\n",
      "step 10200 of 80000: train_loss 0.6372, val loss 0.6375\n",
      "step 10400 of 80000: train_loss 0.6423, val loss 0.6338\n",
      "step 10600 of 80000: train_loss 0.6553, val loss 0.6530\n",
      "step 10800 of 80000: train_loss 0.6678, val loss 0.6608\n",
      "step 11000 of 80000: train_loss 0.6272, val loss 0.6309\n",
      "step 11200 of 80000: train_loss 0.6307, val loss 0.6402\n",
      "step 11400 of 80000: train_loss 0.6347, val loss 0.6384\n",
      "step 11600 of 80000: train_loss 0.6513, val loss 0.6401\n",
      "step 11800 of 80000: train_loss 0.6476, val loss 0.6425\n",
      "step 12000 of 80000: train_loss 0.6627, val loss 0.6524\n",
      "step 12200 of 80000: train_loss 0.6391, val loss 0.6343\n",
      "step 12400 of 80000: train_loss 0.6921, val loss 0.6906\n",
      "step 12600 of 80000: train_loss 0.6310, val loss 0.6369\n",
      "step 12800 of 80000: train_loss 0.6417, val loss 0.6437\n",
      "step 13000 of 80000: train_loss 0.6290, val loss 0.6351\n",
      "step 13200 of 80000: train_loss 0.6282, val loss 0.6416\n",
      "step 13400 of 80000: train_loss 0.6558, val loss 0.6335\n",
      "step 13600 of 80000: train_loss 0.6223, val loss 0.6311\n",
      "step 13800 of 80000: train_loss 0.6448, val loss 0.6376\n",
      "step 14000 of 80000: train_loss 0.6466, val loss 0.6356\n",
      "step 14200 of 80000: train_loss 0.6362, val loss 0.6219\n",
      "step 14400 of 80000: train_loss 0.8425, val loss 0.8491\n",
      "step 14600 of 80000: train_loss 0.7703, val loss 0.7660\n",
      "step 14800 of 80000: train_loss 0.7227, val loss 0.7357\n",
      "step 15000 of 80000: train_loss 0.7116, val loss 0.7151\n",
      "step 15200 of 80000: train_loss 0.6950, val loss 0.6960\n",
      "step 15400 of 80000: train_loss 0.6717, val loss 0.6866\n",
      "step 15600 of 80000: train_loss 0.6842, val loss 0.6759\n",
      "step 15800 of 80000: train_loss 0.6753, val loss 0.6943\n",
      "step 16000 of 80000: train_loss 0.6691, val loss 0.6707\n",
      "step 16200 of 80000: train_loss 0.6628, val loss 0.6671\n",
      "step 16400 of 80000: train_loss 0.6572, val loss 0.6525\n",
      "step 16600 of 80000: train_loss 0.6677, val loss 0.6518\n",
      "step 16800 of 80000: train_loss 0.6459, val loss 0.6542\n",
      "step 17000 of 80000: train_loss 0.6494, val loss 0.6535\n",
      "step 17200 of 80000: train_loss 0.6437, val loss 0.6542\n",
      "step 17400 of 80000: train_loss 0.6808, val loss 0.6695\n",
      "step 17600 of 80000: train_loss 0.6235, val loss 0.6354\n",
      "step 17800 of 80000: train_loss 0.6518, val loss 0.6448\n",
      "step 18000 of 80000: train_loss 0.6498, val loss 0.6533\n",
      "step 18200 of 80000: train_loss 0.6832, val loss 0.6846\n",
      "step 18400 of 80000: train_loss 0.6383, val loss 0.6217\n",
      "step 18600 of 80000: train_loss 0.6417, val loss 0.6351\n",
      "step 18800 of 80000: train_loss 0.6643, val loss 0.6483\n",
      "step 19000 of 80000: train_loss 0.6491, val loss 0.6389\n",
      "step 19200 of 80000: train_loss 0.6299, val loss 0.6316\n",
      "step 19400 of 80000: train_loss 0.6212, val loss 0.6274\n",
      "step 19600 of 80000: train_loss 0.6272, val loss 0.6318\n",
      "step 19800 of 80000: train_loss 0.6285, val loss 0.6337\n",
      "step 20000 of 80000: train_loss 0.6306, val loss 0.6473\n",
      "step 20200 of 80000: train_loss 0.6340, val loss 0.6468\n",
      "step 20400 of 80000: train_loss 0.6347, val loss 0.6347\n",
      "step 20600 of 80000: train_loss 0.6353, val loss 0.6288\n",
      "step 20800 of 80000: train_loss 0.6411, val loss 0.6518\n",
      "step 21000 of 80000: train_loss 0.6368, val loss 0.6314\n",
      "step 21200 of 80000: train_loss 0.6341, val loss 0.6364\n",
      "step 21400 of 80000: train_loss 0.6284, val loss 0.6353\n",
      "step 21600 of 80000: train_loss 0.6306, val loss 0.6153\n",
      "step 21800 of 80000: train_loss 0.6272, val loss 0.6187\n",
      "step 22000 of 80000: train_loss 0.6308, val loss 0.6325\n",
      "step 22200 of 80000: train_loss 0.6245, val loss 0.6376\n",
      "step 22400 of 80000: train_loss 0.6326, val loss 0.6347\n",
      "step 22600 of 80000: train_loss 0.6145, val loss 0.6125\n",
      "step 22800 of 80000: train_loss 0.6391, val loss 0.6357\n",
      "step 23000 of 80000: train_loss 0.6380, val loss 0.6537\n",
      "step 23200 of 80000: train_loss 0.6276, val loss 0.6345\n",
      "step 23400 of 80000: train_loss 0.6229, val loss 0.6136\n",
      "step 23600 of 80000: train_loss 0.6121, val loss 0.6139\n",
      "step 23800 of 80000: train_loss 0.6267, val loss 0.6237\n",
      "step 24000 of 80000: train_loss 0.6418, val loss 0.6189\n",
      "step 24200 of 80000: train_loss 0.6334, val loss 0.6344\n",
      "step 24400 of 80000: train_loss 0.6328, val loss 0.6301\n",
      "step 24600 of 80000: train_loss 0.6238, val loss 0.6343\n",
      "step 24800 of 80000: train_loss 0.6233, val loss 0.6207\n",
      "step 25000 of 80000: train_loss 0.6205, val loss 0.6410\n",
      "step 25200 of 80000: train_loss 0.6206, val loss 0.5960\n",
      "step 25400 of 80000: train_loss 0.6286, val loss 0.6254\n",
      "step 25600 of 80000: train_loss 0.6405, val loss 0.6530\n",
      "step 25800 of 80000: train_loss 0.6090, val loss 0.6250\n",
      "step 26000 of 80000: train_loss 0.6295, val loss 0.6119\n",
      "step 26200 of 80000: train_loss 0.6203, val loss 0.6313\n",
      "step 26400 of 80000: train_loss 0.6214, val loss 0.6219\n",
      "step 26600 of 80000: train_loss 0.6233, val loss 0.6387\n",
      "step 26800 of 80000: train_loss 0.6304, val loss 0.6139\n",
      "step 27000 of 80000: train_loss 0.6170, val loss 0.6281\n",
      "step 27200 of 80000: train_loss 0.6369, val loss 0.6204\n",
      "step 27400 of 80000: train_loss 0.6171, val loss 0.6141\n",
      "step 27600 of 80000: train_loss 0.6238, val loss 0.6275\n",
      "step 27800 of 80000: train_loss 0.6252, val loss 0.6237\n",
      "step 28000 of 80000: train_loss 0.6234, val loss 0.6197\n",
      "step 28200 of 80000: train_loss 0.6285, val loss 0.6342\n",
      "step 28400 of 80000: train_loss 0.6054, val loss 0.6094\n",
      "step 28600 of 80000: train_loss 0.6149, val loss 0.6163\n",
      "step 28800 of 80000: train_loss 0.6034, val loss 0.5949\n",
      "step 29000 of 80000: train_loss 0.6266, val loss 0.6232\n",
      "step 29200 of 80000: train_loss 0.6210, val loss 0.6286\n",
      "step 29400 of 80000: train_loss 0.6110, val loss 0.6149\n",
      "step 29600 of 80000: train_loss 0.6158, val loss 0.6284\n",
      "step 29800 of 80000: train_loss 0.6513, val loss 0.6577\n",
      "step 30000 of 80000: train_loss 0.6085, val loss 0.6075\n",
      "step 30200 of 80000: train_loss 0.6333, val loss 0.6236\n",
      "step 30400 of 80000: train_loss 0.6034, val loss 0.6109\n",
      "step 30600 of 80000: train_loss 0.6150, val loss 0.6122\n",
      "step 30800 of 80000: train_loss 0.6134, val loss 0.6205\n",
      "step 31000 of 80000: train_loss 0.6205, val loss 0.6238\n",
      "step 31200 of 80000: train_loss 0.6227, val loss 0.6278\n",
      "step 31400 of 80000: train_loss 0.6200, val loss 0.6240\n",
      "step 31600 of 80000: train_loss 0.6029, val loss 0.6106\n",
      "step 31800 of 80000: train_loss 0.6225, val loss 0.6164\n",
      "step 32000 of 80000: train_loss 0.6206, val loss 0.6251\n",
      "step 32200 of 80000: train_loss 0.6184, val loss 0.6331\n",
      "step 32400 of 80000: train_loss 0.6095, val loss 0.6294\n",
      "step 32600 of 80000: train_loss 0.6217, val loss 0.6031\n",
      "step 32800 of 80000: train_loss 0.6164, val loss 0.6170\n",
      "step 33000 of 80000: train_loss 0.6149, val loss 0.6212\n",
      "step 33200 of 80000: train_loss 0.6048, val loss 0.5983\n",
      "step 33400 of 80000: train_loss 0.6201, val loss 0.6150\n",
      "step 33600 of 80000: train_loss 0.6250, val loss 0.6140\n",
      "step 33800 of 80000: train_loss 0.6302, val loss 0.6206\n",
      "step 34000 of 80000: train_loss 0.6236, val loss 0.6064\n",
      "step 34200 of 80000: train_loss 0.6121, val loss 0.6256\n",
      "step 34400 of 80000: train_loss 0.6198, val loss 0.6296\n",
      "step 34600 of 80000: train_loss 0.6200, val loss 0.6393\n",
      "step 34800 of 80000: train_loss 0.6134, val loss 0.6125\n",
      "step 35000 of 80000: train_loss 0.5969, val loss 0.6024\n",
      "step 35200 of 80000: train_loss 0.6194, val loss 0.6272\n",
      "step 35400 of 80000: train_loss 0.6096, val loss 0.6182\n",
      "step 35600 of 80000: train_loss 0.6295, val loss 0.6282\n",
      "step 35800 of 80000: train_loss 0.5982, val loss 0.6097\n",
      "step 36000 of 80000: train_loss 0.6162, val loss 0.6188\n",
      "step 36200 of 80000: train_loss 0.6141, val loss 0.6158\n",
      "step 36400 of 80000: train_loss 0.6117, val loss 0.6105\n",
      "step 36600 of 80000: train_loss 0.6373, val loss 0.6331\n",
      "step 36800 of 80000: train_loss 0.6056, val loss 0.6197\n",
      "step 37000 of 80000: train_loss 0.6177, val loss 0.6156\n",
      "step 37200 of 80000: train_loss 0.6073, val loss 0.6105\n",
      "step 37400 of 80000: train_loss 0.6182, val loss 0.6141\n",
      "step 37600 of 80000: train_loss 0.5910, val loss 0.6017\n",
      "step 37800 of 80000: train_loss 0.6113, val loss 0.6178\n",
      "step 38000 of 80000: train_loss 0.6011, val loss 0.6216\n",
      "step 38200 of 80000: train_loss 0.6160, val loss 0.6032\n",
      "step 38400 of 80000: train_loss 0.6198, val loss 0.6033\n",
      "step 38600 of 80000: train_loss 0.6094, val loss 0.6130\n",
      "step 38800 of 80000: train_loss 0.6326, val loss 0.6230\n",
      "step 39000 of 80000: train_loss 0.6176, val loss 0.5982\n",
      "step 39200 of 80000: train_loss 0.6145, val loss 0.6108\n",
      "step 39400 of 80000: train_loss 0.6057, val loss 0.6035\n",
      "step 39600 of 80000: train_loss 0.6040, val loss 0.6045\n",
      "step 39800 of 80000: train_loss 0.6340, val loss 0.6241\n",
      "step 40000 of 80000: train_loss 0.6104, val loss 0.6164\n",
      "step 40200 of 80000: train_loss 0.5951, val loss 0.6107\n",
      "step 40400 of 80000: train_loss 0.5988, val loss 0.6097\n",
      "step 40600 of 80000: train_loss 0.6135, val loss 0.6063\n",
      "step 40800 of 80000: train_loss 0.6115, val loss 0.6087\n",
      "step 41000 of 80000: train_loss 0.6353, val loss 0.6410\n",
      "step 41200 of 80000: train_loss 0.6097, val loss 0.6033\n",
      "step 41400 of 80000: train_loss 0.6209, val loss 0.6179\n",
      "step 41600 of 80000: train_loss 0.6152, val loss 0.6222\n",
      "step 41800 of 80000: train_loss 0.6056, val loss 0.6062\n",
      "step 42000 of 80000: train_loss 0.6265, val loss 0.5980\n",
      "step 42200 of 80000: train_loss 0.6080, val loss 0.6191\n",
      "step 42400 of 80000: train_loss 0.6173, val loss 0.6189\n",
      "step 42600 of 80000: train_loss 0.6111, val loss 0.6159\n",
      "step 42800 of 80000: train_loss 0.6012, val loss 0.6067\n",
      "step 43000 of 80000: train_loss 0.6104, val loss 0.6036\n",
      "step 43200 of 80000: train_loss 0.5938, val loss 0.5995\n",
      "step 43400 of 80000: train_loss 0.6243, val loss 0.6210\n",
      "step 43600 of 80000: train_loss 0.6069, val loss 0.6019\n",
      "step 43800 of 80000: train_loss 0.5931, val loss 0.6250\n",
      "step 44000 of 80000: train_loss 0.6048, val loss 0.6039\n",
      "step 44200 of 80000: train_loss 0.6024, val loss 0.6020\n",
      "step 44400 of 80000: train_loss 0.6084, val loss 0.6025\n",
      "step 44600 of 80000: train_loss 0.5931, val loss 0.6073\n",
      "step 44800 of 80000: train_loss 0.6124, val loss 0.6057\n",
      "step 45000 of 80000: train_loss 0.6127, val loss 0.6196\n",
      "step 45200 of 80000: train_loss 0.6300, val loss 0.6114\n",
      "step 45400 of 80000: train_loss 0.6089, val loss 0.6179\n",
      "step 45600 of 80000: train_loss 0.6130, val loss 0.6143\n",
      "step 45800 of 80000: train_loss 0.6010, val loss 0.5998\n",
      "step 46000 of 80000: train_loss 0.6014, val loss 0.6107\n",
      "step 46200 of 80000: train_loss 0.6159, val loss 0.6095\n",
      "step 46400 of 80000: train_loss 0.6417, val loss 0.6336\n",
      "step 46600 of 80000: train_loss 0.6069, val loss 0.6257\n",
      "step 46800 of 80000: train_loss 0.6062, val loss 0.6021\n",
      "step 47000 of 80000: train_loss 0.6300, val loss 0.6306\n",
      "step 47200 of 80000: train_loss 0.6146, val loss 0.6117\n",
      "step 47400 of 80000: train_loss 0.6199, val loss 0.6075\n",
      "step 47600 of 80000: train_loss 0.5905, val loss 0.6063\n",
      "step 47800 of 80000: train_loss 0.6156, val loss 0.6024\n",
      "step 48000 of 80000: train_loss 0.5891, val loss 0.6174\n",
      "step 48200 of 80000: train_loss 0.6062, val loss 0.6065\n",
      "step 48400 of 80000: train_loss 0.6208, val loss 0.6251\n",
      "step 48600 of 80000: train_loss 0.6046, val loss 0.6133\n",
      "step 48800 of 80000: train_loss 0.6191, val loss 0.6224\n",
      "step 49000 of 80000: train_loss 0.6250, val loss 0.6267\n",
      "step 49200 of 80000: train_loss 0.6254, val loss 0.5938\n",
      "step 49400 of 80000: train_loss 0.6042, val loss 0.6096\n",
      "step 49600 of 80000: train_loss 0.6133, val loss 0.6183\n",
      "step 49800 of 80000: train_loss 0.6138, val loss 0.6169\n",
      "step 50000 of 80000: train_loss 0.5941, val loss 0.6026\n",
      "step 50200 of 80000: train_loss 0.6005, val loss 0.6001\n",
      "step 50400 of 80000: train_loss 0.6147, val loss 0.6072\n",
      "step 50600 of 80000: train_loss 0.6096, val loss 0.6046\n",
      "step 50800 of 80000: train_loss 0.6120, val loss 0.5986\n",
      "step 51000 of 80000: train_loss 0.6068, val loss 0.5989\n",
      "step 51200 of 80000: train_loss 0.6239, val loss 0.6106\n",
      "step 51400 of 80000: train_loss 0.6227, val loss 0.6004\n",
      "step 51600 of 80000: train_loss 0.6094, val loss 0.6109\n",
      "step 51800 of 80000: train_loss 0.5927, val loss 0.6010\n",
      "step 52000 of 80000: train_loss 0.6044, val loss 0.6223\n",
      "step 52200 of 80000: train_loss 0.6055, val loss 0.6020\n",
      "step 52400 of 80000: train_loss 0.6119, val loss 0.6163\n",
      "step 52600 of 80000: train_loss 0.5980, val loss 0.6125\n",
      "step 52800 of 80000: train_loss 0.6202, val loss 0.5915\n",
      "step 53000 of 80000: train_loss 0.6070, val loss 0.6017\n",
      "step 53200 of 80000: train_loss 0.6140, val loss 0.6213\n",
      "step 53400 of 80000: train_loss 0.6134, val loss 0.6051\n",
      "step 53600 of 80000: train_loss 0.6087, val loss 0.6068\n",
      "step 53800 of 80000: train_loss 0.6196, val loss 0.6228\n",
      "step 54000 of 80000: train_loss 0.6140, val loss 0.6043\n",
      "step 54200 of 80000: train_loss 0.5963, val loss 0.6124\n",
      "step 54400 of 80000: train_loss 0.5984, val loss 0.6135\n",
      "step 54600 of 80000: train_loss 0.6020, val loss 0.6105\n",
      "step 54800 of 80000: train_loss 0.6107, val loss 0.6027\n",
      "step 55000 of 80000: train_loss 0.6199, val loss 0.6084\n",
      "step 55200 of 80000: train_loss 0.6227, val loss 0.6374\n",
      "step 55400 of 80000: train_loss 0.5960, val loss 0.6122\n",
      "step 55600 of 80000: train_loss 0.6066, val loss 0.6108\n",
      "step 55800 of 80000: train_loss 0.6047, val loss 0.6045\n",
      "step 56000 of 80000: train_loss 0.6014, val loss 0.6139\n",
      "step 56200 of 80000: train_loss 0.5899, val loss 0.6090\n",
      "step 56400 of 80000: train_loss 0.6377, val loss 0.6399\n",
      "step 56600 of 80000: train_loss 0.6044, val loss 0.5992\n",
      "step 56800 of 80000: train_loss 0.6131, val loss 0.5937\n",
      "step 57000 of 80000: train_loss 0.6141, val loss 0.6002\n",
      "step 57200 of 80000: train_loss 0.5994, val loss 0.5944\n",
      "step 57400 of 80000: train_loss 0.6056, val loss 0.6091\n",
      "step 57600 of 80000: train_loss 0.6040, val loss 0.6013\n",
      "step 57800 of 80000: train_loss 0.6008, val loss 0.6095\n",
      "step 58000 of 80000: train_loss 0.6046, val loss 0.6070\n",
      "step 58200 of 80000: train_loss 0.6251, val loss 0.6271\n",
      "step 58400 of 80000: train_loss 0.6261, val loss 0.6203\n",
      "step 58600 of 80000: train_loss 0.6094, val loss 0.6108\n",
      "step 58800 of 80000: train_loss 0.6051, val loss 0.6044\n",
      "step 59000 of 80000: train_loss 0.6135, val loss 0.6080\n",
      "step 59200 of 80000: train_loss 0.6203, val loss 0.6113\n",
      "step 59400 of 80000: train_loss 0.5825, val loss 0.5950\n",
      "step 59600 of 80000: train_loss 0.6253, val loss 0.6229\n",
      "step 59800 of 80000: train_loss 0.6151, val loss 0.6109\n",
      "step 60000 of 80000: train_loss 0.6002, val loss 0.6099\n",
      "step 60200 of 80000: train_loss 0.6161, val loss 0.6097\n",
      "step 60400 of 80000: train_loss 0.5992, val loss 0.6036\n",
      "step 60600 of 80000: train_loss 0.6082, val loss 0.6328\n",
      "step 60800 of 80000: train_loss 0.6053, val loss 0.6004\n",
      "step 61000 of 80000: train_loss 0.6034, val loss 0.6182\n",
      "step 61200 of 80000: train_loss 0.6057, val loss 0.6018\n",
      "step 61400 of 80000: train_loss 0.6013, val loss 0.5953\n",
      "step 61600 of 80000: train_loss 0.6366, val loss 0.6344\n",
      "step 61800 of 80000: train_loss 0.6148, val loss 0.6147\n",
      "step 62000 of 80000: train_loss 0.6084, val loss 0.6058\n",
      "step 62200 of 80000: train_loss 0.6060, val loss 0.6167\n",
      "step 62400 of 80000: train_loss 0.6125, val loss 0.6227\n",
      "step 62600 of 80000: train_loss 0.6051, val loss 0.6223\n",
      "step 62800 of 80000: train_loss 0.6031, val loss 0.6095\n",
      "step 63000 of 80000: train_loss 0.6154, val loss 0.6110\n",
      "step 63200 of 80000: train_loss 0.6268, val loss 0.6276\n",
      "step 63400 of 80000: train_loss 0.6023, val loss 0.6008\n",
      "step 63600 of 80000: train_loss 0.6076, val loss 0.6178\n",
      "step 63800 of 80000: train_loss 0.5955, val loss 0.6114\n",
      "step 64000 of 80000: train_loss 0.6249, val loss 0.6237\n",
      "step 64200 of 80000: train_loss 0.6115, val loss 0.6104\n",
      "step 64400 of 80000: train_loss 0.6390, val loss 0.6459\n",
      "step 64600 of 80000: train_loss 0.6020, val loss 0.6042\n",
      "step 64800 of 80000: train_loss 0.6006, val loss 0.6034\n",
      "step 65000 of 80000: train_loss 0.6182, val loss 0.6158\n",
      "step 65200 of 80000: train_loss 0.6113, val loss 0.6040\n",
      "step 65400 of 80000: train_loss 0.6043, val loss 0.6078\n",
      "step 65600 of 80000: train_loss 0.6040, val loss 0.6014\n",
      "step 65800 of 80000: train_loss 0.6267, val loss 0.6275\n",
      "step 66000 of 80000: train_loss 0.6322, val loss 0.6175\n",
      "step 66200 of 80000: train_loss 0.6030, val loss 0.6045\n",
      "step 66400 of 80000: train_loss 0.5975, val loss 0.6115\n",
      "step 66600 of 80000: train_loss 0.6197, val loss 0.6153\n",
      "step 66800 of 80000: train_loss 0.5966, val loss 0.5979\n",
      "step 67000 of 80000: train_loss 0.6222, val loss 0.6244\n",
      "step 67200 of 80000: train_loss 0.6048, val loss 0.5952\n",
      "step 67400 of 80000: train_loss 0.5947, val loss 0.6144\n",
      "step 67600 of 80000: train_loss 0.5779, val loss 0.6004\n",
      "step 67800 of 80000: train_loss 0.6000, val loss 0.6032\n",
      "step 68000 of 80000: train_loss 0.6061, val loss 0.6090\n",
      "step 68200 of 80000: train_loss 0.6013, val loss 0.6118\n",
      "step 68400 of 80000: train_loss 0.6131, val loss 0.6101\n",
      "step 68600 of 80000: train_loss 0.5955, val loss 0.6100\n",
      "step 68800 of 80000: train_loss 0.6092, val loss 0.6021\n",
      "step 69000 of 80000: train_loss 0.6298, val loss 0.6175\n",
      "step 69200 of 80000: train_loss 0.6096, val loss 0.6124\n",
      "step 69400 of 80000: train_loss 0.6050, val loss 0.6027\n",
      "step 69600 of 80000: train_loss 0.5976, val loss 0.6107\n",
      "step 69800 of 80000: train_loss 0.6241, val loss 0.6316\n",
      "step 70000 of 80000: train_loss 0.6063, val loss 0.6007\n",
      "step 70200 of 80000: train_loss 0.6076, val loss 0.6108\n",
      "step 70400 of 80000: train_loss 0.6002, val loss 0.6011\n",
      "step 70600 of 80000: train_loss 0.6037, val loss 0.6167\n",
      "step 70800 of 80000: train_loss 0.6107, val loss 0.6031\n",
      "step 71000 of 80000: train_loss 0.6280, val loss 0.6197\n",
      "step 71200 of 80000: train_loss 0.6464, val loss 0.6561\n",
      "step 71400 of 80000: train_loss 0.6126, val loss 0.5979\n",
      "step 71600 of 80000: train_loss 0.6001, val loss 0.6047\n",
      "step 71800 of 80000: train_loss 0.6038, val loss 0.6153\n",
      "step 72000 of 80000: train_loss 0.6129, val loss 0.6034\n",
      "step 72200 of 80000: train_loss 0.5997, val loss 0.5968\n",
      "step 72400 of 80000: train_loss 0.5905, val loss 0.6367\n",
      "step 72600 of 80000: train_loss 0.6156, val loss 0.6094\n",
      "step 72800 of 80000: train_loss 0.6793, val loss 0.6872\n",
      "step 73000 of 80000: train_loss 0.6264, val loss 0.6148\n",
      "step 73200 of 80000: train_loss 0.6095, val loss 0.6299\n",
      "step 73400 of 80000: train_loss 0.5979, val loss 0.6072\n",
      "step 73600 of 80000: train_loss 0.6113, val loss 0.6075\n",
      "step 73800 of 80000: train_loss 0.6060, val loss 0.6074\n",
      "step 74000 of 80000: train_loss 0.6097, val loss 0.6073\n",
      "step 74200 of 80000: train_loss 0.6297, val loss 0.6228\n",
      "step 74400 of 80000: train_loss 0.6233, val loss 0.6195\n",
      "step 74600 of 80000: train_loss 0.6097, val loss 0.6086\n",
      "step 74800 of 80000: train_loss 0.6002, val loss 0.6076\n",
      "step 75000 of 80000: train_loss 0.6226, val loss 0.5923\n",
      "step 75200 of 80000: train_loss 0.6040, val loss 0.6067\n",
      "step 75400 of 80000: train_loss 0.6096, val loss 0.6128\n",
      "step 75600 of 80000: train_loss 0.6066, val loss 0.6264\n",
      "step 75800 of 80000: train_loss 0.6058, val loss 0.6151\n",
      "step 76000 of 80000: train_loss 0.5996, val loss 0.5937\n",
      "step 76200 of 80000: train_loss 0.5938, val loss 0.6002\n",
      "step 76400 of 80000: train_loss 0.6061, val loss 0.6162\n",
      "step 76600 of 80000: train_loss 0.5973, val loss 0.6163\n",
      "step 76800 of 80000: train_loss 0.6207, val loss 0.6079\n",
      "step 77000 of 80000: train_loss 0.6046, val loss 0.6005\n",
      "step 77200 of 80000: train_loss 0.6089, val loss 0.5961\n",
      "step 77400 of 80000: train_loss 0.6060, val loss 0.5926\n",
      "step 77600 of 80000: train_loss 0.6065, val loss 0.6078\n",
      "step 77800 of 80000: train_loss 0.5942, val loss 0.5987\n",
      "step 78000 of 80000: train_loss 0.5941, val loss 0.6065\n",
      "step 78200 of 80000: train_loss 0.6078, val loss 0.6128\n",
      "step 78400 of 80000: train_loss 0.5912, val loss 0.6008\n",
      "step 78600 of 80000: train_loss 0.6085, val loss 0.6005\n",
      "step 78800 of 80000: train_loss 0.6170, val loss 0.6201\n",
      "step 79000 of 80000: train_loss 0.5988, val loss 0.6050\n",
      "step 79200 of 80000: train_loss 0.5888, val loss 0.5917\n",
      "step 79400 of 80000: train_loss 0.5904, val loss 0.5966\n",
      "step 79600 of 80000: train_loss 0.6029, val loss 0.5942\n",
      "step 79800 of 80000: train_loss 0.6012, val loss 0.6067\n"
     ]
    }
   ],
   "source": [
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62f2ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [0.14 0.14 0.14] est ATE: [0.36135162 0.22516801 0.3562973 ] error: [0.22135162 0.08516801 0.2162973 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f20204b4280>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "inf = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model(train_data.to(device)).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c381d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view attention maps\n",
    "maps = []\n",
    "for j in range(n_layers):\n",
    "    heads = model.blocks[j].mha.heads\n",
    "    for i in range(num_heads):\n",
    "        maps.append(heads[i].att_wei.mean(0).cpu().detach().numpy())\n",
    "\n",
    "maps = np.stack(maps).mean(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(maps, cmap='hot', interpolation='nearest')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=1)\n",
    "# Setting the axis tick labels\n",
    "ax.set_xticks(np.arange(len(list(DAGnx.nodes))))\n",
    "ax.set_yticks(np.arange(len(list(DAGnx.nodes))))\n",
    "\n",
    "ax.set_xticklabels(list(DAGnx.nodes))\n",
    "ax.set_yticklabels(list(DAGnx.nodes))\n",
    "\n",
    "# Rotating the tick labels inorder to set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('attention_maps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed695e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est ATE: [0.48067921 0.48479529 0.4348332 ] error: [0.01932079 0.01520471 0.0651668 ]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inf = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X':np.array([1.0,])}\n",
    "effect_var = 'M'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "ATE = np.array([0.5, 0.5, 0.5])\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('est ATE:', ATE_pred, 'error:', eATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed31067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
