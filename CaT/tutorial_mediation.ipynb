{
 "cells": [
  {
   "cell_type": "code",
   "id": "b0949a58-1135-4b21-9068-ee725ae55a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T09:42:20.659601Z",
     "start_time": "2024-09-29T09:42:19.404479Z"
    }
   },
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import CaT\n",
    "from CaT.datasets import reorder_dag, get_full_ordering\n",
    "from utils.inference import CausalInference\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 50000\n",
    "batch_size = 100\n",
    "max_iters = 80000\n",
    "eval_interval = 200\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.3\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 5e-3\n",
    "ff_n_embed = 6\n",
    "num_heads = 2\n",
    "n_layers = 2\n",
    "embed_dim = 5\n",
    "head_size = 6\n",
    "d = 5\n",
    "\n",
    "\n",
    "def generate_data_mediation(N, d=5):\n",
    "    DAGnx = nx.DiGraph()\n",
    "    \n",
    "    Ux = np.random.randn(N,d)\n",
    "    X =  Ux\n",
    "\n",
    "    Um = np.random.randn(N,d)\n",
    "    M =  0.2 * X + Um\n",
    "\n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y =  0.7 * M + 0.1 * Uy\n",
    "\n",
    "    M0 = 0.2 * 0 + Um \n",
    "    M1 = 0.2 * 1 + Um\n",
    "\n",
    "    Y0 = 0.7 * M0 + 0.1 * Uy \n",
    "    Y1 = 0.7 * M1 + 0.1 * Uy   # total effect   = 0.7 * 0.2 = 0.14\n",
    "\n",
    "    all_data_dict = {'X': X, 'M': M, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'M': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'M'), ('M', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "5e1c09c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T09:42:20.904812Z",
     "start_time": "2024-09-29T09:42:20.661021Z"
    }
   },
   "source": [
    "d=3\n",
    "_, _, _, _, _, Y0, Y1 = generate_data_mediation(N=1000000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  # multi-dim ATE based off a large sample\n",
    "all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data_mediation(N=sample_size, d=d)\n",
    "print(var_names, ATE)\n",
    "print(all_data.shape)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'M', 'Y'] [0.14 0.14 0.14]\n",
      "(50000, 3, 3)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "63c1b6ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T09:42:21.556347Z",
     "start_time": "2024-09-29T09:42:20.905832Z"
    }
   },
   "source": [
    "input_dim = all_data.shape[2]\n",
    "\n",
    "model = CaT(input_dim=input_dim,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    head_size=head_size,\n",
    "                    num_heads=num_heads,\n",
    "                    ff_n_embed=ff_n_embed,\n",
    "                    embed_dim= embed_dim,\n",
    "                    dag=DAGnx,\n",
    "                    causal_ordering=causal_ordering,\n",
    "                    n_layers=n_layers,\n",
    "                    device=device,\n",
    "                    var_types=var_types, activation_function='Swish'\n",
    "                    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "d32adc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:01:07.852531Z",
     "start_time": "2024-09-29T09:42:21.557247Z"
    }
   },
   "source": [
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 80000: train_loss 2.4659, val loss 2.4663\n",
      "step 200 of 80000: train_loss 1.0544, val loss 1.0202\n",
      "step 400 of 80000: train_loss 1.0212, val loss 1.0016\n",
      "step 600 of 80000: train_loss 1.0230, val loss 0.9967\n",
      "step 800 of 80000: train_loss 1.0213, val loss 0.9946\n",
      "step 1000 of 80000: train_loss 1.0120, val loss 1.0009\n",
      "step 1200 of 80000: train_loss 1.0336, val loss 0.9891\n",
      "step 1400 of 80000: train_loss 1.0127, val loss 0.9865\n",
      "step 1600 of 80000: train_loss 1.0181, val loss 0.9999\n",
      "step 1800 of 80000: train_loss 1.0162, val loss 0.9922\n",
      "step 2000 of 80000: train_loss 1.0119, val loss 1.0016\n",
      "step 2200 of 80000: train_loss 1.0096, val loss 1.0066\n",
      "step 2400 of 80000: train_loss 1.0165, val loss 0.9978\n",
      "step 2600 of 80000: train_loss 1.0192, val loss 0.9886\n",
      "step 2800 of 80000: train_loss 1.0276, val loss 1.0108\n",
      "step 3000 of 80000: train_loss 1.0181, val loss 0.9956\n",
      "step 3200 of 80000: train_loss 1.0199, val loss 1.0013\n",
      "step 3400 of 80000: train_loss 1.0028, val loss 0.9939\n",
      "step 3600 of 80000: train_loss 1.0109, val loss 1.0065\n",
      "step 3800 of 80000: train_loss 1.0125, val loss 0.9930\n",
      "step 4000 of 80000: train_loss 1.0151, val loss 0.9995\n",
      "step 4200 of 80000: train_loss 1.0154, val loss 0.9905\n",
      "step 4400 of 80000: train_loss 1.0080, val loss 0.9968\n",
      "step 4600 of 80000: train_loss 1.0169, val loss 0.9995\n",
      "step 4800 of 80000: train_loss 1.0074, val loss 0.9912\n",
      "step 5000 of 80000: train_loss 1.0106, val loss 0.9989\n",
      "step 5200 of 80000: train_loss 1.0232, val loss 0.9899\n",
      "step 5400 of 80000: train_loss 1.0111, val loss 0.9939\n",
      "step 5600 of 80000: train_loss 1.0185, val loss 1.0001\n",
      "step 5800 of 80000: train_loss 1.0117, val loss 1.0023\n",
      "step 6000 of 80000: train_loss 1.0231, val loss 1.0109\n",
      "step 6200 of 80000: train_loss 1.0079, val loss 1.0010\n",
      "step 6400 of 80000: train_loss 1.0240, val loss 0.9940\n",
      "step 6600 of 80000: train_loss 1.0221, val loss 1.0040\n",
      "step 6800 of 80000: train_loss 1.0074, val loss 1.0027\n",
      "step 7000 of 80000: train_loss 1.0232, val loss 0.9707\n",
      "step 7200 of 80000: train_loss 0.9997, val loss 1.0150\n",
      "step 7400 of 80000: train_loss 1.0239, val loss 0.9887\n",
      "step 7600 of 80000: train_loss 1.0158, val loss 1.0018\n",
      "step 7800 of 80000: train_loss 1.0166, val loss 1.0031\n",
      "step 8000 of 80000: train_loss 1.0152, val loss 0.9963\n",
      "step 8200 of 80000: train_loss 1.0225, val loss 0.9940\n",
      "step 8400 of 80000: train_loss 1.0104, val loss 0.9946\n",
      "step 8600 of 80000: train_loss 1.0148, val loss 0.9882\n",
      "step 8800 of 80000: train_loss 1.0111, val loss 0.9957\n",
      "step 9000 of 80000: train_loss 1.0253, val loss 1.0034\n",
      "step 9200 of 80000: train_loss 1.0019, val loss 1.0218\n",
      "step 9400 of 80000: train_loss 1.0183, val loss 0.9929\n",
      "step 9600 of 80000: train_loss 1.0230, val loss 1.0009\n",
      "step 9800 of 80000: train_loss 1.0217, val loss 1.0005\n",
      "step 10000 of 80000: train_loss 1.0055, val loss 0.9959\n",
      "step 10200 of 80000: train_loss 1.0176, val loss 0.9849\n",
      "step 10400 of 80000: train_loss 1.0293, val loss 0.9927\n",
      "step 10600 of 80000: train_loss 1.0220, val loss 0.9972\n",
      "step 10800 of 80000: train_loss 1.0199, val loss 0.9970\n",
      "step 11000 of 80000: train_loss 1.0023, val loss 0.9956\n",
      "step 11200 of 80000: train_loss 1.0122, val loss 1.0015\n",
      "step 11400 of 80000: train_loss 1.0255, val loss 0.9911\n",
      "step 11600 of 80000: train_loss 1.0147, val loss 0.9832\n",
      "step 11800 of 80000: train_loss 1.0184, val loss 0.9904\n",
      "step 12000 of 80000: train_loss 1.0184, val loss 0.9939\n",
      "step 12200 of 80000: train_loss 1.0229, val loss 1.0012\n",
      "step 12400 of 80000: train_loss 1.0133, val loss 0.9873\n",
      "step 12600 of 80000: train_loss 1.0085, val loss 1.0025\n",
      "step 12800 of 80000: train_loss 1.0024, val loss 1.0002\n",
      "step 13000 of 80000: train_loss 1.0122, val loss 0.9919\n",
      "step 13200 of 80000: train_loss 0.9965, val loss 1.0024\n",
      "step 13400 of 80000: train_loss 1.0168, val loss 0.9884\n",
      "step 13600 of 80000: train_loss 1.0042, val loss 0.9938\n",
      "step 13800 of 80000: train_loss 1.0109, val loss 0.9896\n",
      "step 14000 of 80000: train_loss 1.0193, val loss 0.9758\n",
      "step 14200 of 80000: train_loss 1.0235, val loss 0.9961\n",
      "step 14400 of 80000: train_loss 1.0003, val loss 0.9924\n",
      "step 14600 of 80000: train_loss 1.0220, val loss 1.0043\n",
      "step 14800 of 80000: train_loss 1.0115, val loss 1.0061\n",
      "step 15000 of 80000: train_loss 1.0170, val loss 0.9930\n",
      "step 15200 of 80000: train_loss 1.0183, val loss 0.9852\n",
      "step 15400 of 80000: train_loss 0.9952, val loss 0.9939\n",
      "step 15600 of 80000: train_loss 1.0147, val loss 1.0035\n",
      "step 15800 of 80000: train_loss 1.0127, val loss 1.0072\n",
      "step 16000 of 80000: train_loss 1.0230, val loss 1.0013\n",
      "step 16200 of 80000: train_loss 1.0083, val loss 0.9956\n",
      "step 16400 of 80000: train_loss 1.0129, val loss 0.9881\n",
      "step 16600 of 80000: train_loss 1.0193, val loss 0.9934\n",
      "step 16800 of 80000: train_loss 1.0059, val loss 0.9989\n",
      "step 17000 of 80000: train_loss 1.0140, val loss 1.0019\n",
      "step 17200 of 80000: train_loss 1.0061, val loss 0.9972\n",
      "step 17400 of 80000: train_loss 1.0009, val loss 0.9797\n",
      "step 17600 of 80000: train_loss 1.0148, val loss 0.9832\n",
      "step 17800 of 80000: train_loss 1.0145, val loss 0.9867\n",
      "step 18000 of 80000: train_loss 1.0144, val loss 0.9954\n",
      "step 18200 of 80000: train_loss 1.0187, val loss 0.9990\n",
      "step 18400 of 80000: train_loss 1.0190, val loss 0.9847\n",
      "step 18600 of 80000: train_loss 1.0119, val loss 0.9887\n",
      "step 18800 of 80000: train_loss 1.0268, val loss 0.9859\n",
      "step 19000 of 80000: train_loss 1.0297, val loss 0.9974\n",
      "step 19200 of 80000: train_loss 1.0146, val loss 0.9963\n",
      "step 19400 of 80000: train_loss 1.0162, val loss 1.0126\n",
      "step 19600 of 80000: train_loss 1.0160, val loss 0.9957\n",
      "step 19800 of 80000: train_loss 1.0117, val loss 0.9990\n",
      "step 20000 of 80000: train_loss 0.9981, val loss 1.0027\n",
      "step 20200 of 80000: train_loss 1.0021, val loss 0.9933\n",
      "step 20400 of 80000: train_loss 1.0136, val loss 0.9974\n",
      "step 20600 of 80000: train_loss 1.0160, val loss 0.9902\n",
      "step 20800 of 80000: train_loss 1.0279, val loss 0.9958\n",
      "step 21000 of 80000: train_loss 1.0080, val loss 0.9738\n",
      "step 21200 of 80000: train_loss 1.0173, val loss 0.9896\n",
      "step 21400 of 80000: train_loss 1.0102, val loss 1.0013\n",
      "step 21600 of 80000: train_loss 1.0237, val loss 0.9838\n",
      "step 21800 of 80000: train_loss 1.0186, val loss 0.9807\n",
      "step 22000 of 80000: train_loss 1.0192, val loss 1.0024\n",
      "step 22200 of 80000: train_loss 1.0224, val loss 1.0026\n",
      "step 22400 of 80000: train_loss 1.0100, val loss 0.9987\n",
      "step 22600 of 80000: train_loss 1.0061, val loss 0.9847\n",
      "step 22800 of 80000: train_loss 1.0215, val loss 0.9900\n",
      "step 23000 of 80000: train_loss 1.0102, val loss 0.9937\n",
      "step 23200 of 80000: train_loss 1.0143, val loss 0.9946\n",
      "step 23400 of 80000: train_loss 1.0178, val loss 0.9818\n",
      "step 23600 of 80000: train_loss 1.0159, val loss 0.9965\n",
      "step 23800 of 80000: train_loss 1.0080, val loss 0.9876\n",
      "step 24000 of 80000: train_loss 1.0158, val loss 0.9967\n",
      "step 24200 of 80000: train_loss 1.0173, val loss 0.9963\n",
      "step 24400 of 80000: train_loss 1.0222, val loss 0.9990\n",
      "step 24600 of 80000: train_loss 1.0113, val loss 0.9907\n",
      "step 24800 of 80000: train_loss 1.0163, val loss 0.9952\n",
      "step 25000 of 80000: train_loss 1.0113, val loss 1.0055\n",
      "step 25200 of 80000: train_loss 1.0046, val loss 0.9850\n",
      "step 25400 of 80000: train_loss 1.0134, val loss 0.9866\n",
      "step 25600 of 80000: train_loss 1.0104, val loss 0.9981\n",
      "step 25800 of 80000: train_loss 1.0086, val loss 0.9919\n",
      "step 26000 of 80000: train_loss 1.0053, val loss 0.9822\n",
      "step 26200 of 80000: train_loss 1.0036, val loss 1.0032\n",
      "step 26400 of 80000: train_loss 1.0225, val loss 1.0010\n",
      "step 26600 of 80000: train_loss 1.0114, val loss 1.0049\n",
      "step 26800 of 80000: train_loss 1.0243, val loss 0.9980\n",
      "step 27000 of 80000: train_loss 1.0206, val loss 1.0067\n",
      "step 27200 of 80000: train_loss 1.0033, val loss 0.9762\n",
      "step 27400 of 80000: train_loss 1.0162, val loss 0.9988\n",
      "step 27600 of 80000: train_loss 1.0226, val loss 0.9955\n",
      "step 27800 of 80000: train_loss 1.0237, val loss 1.0043\n",
      "step 28000 of 80000: train_loss 1.0111, val loss 0.9893\n",
      "step 28200 of 80000: train_loss 1.0039, val loss 1.0172\n",
      "step 28400 of 80000: train_loss 1.0127, val loss 0.9930\n",
      "step 28600 of 80000: train_loss 1.0024, val loss 0.9914\n",
      "step 28800 of 80000: train_loss 1.0037, val loss 0.9821\n",
      "step 29000 of 80000: train_loss 1.0119, val loss 0.9995\n",
      "step 29200 of 80000: train_loss 1.0051, val loss 0.9985\n",
      "step 29400 of 80000: train_loss 1.0157, val loss 0.9893\n",
      "step 29600 of 80000: train_loss 1.0010, val loss 0.9811\n",
      "step 29800 of 80000: train_loss 1.0136, val loss 1.0011\n",
      "step 30000 of 80000: train_loss 1.0063, val loss 0.9928\n",
      "step 30200 of 80000: train_loss 1.0238, val loss 0.9955\n",
      "step 30400 of 80000: train_loss 1.0050, val loss 0.9955\n",
      "step 30600 of 80000: train_loss 1.0264, val loss 0.9921\n",
      "step 30800 of 80000: train_loss 1.0107, val loss 0.9940\n",
      "step 31000 of 80000: train_loss 1.0139, val loss 1.0078\n",
      "step 31200 of 80000: train_loss 1.0144, val loss 1.0118\n",
      "step 31400 of 80000: train_loss 1.0308, val loss 0.9937\n",
      "step 31600 of 80000: train_loss 1.0073, val loss 0.9978\n",
      "step 31800 of 80000: train_loss 1.0170, val loss 1.0032\n",
      "step 32000 of 80000: train_loss 1.0100, val loss 0.9992\n",
      "step 32200 of 80000: train_loss 1.0047, val loss 0.9991\n",
      "step 32400 of 80000: train_loss 1.0176, val loss 1.0007\n",
      "step 32600 of 80000: train_loss 1.0228, val loss 0.9773\n",
      "step 32800 of 80000: train_loss 1.0095, val loss 1.0023\n",
      "step 33000 of 80000: train_loss 1.0112, val loss 0.9927\n",
      "step 33200 of 80000: train_loss 1.0008, val loss 0.9849\n",
      "step 33400 of 80000: train_loss 1.0116, val loss 0.9829\n",
      "step 33600 of 80000: train_loss 1.0209, val loss 0.9908\n",
      "step 33800 of 80000: train_loss 1.0143, val loss 0.9963\n",
      "step 34000 of 80000: train_loss 1.0157, val loss 0.9865\n",
      "step 34200 of 80000: train_loss 1.0128, val loss 1.0066\n",
      "step 34400 of 80000: train_loss 1.0110, val loss 0.9853\n",
      "step 34600 of 80000: train_loss 1.0074, val loss 0.9948\n",
      "step 34800 of 80000: train_loss 1.0161, val loss 1.0008\n",
      "step 35000 of 80000: train_loss 1.0036, val loss 0.9781\n",
      "step 35200 of 80000: train_loss 1.0218, val loss 0.9913\n",
      "step 35400 of 80000: train_loss 1.0074, val loss 1.0034\n",
      "step 35600 of 80000: train_loss 1.0207, val loss 0.9968\n",
      "step 35800 of 80000: train_loss 1.0110, val loss 0.9717\n",
      "step 36000 of 80000: train_loss 1.0147, val loss 0.9883\n",
      "step 36200 of 80000: train_loss 1.0186, val loss 0.9967\n",
      "step 36400 of 80000: train_loss 1.0040, val loss 0.9995\n",
      "step 36600 of 80000: train_loss 1.0205, val loss 0.9880\n",
      "step 36800 of 80000: train_loss 1.0033, val loss 1.0067\n",
      "step 37000 of 80000: train_loss 1.0146, val loss 1.0017\n",
      "step 37200 of 80000: train_loss 1.0077, val loss 0.9998\n",
      "step 37400 of 80000: train_loss 1.0143, val loss 0.9995\n",
      "step 37600 of 80000: train_loss 1.0000, val loss 1.0049\n",
      "step 37800 of 80000: train_loss 1.0080, val loss 0.9825\n",
      "step 38000 of 80000: train_loss 1.0222, val loss 0.9975\n",
      "step 38200 of 80000: train_loss 1.0184, val loss 0.9849\n",
      "step 38400 of 80000: train_loss 1.0219, val loss 0.9870\n",
      "step 38600 of 80000: train_loss 1.0082, val loss 0.9919\n",
      "step 38800 of 80000: train_loss 1.0177, val loss 0.9903\n",
      "step 39000 of 80000: train_loss 1.0122, val loss 0.9804\n",
      "step 39200 of 80000: train_loss 1.0270, val loss 0.9902\n",
      "step 39400 of 80000: train_loss 1.0154, val loss 0.9823\n",
      "step 39600 of 80000: train_loss 1.0045, val loss 0.9938\n",
      "step 39800 of 80000: train_loss 1.0215, val loss 0.9956\n",
      "step 40000 of 80000: train_loss 1.0117, val loss 0.9982\n",
      "step 40200 of 80000: train_loss 1.0037, val loss 0.9968\n",
      "step 40400 of 80000: train_loss 1.0123, val loss 0.9965\n",
      "step 40600 of 80000: train_loss 1.0183, val loss 0.9949\n",
      "step 40800 of 80000: train_loss 1.0265, val loss 0.9980\n",
      "step 41000 of 80000: train_loss 1.0160, val loss 0.9908\n",
      "step 41200 of 80000: train_loss 1.0045, val loss 0.9968\n",
      "step 41400 of 80000: train_loss 1.0111, val loss 1.0005\n",
      "step 41600 of 80000: train_loss 1.0169, val loss 0.9949\n",
      "step 41800 of 80000: train_loss 1.0054, val loss 0.9823\n",
      "step 42000 of 80000: train_loss 1.0243, val loss 0.9748\n",
      "step 42200 of 80000: train_loss 1.0177, val loss 0.9937\n",
      "step 42400 of 80000: train_loss 1.0180, val loss 1.0067\n",
      "step 42600 of 80000: train_loss 1.0036, val loss 1.0000\n",
      "step 42800 of 80000: train_loss 1.0207, val loss 0.9797\n",
      "step 43000 of 80000: train_loss 1.0013, val loss 0.9849\n",
      "step 43200 of 80000: train_loss 1.0048, val loss 0.9927\n",
      "step 43400 of 80000: train_loss 1.0253, val loss 0.9919\n",
      "step 43600 of 80000: train_loss 1.0179, val loss 0.9826\n",
      "step 43800 of 80000: train_loss 1.0115, val loss 1.0033\n",
      "step 44000 of 80000: train_loss 1.0081, val loss 0.9849\n",
      "step 44200 of 80000: train_loss 1.0011, val loss 0.9819\n",
      "step 44400 of 80000: train_loss 1.0124, val loss 0.9887\n",
      "step 44600 of 80000: train_loss 1.0029, val loss 0.9875\n",
      "step 44800 of 80000: train_loss 1.0146, val loss 1.0013\n",
      "step 45000 of 80000: train_loss 1.0121, val loss 0.9943\n",
      "step 45200 of 80000: train_loss 1.0215, val loss 0.9968\n",
      "step 45400 of 80000: train_loss 1.0047, val loss 0.9915\n",
      "step 45600 of 80000: train_loss 0.9952, val loss 0.9870\n",
      "step 45800 of 80000: train_loss 1.0122, val loss 0.9839\n",
      "step 46000 of 80000: train_loss 1.0113, val loss 1.0071\n",
      "step 46200 of 80000: train_loss 1.0170, val loss 0.9871\n",
      "step 46400 of 80000: train_loss 1.0189, val loss 0.9890\n",
      "step 46600 of 80000: train_loss 1.0176, val loss 1.0032\n",
      "step 46800 of 80000: train_loss 1.0211, val loss 0.9882\n",
      "step 47000 of 80000: train_loss 1.0071, val loss 1.0001\n",
      "step 47200 of 80000: train_loss 1.0109, val loss 0.9839\n",
      "step 47400 of 80000: train_loss 1.0247, val loss 0.9993\n",
      "step 47600 of 80000: train_loss 0.9938, val loss 0.9920\n",
      "step 47800 of 80000: train_loss 1.0107, val loss 0.9831\n",
      "step 48000 of 80000: train_loss 1.0121, val loss 1.0019\n",
      "step 48200 of 80000: train_loss 1.0171, val loss 0.9996\n",
      "step 48400 of 80000: train_loss 1.0294, val loss 0.9985\n",
      "step 48600 of 80000: train_loss 1.0070, val loss 1.0060\n",
      "step 48800 of 80000: train_loss 1.0158, val loss 1.0033\n",
      "step 49000 of 80000: train_loss 1.0131, val loss 1.0036\n",
      "step 49200 of 80000: train_loss 1.0375, val loss 0.9844\n",
      "step 49400 of 80000: train_loss 1.0216, val loss 0.9953\n",
      "step 49600 of 80000: train_loss 1.0029, val loss 1.0007\n",
      "step 49800 of 80000: train_loss 1.0148, val loss 0.9943\n",
      "step 50000 of 80000: train_loss 1.0221, val loss 0.9972\n",
      "step 50200 of 80000: train_loss 1.0106, val loss 0.9952\n",
      "step 50400 of 80000: train_loss 1.0194, val loss 0.9943\n",
      "step 50600 of 80000: train_loss 1.0117, val loss 0.9963\n",
      "step 50800 of 80000: train_loss 1.0122, val loss 0.9878\n",
      "step 51000 of 80000: train_loss 1.0167, val loss 0.9958\n",
      "step 51200 of 80000: train_loss 1.0187, val loss 0.9884\n",
      "step 51400 of 80000: train_loss 1.0275, val loss 1.0041\n",
      "step 51600 of 80000: train_loss 1.0091, val loss 0.9973\n",
      "step 51800 of 80000: train_loss 1.0092, val loss 0.9914\n",
      "step 52000 of 80000: train_loss 1.0058, val loss 1.0087\n",
      "step 52200 of 80000: train_loss 1.0085, val loss 0.9914\n",
      "step 52400 of 80000: train_loss 1.0143, val loss 0.9948\n",
      "step 52600 of 80000: train_loss 1.0060, val loss 0.9901\n",
      "step 52800 of 80000: train_loss 1.0094, val loss 0.9702\n",
      "step 53000 of 80000: train_loss 1.0124, val loss 0.9905\n",
      "step 53200 of 80000: train_loss 1.0158, val loss 1.0042\n",
      "step 53400 of 80000: train_loss 1.0103, val loss 0.9885\n",
      "step 53600 of 80000: train_loss 1.0187, val loss 0.9923\n",
      "step 53800 of 80000: train_loss 1.0211, val loss 1.0029\n",
      "step 54000 of 80000: train_loss 1.0169, val loss 0.9923\n",
      "step 54200 of 80000: train_loss 1.0002, val loss 0.9981\n",
      "step 54400 of 80000: train_loss 0.9942, val loss 0.9980\n",
      "step 54600 of 80000: train_loss 1.0002, val loss 0.9895\n",
      "step 54800 of 80000: train_loss 1.0079, val loss 0.9940\n",
      "step 55000 of 80000: train_loss 1.0253, val loss 0.9931\n",
      "step 55200 of 80000: train_loss 1.0103, val loss 1.0052\n",
      "step 55400 of 80000: train_loss 1.0093, val loss 1.0037\n",
      "step 55600 of 80000: train_loss 1.0092, val loss 0.9987\n",
      "step 55800 of 80000: train_loss 1.0178, val loss 0.9961\n",
      "step 56000 of 80000: train_loss 1.0161, val loss 1.0044\n",
      "step 56200 of 80000: train_loss 0.9934, val loss 0.9994\n",
      "step 56400 of 80000: train_loss 1.0175, val loss 0.9964\n",
      "step 56600 of 80000: train_loss 1.0017, val loss 0.9882\n",
      "step 56800 of 80000: train_loss 1.0120, val loss 0.9942\n",
      "step 57000 of 80000: train_loss 1.0156, val loss 0.9834\n",
      "step 57200 of 80000: train_loss 1.0088, val loss 0.9918\n",
      "step 57400 of 80000: train_loss 1.0075, val loss 0.9928\n",
      "step 57600 of 80000: train_loss 1.0243, val loss 1.0012\n",
      "step 57800 of 80000: train_loss 1.0102, val loss 0.9955\n",
      "step 58000 of 80000: train_loss 1.0161, val loss 0.9858\n",
      "step 58200 of 80000: train_loss 1.0109, val loss 0.9907\n",
      "step 58400 of 80000: train_loss 1.0325, val loss 1.0065\n",
      "step 58600 of 80000: train_loss 1.0143, val loss 0.9991\n",
      "step 58800 of 80000: train_loss 1.0139, val loss 0.9994\n",
      "step 59000 of 80000: train_loss 1.0148, val loss 1.0076\n",
      "step 59200 of 80000: train_loss 1.0144, val loss 0.9911\n",
      "step 59400 of 80000: train_loss 1.0094, val loss 0.9822\n",
      "step 59600 of 80000: train_loss 1.0233, val loss 1.0037\n",
      "step 59800 of 80000: train_loss 1.0200, val loss 0.9849\n",
      "step 60000 of 80000: train_loss 1.0063, val loss 1.0013\n",
      "step 60200 of 80000: train_loss 1.0112, val loss 0.9951\n",
      "step 60400 of 80000: train_loss 1.0068, val loss 0.9849\n",
      "step 60600 of 80000: train_loss 1.0058, val loss 1.0102\n",
      "step 60800 of 80000: train_loss 1.0123, val loss 0.9978\n",
      "step 61000 of 80000: train_loss 1.0101, val loss 0.9994\n",
      "step 61200 of 80000: train_loss 1.0252, val loss 0.9909\n",
      "step 61400 of 80000: train_loss 1.0112, val loss 0.9774\n",
      "step 61600 of 80000: train_loss 1.0026, val loss 0.9919\n",
      "step 61800 of 80000: train_loss 1.0169, val loss 0.9936\n",
      "step 62000 of 80000: train_loss 1.0122, val loss 0.9796\n",
      "step 62200 of 80000: train_loss 1.0154, val loss 1.0027\n",
      "step 62400 of 80000: train_loss 1.0117, val loss 1.0025\n",
      "step 62600 of 80000: train_loss 0.9949, val loss 1.0031\n",
      "step 62800 of 80000: train_loss 1.0020, val loss 0.9889\n",
      "step 63000 of 80000: train_loss 1.0080, val loss 0.9955\n",
      "step 63200 of 80000: train_loss 1.0174, val loss 0.9993\n",
      "step 63400 of 80000: train_loss 1.0109, val loss 0.9892\n",
      "step 63600 of 80000: train_loss 1.0089, val loss 0.9981\n",
      "step 63800 of 80000: train_loss 1.0097, val loss 0.9947\n",
      "step 64000 of 80000: train_loss 1.0299, val loss 1.0010\n",
      "step 64200 of 80000: train_loss 1.0224, val loss 0.9963\n",
      "step 64400 of 80000: train_loss 1.0138, val loss 0.9789\n",
      "step 64600 of 80000: train_loss 1.0105, val loss 0.9924\n",
      "step 64800 of 80000: train_loss 1.0131, val loss 0.9827\n",
      "step 65000 of 80000: train_loss 1.0218, val loss 0.9938\n",
      "step 65200 of 80000: train_loss 1.0170, val loss 0.9953\n",
      "step 65400 of 80000: train_loss 1.0071, val loss 0.9930\n",
      "step 65600 of 80000: train_loss 1.0230, val loss 1.0041\n",
      "step 65800 of 80000: train_loss 1.0162, val loss 1.0022\n",
      "step 66000 of 80000: train_loss 1.0299, val loss 1.0005\n",
      "step 66200 of 80000: train_loss 1.0079, val loss 0.9983\n",
      "step 66400 of 80000: train_loss 1.0181, val loss 0.9987\n",
      "step 66600 of 80000: train_loss 1.0127, val loss 0.9920\n",
      "step 66800 of 80000: train_loss 1.0124, val loss 1.0013\n",
      "step 67000 of 80000: train_loss 1.0101, val loss 1.0023\n",
      "step 67200 of 80000: train_loss 1.0139, val loss 0.9871\n",
      "step 67400 of 80000: train_loss 1.0095, val loss 1.0069\n",
      "step 67600 of 80000: train_loss 0.9983, val loss 0.9898\n",
      "step 67800 of 80000: train_loss 1.0122, val loss 1.0003\n",
      "step 68000 of 80000: train_loss 1.0226, val loss 0.9946\n",
      "step 68200 of 80000: train_loss 1.0129, val loss 1.0011\n",
      "step 68400 of 80000: train_loss 1.0180, val loss 0.9918\n",
      "step 68600 of 80000: train_loss 1.0126, val loss 0.9943\n",
      "step 68800 of 80000: train_loss 1.0132, val loss 0.9884\n",
      "step 69000 of 80000: train_loss 1.0330, val loss 0.9931\n",
      "step 69200 of 80000: train_loss 1.0273, val loss 0.9972\n",
      "step 69400 of 80000: train_loss 1.0247, val loss 0.9940\n",
      "step 69600 of 80000: train_loss 1.0100, val loss 0.9872\n",
      "step 69800 of 80000: train_loss 0.9983, val loss 0.9788\n",
      "step 70000 of 80000: train_loss 1.0156, val loss 0.9915\n",
      "step 70200 of 80000: train_loss 1.0120, val loss 0.9928\n",
      "step 70400 of 80000: train_loss 0.9992, val loss 0.9882\n",
      "step 70600 of 80000: train_loss 1.0213, val loss 1.0019\n",
      "step 70800 of 80000: train_loss 1.0092, val loss 0.9958\n",
      "step 71000 of 80000: train_loss 1.0210, val loss 0.9884\n",
      "step 71200 of 80000: train_loss 1.0063, val loss 0.9974\n",
      "step 71400 of 80000: train_loss 1.0246, val loss 0.9774\n",
      "step 71600 of 80000: train_loss 1.0093, val loss 0.9985\n",
      "step 71800 of 80000: train_loss 1.0122, val loss 0.9997\n",
      "step 72000 of 80000: train_loss 1.0102, val loss 0.9964\n",
      "step 72200 of 80000: train_loss 1.0009, val loss 1.0045\n",
      "step 72400 of 80000: train_loss 1.0091, val loss 1.0157\n",
      "step 72600 of 80000: train_loss 1.0120, val loss 0.9925\n",
      "step 72800 of 80000: train_loss 1.0114, val loss 0.9961\n",
      "step 73000 of 80000: train_loss 1.0143, val loss 0.9937\n",
      "step 73200 of 80000: train_loss 1.0064, val loss 0.9905\n",
      "step 73400 of 80000: train_loss 1.0110, val loss 0.9963\n",
      "step 73600 of 80000: train_loss 1.0115, val loss 0.9988\n",
      "step 73800 of 80000: train_loss 1.0047, val loss 0.9945\n",
      "step 74000 of 80000: train_loss 1.0297, val loss 0.9825\n",
      "step 74200 of 80000: train_loss 1.0134, val loss 0.9906\n",
      "step 74400 of 80000: train_loss 1.0225, val loss 0.9957\n",
      "step 74600 of 80000: train_loss 1.0167, val loss 1.0030\n",
      "step 74800 of 80000: train_loss 1.0189, val loss 0.9883\n",
      "step 75000 of 80000: train_loss 1.0172, val loss 0.9846\n",
      "step 75200 of 80000: train_loss 1.0013, val loss 0.9877\n",
      "step 75400 of 80000: train_loss 1.0112, val loss 0.9943\n",
      "step 75600 of 80000: train_loss 0.9999, val loss 0.9957\n",
      "step 75800 of 80000: train_loss 1.0079, val loss 0.9976\n",
      "step 76000 of 80000: train_loss 1.0024, val loss 0.9814\n",
      "step 76200 of 80000: train_loss 1.0020, val loss 0.9919\n",
      "step 76400 of 80000: train_loss 1.0091, val loss 0.9952\n",
      "step 76600 of 80000: train_loss 1.0200, val loss 1.0011\n",
      "step 76800 of 80000: train_loss 1.0198, val loss 0.9945\n",
      "step 77000 of 80000: train_loss 1.0114, val loss 0.9893\n",
      "step 77200 of 80000: train_loss 1.0214, val loss 0.9924\n",
      "step 77400 of 80000: train_loss 1.0174, val loss 0.9773\n",
      "step 77600 of 80000: train_loss 1.0139, val loss 0.9985\n",
      "step 77800 of 80000: train_loss 1.0063, val loss 0.9921\n",
      "step 78000 of 80000: train_loss 1.0113, val loss 0.9865\n",
      "step 78200 of 80000: train_loss 1.0133, val loss 1.0091\n",
      "step 78400 of 80000: train_loss 1.0116, val loss 0.9893\n",
      "step 78600 of 80000: train_loss 1.0125, val loss 0.9882\n",
      "step 78800 of 80000: train_loss 1.0188, val loss 0.9788\n",
      "step 79000 of 80000: train_loss 1.0115, val loss 0.9951\n",
      "step 79200 of 80000: train_loss 1.0256, val loss 0.9873\n",
      "step 79400 of 80000: train_loss 1.0049, val loss 0.9935\n",
      "step 79600 of 80000: train_loss 1.0141, val loss 0.9879\n",
      "step 79800 of 80000: train_loss 1.0059, val loss 0.9845\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "62f2ec5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:01:08.151200Z",
     "start_time": "2024-09-29T10:01:07.854040Z"
    }
   },
   "source": [
    "model.eval()\n",
    "inf = CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model(train_data.to(device)).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [0.14 0.14 0.14] est ATE: [0.12974264 0.13025396 0.13706632] error: [0.01025736 0.00974604 0.00293368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthewvowels/GitHub/Causal_Transformer/utils/inference.py:53: UserWarning: No mask has been specified. If padding has been used, the absence of a mask may lead to incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5fb8a72370>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c381d7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:01:08.417923Z",
     "start_time": "2024-09-29T10:01:08.152112Z"
    }
   },
   "source": [
    "# view attention maps\n",
    "maps = []\n",
    "for j in range(n_layers):\n",
    "    heads = model.blocks[j].mha.heads\n",
    "    for i in range(num_heads):\n",
    "        maps.append(heads[i].att_wei.mean(0).cpu().detach().numpy())\n",
    "\n",
    "maps = np.stack(maps).mean(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(maps, cmap='hot', interpolation='nearest')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=1)\n",
    "# Setting the axis tick labels\n",
    "ax.set_xticks(np.arange(len(list(DAGnx.nodes))))\n",
    "ax.set_yticks(np.arange(len(list(DAGnx.nodes))))\n",
    "\n",
    "ax.set_xticklabels(list(DAGnx.nodes))\n",
    "ax.set_yticklabels(list(DAGnx.nodes))\n",
    "\n",
    "# Rotating the tick labels inorder to set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('attention_maps.png')"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Head' object has no attribute 'att_wei'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m     heads \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mblocks[j]\u001B[38;5;241m.\u001B[39mmha\u001B[38;5;241m.\u001B[39mheads\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_heads):\n\u001B[0;32m----> 6\u001B[0m         maps\u001B[38;5;241m.\u001B[39mappend(\u001B[43mheads\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43matt_wei\u001B[49m\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m      8\u001B[0m maps \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(maps)\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     10\u001B[0m fig, ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots()\n",
      "File \u001B[0;32m~/anaconda3/envs/cat_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1709\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1708\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1709\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Head' object has no attribute 'att_wei'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ed695e3f",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "inf = CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X':np.array([1.0,])}\n",
    "effect_var = 'M'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "ATE = np.array([0.5, 0.5, 0.5])\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('est ATE:', ATE_pred, 'error:', eATE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ed31067",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
