{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6504d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import CaT\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_full_ordering, reorder_dag\n",
    "\n",
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 10000\n",
    "batch_size = 100\n",
    "max_iters = 100000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.3\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 5e-3\n",
    "ff_n_embed = 6\n",
    "num_heads = 1\n",
    "n_layers = 1\n",
    "head_size = 6\n",
    "d = 5\n",
    "\n",
    "def generate_data(N, d=5):\n",
    "    DAGnx = nx.DiGraph()\n",
    "    \n",
    "    Ux = np.random.randn(N,d)\n",
    "    X =  Ux\n",
    "    \n",
    "    Ub = np.random.randn(N,d)\n",
    "    B =  Ub\n",
    "    \n",
    "    Uc = np.random.randn(N,d)\n",
    "    C =  Uc\n",
    "    \n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y = X + B + C + 0.1 * Uy\n",
    "\n",
    "    Y0 = B + C\n",
    "    Y1 = 1 + B + C\n",
    "\n",
    "    all_data_dict = {'X': X, 'B': B, 'C': C, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'B': 'cont', 'C': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'Y'), ('B', 'Y'), ('C', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1\n",
    "\n",
    "\n",
    "def generate_data_mediation(N, d=5):\n",
    "    DAGnx = nx.DiGraph()\n",
    "    \n",
    "    Ux = np.random.randn(N,d)\n",
    "    X =  Ux\n",
    "\n",
    "    Um = np.random.randn(N,d)\n",
    "    M =  0.5 * X + Um\n",
    "\n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y =  0.5 * M + 0.1 * Uy\n",
    "\n",
    "    M0 = 0.5 * 0 + Um \n",
    "    M1 = 0.5 * 1 + Um\n",
    "\n",
    "    Y0 = 0.5 * M0 + 0.1 * Uy \n",
    "    Y1 = 0.5 * M1 + 0.1 * Uy \n",
    "\n",
    "    all_data_dict = {'X': X, 'M': M, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'M': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'M'), ('M', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aaf293",
   "metadata": {},
   "source": [
    "## Exogenous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ae1124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'B', 'C', 'Y'] [1. 1. 1.]\n",
      "(10000, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "d=3\n",
    "_, _, _, _, _, Y0, Y1 = generate_data(N=1000000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  # multi-dim ATE based off a large sample\n",
    "all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data(N=sample_size, d=d)\n",
    "print(var_names, ATE)\n",
    "print(all_data.shape)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8056409",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = all_data.shape[2]\n",
    "\n",
    "model = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx,\n",
    "                causal_ordering=causal_ordering,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types_sorted=var_types,\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1982159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 100000: train_loss 3.4988, val loss 3.4819\n",
      "step 100 of 100000: train_loss 1.7980, val loss 1.7560\n",
      "step 200 of 100000: train_loss 1.0250, val loss 0.9920\n",
      "step 300 of 100000: train_loss 1.0055, val loss 0.9417\n",
      "step 400 of 100000: train_loss 0.2804, val loss 0.2744\n",
      "step 500 of 100000: train_loss 0.2135, val loss 0.2073\n",
      "step 600 of 100000: train_loss 0.1361, val loss 0.1427\n",
      "step 700 of 100000: train_loss 0.0683, val loss 0.0700\n",
      "step 800 of 100000: train_loss 0.0367, val loss 0.0351\n",
      "step 900 of 100000: train_loss 0.0283, val loss 0.0292\n",
      "step 1000 of 100000: train_loss 0.0258, val loss 0.0247\n",
      "step 1100 of 100000: train_loss 0.0232, val loss 0.0238\n",
      "step 1200 of 100000: train_loss 0.0227, val loss 0.0222\n",
      "step 1300 of 100000: train_loss 0.0198, val loss 0.0194\n",
      "step 1400 of 100000: train_loss 0.0185, val loss 0.0190\n",
      "step 1500 of 100000: train_loss 0.0175, val loss 0.0175\n",
      "step 1600 of 100000: train_loss 0.0186, val loss 0.0176\n",
      "step 1700 of 100000: train_loss 0.0174, val loss 0.0170\n",
      "step 1800 of 100000: train_loss 0.0170, val loss 0.0165\n",
      "step 1900 of 100000: train_loss 0.0158, val loss 0.0159\n",
      "step 2000 of 100000: train_loss 0.0189, val loss 0.0193\n",
      "step 2100 of 100000: train_loss 0.0152, val loss 0.0155\n",
      "step 2200 of 100000: train_loss 0.0154, val loss 0.0156\n",
      "step 2300 of 100000: train_loss 0.0161, val loss 0.0163\n",
      "step 2400 of 100000: train_loss 0.0144, val loss 0.0142\n",
      "step 2500 of 100000: train_loss 0.0157, val loss 0.0157\n",
      "step 2600 of 100000: train_loss 0.0147, val loss 0.0147\n",
      "step 2700 of 100000: train_loss 0.0144, val loss 0.0145\n",
      "step 2800 of 100000: train_loss 0.0142, val loss 0.0143\n",
      "step 2900 of 100000: train_loss 0.0152, val loss 0.0149\n",
      "step 3000 of 100000: train_loss 0.0150, val loss 0.0156\n",
      "step 3100 of 100000: train_loss 0.0149, val loss 0.0151\n",
      "step 3200 of 100000: train_loss 0.0138, val loss 0.0138\n",
      "step 3300 of 100000: train_loss 0.0135, val loss 0.0137\n",
      "step 3400 of 100000: train_loss 0.0151, val loss 0.0154\n",
      "step 3500 of 100000: train_loss 0.0145, val loss 0.0144\n",
      "step 3600 of 100000: train_loss 0.0140, val loss 0.0145\n",
      "step 3700 of 100000: train_loss 0.0142, val loss 0.0148\n",
      "step 3800 of 100000: train_loss 0.0151, val loss 0.0153\n",
      "step 3900 of 100000: train_loss 0.0144, val loss 0.0148\n",
      "step 4000 of 100000: train_loss 0.0154, val loss 0.0153\n",
      "step 4100 of 100000: train_loss 0.0130, val loss 0.0133\n",
      "step 4200 of 100000: train_loss 0.0129, val loss 0.0131\n",
      "step 4300 of 100000: train_loss 0.0127, val loss 0.0128\n",
      "step 4400 of 100000: train_loss 0.0131, val loss 0.0132\n",
      "step 4500 of 100000: train_loss 0.0148, val loss 0.0150\n",
      "step 4600 of 100000: train_loss 0.0133, val loss 0.0130\n",
      "step 4700 of 100000: train_loss 0.0138, val loss 0.0146\n",
      "step 4800 of 100000: train_loss 0.0140, val loss 0.0141\n",
      "step 4900 of 100000: train_loss 0.0129, val loss 0.0131\n",
      "step 5000 of 100000: train_loss 0.0131, val loss 0.0133\n",
      "step 5100 of 100000: train_loss 0.0142, val loss 0.0146\n",
      "step 5200 of 100000: train_loss 0.0139, val loss 0.0142\n",
      "step 5300 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 5400 of 100000: train_loss 0.0130, val loss 0.0131\n",
      "step 5500 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 5600 of 100000: train_loss 0.0138, val loss 0.0138\n",
      "step 5700 of 100000: train_loss 0.0136, val loss 0.0139\n",
      "step 5800 of 100000: train_loss 0.0140, val loss 0.0142\n",
      "step 5900 of 100000: train_loss 0.0147, val loss 0.0149\n",
      "step 6000 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 6100 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 6200 of 100000: train_loss 0.0130, val loss 0.0132\n",
      "step 6300 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 6400 of 100000: train_loss 0.0122, val loss 0.0124\n",
      "step 6500 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 6600 of 100000: train_loss 0.0129, val loss 0.0132\n",
      "step 6700 of 100000: train_loss 0.0136, val loss 0.0139\n",
      "step 6800 of 100000: train_loss 0.0124, val loss 0.0124\n",
      "step 6900 of 100000: train_loss 0.0132, val loss 0.0136\n",
      "step 7000 of 100000: train_loss 0.0134, val loss 0.0136\n",
      "step 7100 of 100000: train_loss 0.0134, val loss 0.0136\n",
      "step 7200 of 100000: train_loss 0.0130, val loss 0.0134\n",
      "step 7300 of 100000: train_loss 0.0142, val loss 0.0148\n",
      "step 7400 of 100000: train_loss 0.0124, val loss 0.0126\n",
      "step 7500 of 100000: train_loss 0.0129, val loss 0.0126\n",
      "step 7600 of 100000: train_loss 0.0151, val loss 0.0154\n",
      "step 7700 of 100000: train_loss 0.0126, val loss 0.0125\n",
      "step 7800 of 100000: train_loss 0.0135, val loss 0.0135\n",
      "step 7900 of 100000: train_loss 0.0131, val loss 0.0135\n",
      "step 8000 of 100000: train_loss 0.0132, val loss 0.0129\n",
      "step 8100 of 100000: train_loss 0.0145, val loss 0.0145\n",
      "step 8200 of 100000: train_loss 0.0125, val loss 0.0125\n",
      "step 8300 of 100000: train_loss 0.0130, val loss 0.0132\n",
      "step 8400 of 100000: train_loss 0.0136, val loss 0.0140\n",
      "step 8500 of 100000: train_loss 0.0125, val loss 0.0128\n",
      "step 8600 of 100000: train_loss 0.0142, val loss 0.0141\n",
      "step 8700 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 8800 of 100000: train_loss 0.0121, val loss 0.0122\n",
      "step 8900 of 100000: train_loss 0.0124, val loss 0.0133\n",
      "step 9000 of 100000: train_loss 0.0138, val loss 0.0140\n",
      "step 9100 of 100000: train_loss 0.0142, val loss 0.0146\n",
      "step 9200 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 9300 of 100000: train_loss 0.0134, val loss 0.0135\n",
      "step 9400 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 9500 of 100000: train_loss 0.0125, val loss 0.0125\n",
      "step 9600 of 100000: train_loss 0.0131, val loss 0.0133\n",
      "step 9700 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 9800 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 9900 of 100000: train_loss 0.0131, val loss 0.0135\n",
      "step 10000 of 100000: train_loss 0.0129, val loss 0.0137\n",
      "step 10100 of 100000: train_loss 0.0121, val loss 0.0124\n",
      "step 10200 of 100000: train_loss 0.0120, val loss 0.0121\n",
      "step 10300 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 10400 of 100000: train_loss 0.0130, val loss 0.0135\n",
      "step 10500 of 100000: train_loss 0.0137, val loss 0.0138\n",
      "step 10600 of 100000: train_loss 0.0128, val loss 0.0131\n",
      "step 10700 of 100000: train_loss 0.0139, val loss 0.0142\n",
      "step 10800 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 10900 of 100000: train_loss 0.0131, val loss 0.0137\n",
      "step 11000 of 100000: train_loss 0.0130, val loss 0.0136\n",
      "step 11100 of 100000: train_loss 0.0129, val loss 0.0133\n",
      "step 11200 of 100000: train_loss 0.0128, val loss 0.0129\n",
      "step 11300 of 100000: train_loss 0.0157, val loss 0.0154\n",
      "step 11400 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 11500 of 100000: train_loss 0.0133, val loss 0.0136\n",
      "step 11600 of 100000: train_loss 0.0120, val loss 0.0123\n",
      "step 11700 of 100000: train_loss 0.0122, val loss 0.0124\n",
      "step 11800 of 100000: train_loss 0.0127, val loss 0.0132\n",
      "step 11900 of 100000: train_loss 0.0119, val loss 0.0122\n",
      "step 12000 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 12100 of 100000: train_loss 0.0133, val loss 0.0135\n",
      "step 12200 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 12300 of 100000: train_loss 0.0134, val loss 0.0138\n",
      "step 12400 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 12500 of 100000: train_loss 0.0133, val loss 0.0133\n",
      "step 12600 of 100000: train_loss 0.0123, val loss 0.0126\n",
      "step 12700 of 100000: train_loss 0.0118, val loss 0.0124\n",
      "step 12800 of 100000: train_loss 0.0126, val loss 0.0127\n",
      "step 12900 of 100000: train_loss 0.0119, val loss 0.0124\n",
      "step 13000 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 13100 of 100000: train_loss 0.0134, val loss 0.0134\n",
      "step 13200 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 13300 of 100000: train_loss 0.0131, val loss 0.0139\n",
      "step 13400 of 100000: train_loss 0.0133, val loss 0.0136\n",
      "step 13500 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 13600 of 100000: train_loss 0.0130, val loss 0.0132\n",
      "step 13700 of 100000: train_loss 0.0130, val loss 0.0137\n",
      "step 13800 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 13900 of 100000: train_loss 0.0136, val loss 0.0141\n",
      "step 14000 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 14100 of 100000: train_loss 0.0128, val loss 0.0129\n",
      "step 14200 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 14300 of 100000: train_loss 0.0140, val loss 0.0145\n",
      "step 14400 of 100000: train_loss 0.0128, val loss 0.0136\n",
      "step 14500 of 100000: train_loss 0.0129, val loss 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14600 of 100000: train_loss 0.0129, val loss 0.0131\n",
      "step 14700 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 14800 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 14900 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 15000 of 100000: train_loss 0.0126, val loss 0.0128\n",
      "step 15100 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 15200 of 100000: train_loss 0.0119, val loss 0.0121\n",
      "step 15300 of 100000: train_loss 0.0133, val loss 0.0140\n",
      "step 15400 of 100000: train_loss 0.0126, val loss 0.0127\n",
      "step 15500 of 100000: train_loss 0.0139, val loss 0.0145\n",
      "step 15600 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 15700 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 15800 of 100000: train_loss 0.0120, val loss 0.0123\n",
      "step 15900 of 100000: train_loss 0.0130, val loss 0.0133\n",
      "step 16000 of 100000: train_loss 0.0122, val loss 0.0121\n",
      "step 16100 of 100000: train_loss 0.0143, val loss 0.0149\n",
      "step 16200 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 16300 of 100000: train_loss 0.0120, val loss 0.0129\n",
      "step 16400 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 16500 of 100000: train_loss 0.0134, val loss 0.0135\n",
      "step 16600 of 100000: train_loss 0.0125, val loss 0.0128\n",
      "step 16700 of 100000: train_loss 0.0146, val loss 0.0148\n",
      "step 16800 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 16900 of 100000: train_loss 0.0127, val loss 0.0133\n",
      "step 17000 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 17100 of 100000: train_loss 0.0130, val loss 0.0133\n",
      "step 17200 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 17300 of 100000: train_loss 0.0123, val loss 0.0125\n",
      "step 17400 of 100000: train_loss 0.0129, val loss 0.0132\n",
      "step 17500 of 100000: train_loss 0.0132, val loss 0.0136\n",
      "step 17600 of 100000: train_loss 0.0136, val loss 0.0140\n",
      "step 17700 of 100000: train_loss 0.0122, val loss 0.0122\n",
      "step 17800 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 17900 of 100000: train_loss 0.0117, val loss 0.0122\n",
      "step 18000 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 18100 of 100000: train_loss 0.0134, val loss 0.0137\n",
      "step 18200 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 18300 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 18400 of 100000: train_loss 0.0127, val loss 0.0137\n",
      "step 18500 of 100000: train_loss 0.0138, val loss 0.0143\n",
      "step 18600 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 18700 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 18800 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 18900 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 19000 of 100000: train_loss 0.0136, val loss 0.0140\n",
      "step 19100 of 100000: train_loss 0.0136, val loss 0.0144\n",
      "step 19200 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 19300 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 19400 of 100000: train_loss 0.0137, val loss 0.0143\n",
      "step 19500 of 100000: train_loss 0.0146, val loss 0.0152\n",
      "step 19600 of 100000: train_loss 0.0130, val loss 0.0134\n",
      "step 19700 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 19800 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 19900 of 100000: train_loss 0.0129, val loss 0.0130\n",
      "step 20000 of 100000: train_loss 0.0123, val loss 0.0126\n",
      "step 20100 of 100000: train_loss 0.0125, val loss 0.0132\n",
      "step 20200 of 100000: train_loss 0.0118, val loss 0.0123\n",
      "step 20300 of 100000: train_loss 0.0132, val loss 0.0137\n",
      "step 20400 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 20500 of 100000: train_loss 0.0132, val loss 0.0134\n",
      "step 20600 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 20700 of 100000: train_loss 0.0127, val loss 0.0128\n",
      "step 20800 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 20900 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 21000 of 100000: train_loss 0.0131, val loss 0.0133\n",
      "step 21100 of 100000: train_loss 0.0133, val loss 0.0142\n",
      "step 21200 of 100000: train_loss 0.0121, val loss 0.0123\n",
      "step 21300 of 100000: train_loss 0.0125, val loss 0.0133\n",
      "step 21400 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 21500 of 100000: train_loss 0.0130, val loss 0.0133\n",
      "step 21600 of 100000: train_loss 0.0120, val loss 0.0122\n",
      "step 21700 of 100000: train_loss 0.0116, val loss 0.0122\n",
      "step 21800 of 100000: train_loss 0.0133, val loss 0.0139\n",
      "step 21900 of 100000: train_loss 0.0122, val loss 0.0131\n",
      "step 22000 of 100000: train_loss 0.0132, val loss 0.0139\n",
      "step 22100 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 22200 of 100000: train_loss 0.0132, val loss 0.0141\n",
      "step 22300 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 22400 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 22500 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 22600 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 22700 of 100000: train_loss 0.0125, val loss 0.0128\n",
      "step 22800 of 100000: train_loss 0.0118, val loss 0.0125\n",
      "step 22900 of 100000: train_loss 0.0125, val loss 0.0128\n",
      "step 23000 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 23100 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 23200 of 100000: train_loss 0.0127, val loss 0.0133\n",
      "step 23300 of 100000: train_loss 0.0126, val loss 0.0128\n",
      "step 23400 of 100000: train_loss 0.0134, val loss 0.0141\n",
      "step 23500 of 100000: train_loss 0.0114, val loss 0.0118\n",
      "step 23600 of 100000: train_loss 0.0119, val loss 0.0127\n",
      "step 23700 of 100000: train_loss 0.0148, val loss 0.0158\n",
      "step 23800 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 23900 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 24000 of 100000: train_loss 0.0119, val loss 0.0122\n",
      "step 24100 of 100000: train_loss 0.0148, val loss 0.0149\n",
      "step 24200 of 100000: train_loss 0.0132, val loss 0.0140\n",
      "step 24300 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 24400 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 24500 of 100000: train_loss 0.0127, val loss 0.0134\n",
      "step 24600 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 24700 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 24800 of 100000: train_loss 0.0134, val loss 0.0140\n",
      "step 24900 of 100000: train_loss 0.0120, val loss 0.0128\n",
      "step 25000 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 25100 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 25200 of 100000: train_loss 0.0117, val loss 0.0122\n",
      "step 25300 of 100000: train_loss 0.0116, val loss 0.0121\n",
      "step 25400 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 25500 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 25600 of 100000: train_loss 0.0124, val loss 0.0133\n",
      "step 25700 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 25800 of 100000: train_loss 0.0123, val loss 0.0123\n",
      "step 25900 of 100000: train_loss 0.0135, val loss 0.0137\n",
      "step 26000 of 100000: train_loss 0.0119, val loss 0.0119\n",
      "step 26100 of 100000: train_loss 0.0143, val loss 0.0146\n",
      "step 26200 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 26300 of 100000: train_loss 0.0119, val loss 0.0127\n",
      "step 26400 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 26500 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 26600 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 26700 of 100000: train_loss 0.0138, val loss 0.0143\n",
      "step 26800 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 26900 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 27000 of 100000: train_loss 0.0116, val loss 0.0122\n",
      "step 27100 of 100000: train_loss 0.0123, val loss 0.0126\n",
      "step 27200 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 27300 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 27400 of 100000: train_loss 0.0125, val loss 0.0126\n",
      "step 27500 of 100000: train_loss 0.0135, val loss 0.0141\n",
      "step 27600 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 27700 of 100000: train_loss 0.0121, val loss 0.0122\n",
      "step 27800 of 100000: train_loss 0.0126, val loss 0.0125\n",
      "step 27900 of 100000: train_loss 0.0121, val loss 0.0129\n",
      "step 28000 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 28100 of 100000: train_loss 0.0131, val loss 0.0131\n",
      "step 28200 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 28300 of 100000: train_loss 0.0131, val loss 0.0134\n",
      "step 28400 of 100000: train_loss 0.0120, val loss 0.0128\n",
      "step 28500 of 100000: train_loss 0.0137, val loss 0.0142\n",
      "step 28600 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 28700 of 100000: train_loss 0.0118, val loss 0.0124\n",
      "step 28800 of 100000: train_loss 0.0130, val loss 0.0137\n",
      "step 28900 of 100000: train_loss 0.0123, val loss 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29000 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 29100 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 29200 of 100000: train_loss 0.0163, val loss 0.0171\n",
      "step 29300 of 100000: train_loss 0.0122, val loss 0.0123\n",
      "step 29400 of 100000: train_loss 0.0137, val loss 0.0140\n",
      "step 29500 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 29600 of 100000: train_loss 0.0133, val loss 0.0138\n",
      "step 29700 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 29800 of 100000: train_loss 0.0139, val loss 0.0149\n",
      "step 29900 of 100000: train_loss 0.0127, val loss 0.0129\n",
      "step 30000 of 100000: train_loss 0.0123, val loss 0.0131\n",
      "step 30100 of 100000: train_loss 0.0122, val loss 0.0124\n",
      "step 30200 of 100000: train_loss 0.0119, val loss 0.0124\n",
      "step 30300 of 100000: train_loss 0.0141, val loss 0.0144\n",
      "step 30400 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 30500 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 30600 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 30700 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 30800 of 100000: train_loss 0.0133, val loss 0.0138\n",
      "step 30900 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 31000 of 100000: train_loss 0.0149, val loss 0.0155\n",
      "step 31100 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 31200 of 100000: train_loss 0.0140, val loss 0.0152\n",
      "step 31300 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 31400 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 31500 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 31600 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 31700 of 100000: train_loss 0.0123, val loss 0.0125\n",
      "step 31800 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 31900 of 100000: train_loss 0.0127, val loss 0.0129\n",
      "step 32000 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 32100 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 32200 of 100000: train_loss 0.0125, val loss 0.0126\n",
      "step 32300 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 32400 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 32500 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 32600 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 32700 of 100000: train_loss 0.0127, val loss 0.0129\n",
      "step 32800 of 100000: train_loss 0.0127, val loss 0.0131\n",
      "step 32900 of 100000: train_loss 0.0132, val loss 0.0135\n",
      "step 33000 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 33100 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 33200 of 100000: train_loss 0.0136, val loss 0.0140\n",
      "step 33300 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 33400 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 33500 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 33600 of 100000: train_loss 0.0132, val loss 0.0137\n",
      "step 33700 of 100000: train_loss 0.0132, val loss 0.0139\n",
      "step 33800 of 100000: train_loss 0.0138, val loss 0.0143\n",
      "step 33900 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 34000 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 34100 of 100000: train_loss 0.0119, val loss 0.0126\n",
      "step 34200 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 34300 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 34400 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 34500 of 100000: train_loss 0.0141, val loss 0.0143\n",
      "step 34600 of 100000: train_loss 0.0130, val loss 0.0137\n",
      "step 34700 of 100000: train_loss 0.0131, val loss 0.0139\n",
      "step 34800 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 34900 of 100000: train_loss 0.0129, val loss 0.0132\n",
      "step 35000 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 35100 of 100000: train_loss 0.0127, val loss 0.0134\n",
      "step 35200 of 100000: train_loss 0.0134, val loss 0.0138\n",
      "step 35300 of 100000: train_loss 0.0130, val loss 0.0136\n",
      "step 35400 of 100000: train_loss 0.0131, val loss 0.0134\n",
      "step 35500 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 35600 of 100000: train_loss 0.0128, val loss 0.0131\n",
      "step 35700 of 100000: train_loss 0.0117, val loss 0.0123\n",
      "step 35800 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 35900 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 36000 of 100000: train_loss 0.0133, val loss 0.0135\n",
      "step 36100 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 36200 of 100000: train_loss 0.0133, val loss 0.0139\n",
      "step 36300 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 36400 of 100000: train_loss 0.0127, val loss 0.0133\n",
      "step 36500 of 100000: train_loss 0.0113, val loss 0.0122\n",
      "step 36600 of 100000: train_loss 0.0132, val loss 0.0140\n",
      "step 36700 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 36800 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 36900 of 100000: train_loss 0.0132, val loss 0.0139\n",
      "step 37000 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 37100 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 37200 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 37300 of 100000: train_loss 0.0125, val loss 0.0133\n",
      "step 37400 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 37500 of 100000: train_loss 0.0129, val loss 0.0131\n",
      "step 37600 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 37700 of 100000: train_loss 0.0127, val loss 0.0134\n",
      "step 37800 of 100000: train_loss 0.0129, val loss 0.0135\n",
      "step 37900 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 38000 of 100000: train_loss 0.0127, val loss 0.0129\n",
      "step 38100 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 38200 of 100000: train_loss 0.0129, val loss 0.0141\n",
      "step 38300 of 100000: train_loss 0.0125, val loss 0.0126\n",
      "step 38400 of 100000: train_loss 0.0129, val loss 0.0136\n",
      "step 38500 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 38600 of 100000: train_loss 0.0131, val loss 0.0138\n",
      "step 38700 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 38800 of 100000: train_loss 0.0129, val loss 0.0134\n",
      "step 38900 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 39000 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 39100 of 100000: train_loss 0.0128, val loss 0.0135\n",
      "step 39200 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 39300 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 39400 of 100000: train_loss 0.0136, val loss 0.0141\n",
      "step 39500 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 39600 of 100000: train_loss 0.0117, val loss 0.0126\n",
      "step 39700 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 39800 of 100000: train_loss 0.0139, val loss 0.0145\n",
      "step 39900 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 40000 of 100000: train_loss 0.0140, val loss 0.0143\n",
      "step 40100 of 100000: train_loss 0.0119, val loss 0.0127\n",
      "step 40200 of 100000: train_loss 0.0120, val loss 0.0124\n",
      "step 40300 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 40400 of 100000: train_loss 0.0131, val loss 0.0132\n",
      "step 40500 of 100000: train_loss 0.0119, val loss 0.0122\n",
      "step 40600 of 100000: train_loss 0.0119, val loss 0.0126\n",
      "step 40700 of 100000: train_loss 0.0132, val loss 0.0136\n",
      "step 40800 of 100000: train_loss 0.0117, val loss 0.0125\n",
      "step 40900 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 41000 of 100000: train_loss 0.0130, val loss 0.0134\n",
      "step 41100 of 100000: train_loss 0.0117, val loss 0.0125\n",
      "step 41200 of 100000: train_loss 0.0129, val loss 0.0137\n",
      "step 41300 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 41400 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 41500 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 41600 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 41700 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 41800 of 100000: train_loss 0.0116, val loss 0.0122\n",
      "step 41900 of 100000: train_loss 0.0132, val loss 0.0140\n",
      "step 42000 of 100000: train_loss 0.0137, val loss 0.0142\n",
      "step 42100 of 100000: train_loss 0.0129, val loss 0.0133\n",
      "step 42200 of 100000: train_loss 0.0141, val loss 0.0142\n",
      "step 42300 of 100000: train_loss 0.0131, val loss 0.0141\n",
      "step 42400 of 100000: train_loss 0.0129, val loss 0.0135\n",
      "step 42500 of 100000: train_loss 0.0139, val loss 0.0147\n",
      "step 42600 of 100000: train_loss 0.0130, val loss 0.0138\n",
      "step 42700 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 42800 of 100000: train_loss 0.0126, val loss 0.0134\n",
      "step 42900 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 43000 of 100000: train_loss 0.0129, val loss 0.0140\n",
      "step 43100 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 43200 of 100000: train_loss 0.0138, val loss 0.0142\n",
      "step 43300 of 100000: train_loss 0.0122, val loss 0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 43400 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 43500 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 43600 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 43700 of 100000: train_loss 0.0129, val loss 0.0139\n",
      "step 43800 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 43900 of 100000: train_loss 0.0120, val loss 0.0121\n",
      "step 44000 of 100000: train_loss 0.0143, val loss 0.0150\n",
      "step 44100 of 100000: train_loss 0.0118, val loss 0.0127\n",
      "step 44200 of 100000: train_loss 0.0123, val loss 0.0133\n",
      "step 44300 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 44400 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 44500 of 100000: train_loss 0.0138, val loss 0.0148\n",
      "step 44600 of 100000: train_loss 0.0149, val loss 0.0152\n",
      "step 44700 of 100000: train_loss 0.0138, val loss 0.0146\n",
      "step 44800 of 100000: train_loss 0.0139, val loss 0.0151\n",
      "step 44900 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 45000 of 100000: train_loss 0.0129, val loss 0.0137\n",
      "step 45100 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 45200 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 45300 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 45400 of 100000: train_loss 0.0134, val loss 0.0138\n",
      "step 45500 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 45600 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 45700 of 100000: train_loss 0.0125, val loss 0.0132\n",
      "step 45800 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 45900 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 46000 of 100000: train_loss 0.0128, val loss 0.0136\n",
      "step 46100 of 100000: train_loss 0.0134, val loss 0.0140\n",
      "step 46200 of 100000: train_loss 0.0125, val loss 0.0133\n",
      "step 46300 of 100000: train_loss 0.0130, val loss 0.0136\n",
      "step 46400 of 100000: train_loss 0.0129, val loss 0.0136\n",
      "step 46500 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 46600 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 46700 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 46800 of 100000: train_loss 0.0129, val loss 0.0138\n",
      "step 46900 of 100000: train_loss 0.0129, val loss 0.0135\n",
      "step 47000 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 47100 of 100000: train_loss 0.0125, val loss 0.0128\n",
      "step 47200 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 47300 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 47400 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 47500 of 100000: train_loss 0.0131, val loss 0.0133\n",
      "step 47600 of 100000: train_loss 0.0114, val loss 0.0120\n",
      "step 47700 of 100000: train_loss 0.0142, val loss 0.0150\n",
      "step 47800 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 47900 of 100000: train_loss 0.0121, val loss 0.0124\n",
      "step 48000 of 100000: train_loss 0.0125, val loss 0.0127\n",
      "step 48100 of 100000: train_loss 0.0122, val loss 0.0123\n",
      "step 48200 of 100000: train_loss 0.0119, val loss 0.0125\n",
      "step 48300 of 100000: train_loss 0.0136, val loss 0.0139\n",
      "step 48400 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 48500 of 100000: train_loss 0.0132, val loss 0.0137\n",
      "step 48600 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 48700 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 48800 of 100000: train_loss 0.0121, val loss 0.0124\n",
      "step 48900 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 49000 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 49100 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 49200 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 49300 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 49400 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 49500 of 100000: train_loss 0.0137, val loss 0.0139\n",
      "step 49600 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 49700 of 100000: train_loss 0.0134, val loss 0.0143\n",
      "step 49800 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 49900 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 50000 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 50100 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 50200 of 100000: train_loss 0.0142, val loss 0.0149\n",
      "step 50300 of 100000: train_loss 0.0139, val loss 0.0144\n",
      "step 50400 of 100000: train_loss 0.0123, val loss 0.0125\n",
      "step 50500 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 50600 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 50700 of 100000: train_loss 0.0128, val loss 0.0131\n",
      "step 50800 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 50900 of 100000: train_loss 0.0129, val loss 0.0136\n",
      "step 51000 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 51100 of 100000: train_loss 0.0137, val loss 0.0140\n",
      "step 51200 of 100000: train_loss 0.0127, val loss 0.0130\n",
      "step 51300 of 100000: train_loss 0.0122, val loss 0.0132\n",
      "step 51400 of 100000: train_loss 0.0128, val loss 0.0129\n",
      "step 51500 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 51600 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 51700 of 100000: train_loss 0.0117, val loss 0.0123\n",
      "step 51800 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 51900 of 100000: train_loss 0.0129, val loss 0.0142\n",
      "step 52000 of 100000: train_loss 0.0142, val loss 0.0147\n",
      "step 52100 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 52200 of 100000: train_loss 0.0144, val loss 0.0150\n",
      "step 52300 of 100000: train_loss 0.0120, val loss 0.0128\n",
      "step 52400 of 100000: train_loss 0.0119, val loss 0.0125\n",
      "step 52500 of 100000: train_loss 0.0125, val loss 0.0134\n",
      "step 52600 of 100000: train_loss 0.0127, val loss 0.0136\n",
      "step 52700 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 52800 of 100000: train_loss 0.0135, val loss 0.0143\n",
      "step 52900 of 100000: train_loss 0.0128, val loss 0.0135\n",
      "step 53000 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 53100 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 53200 of 100000: train_loss 0.0132, val loss 0.0135\n",
      "step 53300 of 100000: train_loss 0.0114, val loss 0.0119\n",
      "step 53400 of 100000: train_loss 0.0125, val loss 0.0132\n",
      "step 53500 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 53600 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 53700 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 53800 of 100000: train_loss 0.0122, val loss 0.0130\n",
      "step 53900 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 54000 of 100000: train_loss 0.0131, val loss 0.0139\n",
      "step 54100 of 100000: train_loss 0.0133, val loss 0.0139\n",
      "step 54200 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 54300 of 100000: train_loss 0.0131, val loss 0.0141\n",
      "step 54400 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 54500 of 100000: train_loss 0.0130, val loss 0.0135\n",
      "step 54600 of 100000: train_loss 0.0134, val loss 0.0138\n",
      "step 54700 of 100000: train_loss 0.0123, val loss 0.0133\n",
      "step 54800 of 100000: train_loss 0.0154, val loss 0.0155\n",
      "step 54900 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 55000 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 55100 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 55200 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 55300 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 55400 of 100000: train_loss 0.0129, val loss 0.0134\n",
      "step 55500 of 100000: train_loss 0.0136, val loss 0.0143\n",
      "step 55600 of 100000: train_loss 0.0129, val loss 0.0133\n",
      "step 55700 of 100000: train_loss 0.0137, val loss 0.0140\n",
      "step 55800 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 55900 of 100000: train_loss 0.0116, val loss 0.0121\n",
      "step 56000 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 56100 of 100000: train_loss 0.0115, val loss 0.0120\n",
      "step 56200 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 56300 of 100000: train_loss 0.0119, val loss 0.0126\n",
      "step 56400 of 100000: train_loss 0.0133, val loss 0.0138\n",
      "step 56500 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 56600 of 100000: train_loss 0.0133, val loss 0.0141\n",
      "step 56700 of 100000: train_loss 0.0128, val loss 0.0131\n",
      "step 56800 of 100000: train_loss 0.0127, val loss 0.0131\n",
      "step 56900 of 100000: train_loss 0.0130, val loss 0.0138\n",
      "step 57000 of 100000: train_loss 0.0118, val loss 0.0124\n",
      "step 57100 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 57200 of 100000: train_loss 0.0144, val loss 0.0142\n",
      "step 57300 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 57400 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 57500 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 57600 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 57700 of 100000: train_loss 0.0124, val loss 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 57800 of 100000: train_loss 0.0128, val loss 0.0135\n",
      "step 57900 of 100000: train_loss 0.0123, val loss 0.0132\n",
      "step 58000 of 100000: train_loss 0.0141, val loss 0.0141\n",
      "step 58100 of 100000: train_loss 0.0128, val loss 0.0130\n",
      "step 58200 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 58300 of 100000: train_loss 0.0121, val loss 0.0124\n",
      "step 58400 of 100000: train_loss 0.0126, val loss 0.0135\n",
      "step 58500 of 100000: train_loss 0.0118, val loss 0.0125\n",
      "step 58600 of 100000: train_loss 0.0131, val loss 0.0140\n",
      "step 58700 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 58800 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 58900 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 59000 of 100000: train_loss 0.0148, val loss 0.0152\n",
      "step 59100 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 59200 of 100000: train_loss 0.0117, val loss 0.0124\n",
      "step 59300 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 59400 of 100000: train_loss 0.0118, val loss 0.0120\n",
      "step 59500 of 100000: train_loss 0.0152, val loss 0.0154\n",
      "step 59600 of 100000: train_loss 0.0137, val loss 0.0141\n",
      "step 59700 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 59800 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 59900 of 100000: train_loss 0.0130, val loss 0.0132\n",
      "step 60000 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 60100 of 100000: train_loss 0.0132, val loss 0.0136\n",
      "step 60200 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 60300 of 100000: train_loss 0.0129, val loss 0.0134\n",
      "step 60400 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 60500 of 100000: train_loss 0.0145, val loss 0.0148\n",
      "step 60600 of 100000: train_loss 0.0138, val loss 0.0140\n",
      "step 60700 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 60800 of 100000: train_loss 0.0131, val loss 0.0134\n",
      "step 60900 of 100000: train_loss 0.0125, val loss 0.0133\n",
      "step 61000 of 100000: train_loss 0.0132, val loss 0.0137\n",
      "step 61100 of 100000: train_loss 0.0126, val loss 0.0134\n",
      "step 61200 of 100000: train_loss 0.0127, val loss 0.0132\n",
      "step 61300 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 61400 of 100000: train_loss 0.0121, val loss 0.0124\n",
      "step 61500 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 61600 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 61700 of 100000: train_loss 0.0118, val loss 0.0123\n",
      "step 61800 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 61900 of 100000: train_loss 0.0129, val loss 0.0131\n",
      "step 62000 of 100000: train_loss 0.0132, val loss 0.0139\n",
      "step 62100 of 100000: train_loss 0.0119, val loss 0.0121\n",
      "step 62200 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 62300 of 100000: train_loss 0.0123, val loss 0.0125\n",
      "step 62400 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 62500 of 100000: train_loss 0.0133, val loss 0.0137\n",
      "step 62600 of 100000: train_loss 0.0123, val loss 0.0126\n",
      "step 62700 of 100000: train_loss 0.0126, val loss 0.0128\n",
      "step 62800 of 100000: train_loss 0.0123, val loss 0.0126\n",
      "step 62900 of 100000: train_loss 0.0131, val loss 0.0132\n",
      "step 63000 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 63100 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 63200 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 63300 of 100000: train_loss 0.0133, val loss 0.0135\n",
      "step 63400 of 100000: train_loss 0.0135, val loss 0.0139\n",
      "step 63500 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 63600 of 100000: train_loss 0.0147, val loss 0.0155\n",
      "step 63700 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 63800 of 100000: train_loss 0.0121, val loss 0.0129\n",
      "step 63900 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 64000 of 100000: train_loss 0.0125, val loss 0.0134\n",
      "step 64100 of 100000: train_loss 0.0128, val loss 0.0137\n",
      "step 64200 of 100000: train_loss 0.0116, val loss 0.0122\n",
      "step 64300 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 64400 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 64500 of 100000: train_loss 0.0142, val loss 0.0150\n",
      "step 64600 of 100000: train_loss 0.0125, val loss 0.0132\n",
      "step 64700 of 100000: train_loss 0.0132, val loss 0.0132\n",
      "step 64800 of 100000: train_loss 0.0133, val loss 0.0139\n",
      "step 64900 of 100000: train_loss 0.0128, val loss 0.0131\n",
      "step 65000 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 65100 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 65200 of 100000: train_loss 0.0133, val loss 0.0135\n",
      "step 65300 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 65400 of 100000: train_loss 0.0122, val loss 0.0130\n",
      "step 65500 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 65600 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 65700 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 65800 of 100000: train_loss 0.0136, val loss 0.0140\n",
      "step 65900 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 66000 of 100000: train_loss 0.0138, val loss 0.0141\n",
      "step 66100 of 100000: train_loss 0.0127, val loss 0.0135\n",
      "step 66200 of 100000: train_loss 0.0134, val loss 0.0139\n",
      "step 66300 of 100000: train_loss 0.0129, val loss 0.0134\n",
      "step 66400 of 100000: train_loss 0.0121, val loss 0.0124\n",
      "step 66500 of 100000: train_loss 0.0117, val loss 0.0122\n",
      "step 66600 of 100000: train_loss 0.0131, val loss 0.0131\n",
      "step 66700 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 66800 of 100000: train_loss 0.0129, val loss 0.0136\n",
      "step 66900 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 67000 of 100000: train_loss 0.0130, val loss 0.0137\n",
      "step 67100 of 100000: train_loss 0.0146, val loss 0.0151\n",
      "step 67200 of 100000: train_loss 0.0121, val loss 0.0129\n",
      "step 67300 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 67400 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 67500 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 67600 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 67700 of 100000: train_loss 0.0139, val loss 0.0143\n",
      "step 67800 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 67900 of 100000: train_loss 0.0119, val loss 0.0122\n",
      "step 68000 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 68100 of 100000: train_loss 0.0118, val loss 0.0121\n",
      "step 68200 of 100000: train_loss 0.0117, val loss 0.0119\n",
      "step 68300 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 68400 of 100000: train_loss 0.0130, val loss 0.0133\n",
      "step 68500 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 68600 of 100000: train_loss 0.0132, val loss 0.0141\n",
      "step 68700 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 68800 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 68900 of 100000: train_loss 0.0129, val loss 0.0132\n",
      "step 69000 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 69100 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 69200 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 69300 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 69400 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 69500 of 100000: train_loss 0.0135, val loss 0.0142\n",
      "step 69600 of 100000: train_loss 0.0134, val loss 0.0138\n",
      "step 69700 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 69800 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 69900 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 70000 of 100000: train_loss 0.0129, val loss 0.0136\n",
      "step 70100 of 100000: train_loss 0.0129, val loss 0.0137\n",
      "step 70200 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 70300 of 100000: train_loss 0.0130, val loss 0.0138\n",
      "step 70400 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 70500 of 100000: train_loss 0.0138, val loss 0.0140\n",
      "step 70600 of 100000: train_loss 0.0135, val loss 0.0136\n",
      "step 70700 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 70800 of 100000: train_loss 0.0134, val loss 0.0140\n",
      "step 70900 of 100000: train_loss 0.0123, val loss 0.0132\n",
      "step 71000 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 71100 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 71200 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 71300 of 100000: train_loss 0.0129, val loss 0.0135\n",
      "step 71400 of 100000: train_loss 0.0119, val loss 0.0124\n",
      "step 71500 of 100000: train_loss 0.0121, val loss 0.0128\n",
      "step 71600 of 100000: train_loss 0.0132, val loss 0.0135\n",
      "step 71700 of 100000: train_loss 0.0126, val loss 0.0134\n",
      "step 71800 of 100000: train_loss 0.0124, val loss 0.0126\n",
      "step 71900 of 100000: train_loss 0.0128, val loss 0.0136\n",
      "step 72000 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 72100 of 100000: train_loss 0.0127, val loss 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 72200 of 100000: train_loss 0.0118, val loss 0.0125\n",
      "step 72300 of 100000: train_loss 0.0122, val loss 0.0130\n",
      "step 72400 of 100000: train_loss 0.0132, val loss 0.0140\n",
      "step 72500 of 100000: train_loss 0.0132, val loss 0.0137\n",
      "step 72600 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 72700 of 100000: train_loss 0.0132, val loss 0.0140\n",
      "step 72800 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 72900 of 100000: train_loss 0.0131, val loss 0.0137\n",
      "step 73000 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 73100 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 73200 of 100000: train_loss 0.0139, val loss 0.0147\n",
      "step 73300 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 73400 of 100000: train_loss 0.0119, val loss 0.0129\n",
      "step 73500 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 73600 of 100000: train_loss 0.0119, val loss 0.0125\n",
      "step 73700 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 73800 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 73900 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 74000 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 74100 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 74200 of 100000: train_loss 0.0130, val loss 0.0137\n",
      "step 74300 of 100000: train_loss 0.0129, val loss 0.0136\n",
      "step 74400 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 74500 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 74600 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 74700 of 100000: train_loss 0.0129, val loss 0.0134\n",
      "step 74800 of 100000: train_loss 0.0138, val loss 0.0145\n",
      "step 74900 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 75000 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 75100 of 100000: train_loss 0.0115, val loss 0.0117\n",
      "step 75200 of 100000: train_loss 0.0132, val loss 0.0136\n",
      "step 75300 of 100000: train_loss 0.0129, val loss 0.0133\n",
      "step 75400 of 100000: train_loss 0.0126, val loss 0.0135\n",
      "step 75500 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 75600 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 75700 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 75800 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 75900 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 76000 of 100000: train_loss 0.0116, val loss 0.0122\n",
      "step 76100 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 76200 of 100000: train_loss 0.0119, val loss 0.0126\n",
      "step 76300 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 76400 of 100000: train_loss 0.0140, val loss 0.0145\n",
      "step 76500 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 76600 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 76700 of 100000: train_loss 0.0135, val loss 0.0139\n",
      "step 76800 of 100000: train_loss 0.0128, val loss 0.0136\n",
      "step 76900 of 100000: train_loss 0.0118, val loss 0.0123\n",
      "step 77000 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 77100 of 100000: train_loss 0.0154, val loss 0.0159\n",
      "step 77200 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 77300 of 100000: train_loss 0.0131, val loss 0.0138\n",
      "step 77400 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 77500 of 100000: train_loss 0.0124, val loss 0.0125\n",
      "step 77600 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 77700 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 77800 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 77900 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 78000 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 78100 of 100000: train_loss 0.0123, val loss 0.0131\n",
      "step 78200 of 100000: train_loss 0.0119, val loss 0.0127\n",
      "step 78300 of 100000: train_loss 0.0122, val loss 0.0130\n",
      "step 78400 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 78500 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 78600 of 100000: train_loss 0.0137, val loss 0.0143\n",
      "step 78700 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 78800 of 100000: train_loss 0.0119, val loss 0.0126\n",
      "step 78900 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 79000 of 100000: train_loss 0.0123, val loss 0.0132\n",
      "step 79100 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 79200 of 100000: train_loss 0.0126, val loss 0.0130\n",
      "step 79300 of 100000: train_loss 0.0130, val loss 0.0134\n",
      "step 79400 of 100000: train_loss 0.0122, val loss 0.0126\n",
      "step 79500 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 79600 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 79700 of 100000: train_loss 0.0115, val loss 0.0121\n",
      "step 79800 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 79900 of 100000: train_loss 0.0132, val loss 0.0138\n",
      "step 80000 of 100000: train_loss 0.0119, val loss 0.0120\n",
      "step 80100 of 100000: train_loss 0.0118, val loss 0.0124\n",
      "step 80200 of 100000: train_loss 0.0126, val loss 0.0129\n",
      "step 80300 of 100000: train_loss 0.0138, val loss 0.0142\n",
      "step 80400 of 100000: train_loss 0.0115, val loss 0.0121\n",
      "step 80500 of 100000: train_loss 0.0134, val loss 0.0135\n",
      "step 80600 of 100000: train_loss 0.0117, val loss 0.0122\n",
      "step 80700 of 100000: train_loss 0.0133, val loss 0.0141\n",
      "step 80800 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 80900 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 81000 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 81100 of 100000: train_loss 0.0124, val loss 0.0128\n",
      "step 81200 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 81300 of 100000: train_loss 0.0123, val loss 0.0133\n",
      "step 81400 of 100000: train_loss 0.0126, val loss 0.0134\n",
      "step 81500 of 100000: train_loss 0.0137, val loss 0.0147\n",
      "step 81600 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 81700 of 100000: train_loss 0.0130, val loss 0.0134\n",
      "step 81800 of 100000: train_loss 0.0132, val loss 0.0131\n",
      "step 81900 of 100000: train_loss 0.0135, val loss 0.0139\n",
      "step 82000 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 82100 of 100000: train_loss 0.0130, val loss 0.0134\n",
      "step 82200 of 100000: train_loss 0.0117, val loss 0.0125\n",
      "step 82300 of 100000: train_loss 0.0133, val loss 0.0139\n",
      "step 82400 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 82500 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 82600 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 82700 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 82800 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 82900 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 83000 of 100000: train_loss 0.0127, val loss 0.0133\n",
      "step 83100 of 100000: train_loss 0.0120, val loss 0.0124\n",
      "step 83200 of 100000: train_loss 0.0120, val loss 0.0122\n",
      "step 83300 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 83400 of 100000: train_loss 0.0126, val loss 0.0131\n",
      "step 83500 of 100000: train_loss 0.0125, val loss 0.0132\n",
      "step 83600 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 83700 of 100000: train_loss 0.0133, val loss 0.0134\n",
      "step 83800 of 100000: train_loss 0.0121, val loss 0.0131\n",
      "step 83900 of 100000: train_loss 0.0116, val loss 0.0127\n",
      "step 84000 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 84100 of 100000: train_loss 0.0133, val loss 0.0138\n",
      "step 84200 of 100000: train_loss 0.0126, val loss 0.0135\n",
      "step 84300 of 100000: train_loss 0.0121, val loss 0.0125\n",
      "step 84400 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 84500 of 100000: train_loss 0.0132, val loss 0.0135\n",
      "step 84600 of 100000: train_loss 0.0135, val loss 0.0140\n",
      "step 84700 of 100000: train_loss 0.0129, val loss 0.0133\n",
      "step 84800 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 84900 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 85000 of 100000: train_loss 0.0126, val loss 0.0137\n",
      "step 85100 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 85200 of 100000: train_loss 0.0136, val loss 0.0143\n",
      "step 85300 of 100000: train_loss 0.0127, val loss 0.0134\n",
      "step 85400 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 85500 of 100000: train_loss 0.0122, val loss 0.0127\n",
      "step 85600 of 100000: train_loss 0.0120, val loss 0.0124\n",
      "step 85700 of 100000: train_loss 0.0119, val loss 0.0126\n",
      "step 85800 of 100000: train_loss 0.0118, val loss 0.0127\n",
      "step 85900 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 86000 of 100000: train_loss 0.0121, val loss 0.0123\n",
      "step 86100 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 86200 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 86300 of 100000: train_loss 0.0152, val loss 0.0156\n",
      "step 86400 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 86500 of 100000: train_loss 0.0125, val loss 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 86600 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 86700 of 100000: train_loss 0.0133, val loss 0.0139\n",
      "step 86800 of 100000: train_loss 0.0125, val loss 0.0130\n",
      "step 86900 of 100000: train_loss 0.0129, val loss 0.0131\n",
      "step 87000 of 100000: train_loss 0.0122, val loss 0.0128\n",
      "step 87100 of 100000: train_loss 0.0120, val loss 0.0127\n",
      "step 87200 of 100000: train_loss 0.0131, val loss 0.0134\n",
      "step 87300 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 87400 of 100000: train_loss 0.0115, val loss 0.0121\n",
      "step 87500 of 100000: train_loss 0.0133, val loss 0.0136\n",
      "step 87600 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 87700 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 87800 of 100000: train_loss 0.0124, val loss 0.0129\n",
      "step 87900 of 100000: train_loss 0.0137, val loss 0.0142\n",
      "step 88000 of 100000: train_loss 0.0123, val loss 0.0130\n",
      "step 88100 of 100000: train_loss 0.0127, val loss 0.0131\n",
      "step 88200 of 100000: train_loss 0.0140, val loss 0.0144\n",
      "step 88300 of 100000: train_loss 0.0130, val loss 0.0133\n",
      "step 88400 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 88500 of 100000: train_loss 0.0143, val loss 0.0148\n",
      "step 88600 of 100000: train_loss 0.0125, val loss 0.0129\n",
      "step 88700 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 88800 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 88900 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 89000 of 100000: train_loss 0.0139, val loss 0.0141\n",
      "step 89100 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 89200 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 89300 of 100000: train_loss 0.0127, val loss 0.0132\n",
      "step 89400 of 100000: train_loss 0.0134, val loss 0.0142\n",
      "step 89500 of 100000: train_loss 0.0131, val loss 0.0133\n",
      "step 89600 of 100000: train_loss 0.0121, val loss 0.0129\n",
      "step 89700 of 100000: train_loss 0.0124, val loss 0.0131\n",
      "step 89800 of 100000: train_loss 0.0126, val loss 0.0134\n",
      "step 89900 of 100000: train_loss 0.0123, val loss 0.0125\n",
      "step 90000 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 90100 of 100000: train_loss 0.0122, val loss 0.0129\n",
      "step 90200 of 100000: train_loss 0.0126, val loss 0.0132\n",
      "step 90300 of 100000: train_loss 0.0131, val loss 0.0136\n",
      "step 90400 of 100000: train_loss 0.0124, val loss 0.0135\n",
      "step 90500 of 100000: train_loss 0.0121, val loss 0.0127\n",
      "step 90600 of 100000: train_loss 0.0130, val loss 0.0139\n",
      "step 90700 of 100000: train_loss 0.0134, val loss 0.0138\n",
      "step 90800 of 100000: train_loss 0.0131, val loss 0.0133\n",
      "step 90900 of 100000: train_loss 0.0121, val loss 0.0130\n",
      "step 91000 of 100000: train_loss 0.0120, val loss 0.0128\n",
      "step 91100 of 100000: train_loss 0.0122, val loss 0.0125\n",
      "step 91200 of 100000: train_loss 0.0124, val loss 0.0127\n",
      "step 91300 of 100000: train_loss 0.0126, val loss 0.0133\n",
      "step 91400 of 100000: train_loss 0.0129, val loss 0.0132\n",
      "step 91500 of 100000: train_loss 0.0129, val loss 0.0135\n",
      "step 91600 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 91700 of 100000: train_loss 0.0134, val loss 0.0134\n",
      "step 91800 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 91900 of 100000: train_loss 0.0114, val loss 0.0122\n",
      "step 92000 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 92100 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 92200 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 92300 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 92400 of 100000: train_loss 0.0124, val loss 0.0132\n",
      "step 92500 of 100000: train_loss 0.0128, val loss 0.0133\n",
      "step 92600 of 100000: train_loss 0.0123, val loss 0.0131\n",
      "step 92700 of 100000: train_loss 0.0133, val loss 0.0141\n",
      "step 92800 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 92900 of 100000: train_loss 0.0117, val loss 0.0121\n",
      "step 93000 of 100000: train_loss 0.0118, val loss 0.0123\n",
      "step 93100 of 100000: train_loss 0.0119, val loss 0.0127\n",
      "step 93200 of 100000: train_loss 0.0125, val loss 0.0131\n",
      "step 93300 of 100000: train_loss 0.0112, val loss 0.0119\n",
      "step 93400 of 100000: train_loss 0.0123, val loss 0.0128\n",
      "step 93500 of 100000: train_loss 0.0119, val loss 0.0123\n",
      "step 93600 of 100000: train_loss 0.0118, val loss 0.0126\n",
      "step 93700 of 100000: train_loss 0.0118, val loss 0.0122\n",
      "step 93800 of 100000: train_loss 0.0113, val loss 0.0118\n",
      "step 93900 of 100000: train_loss 0.0120, val loss 0.0126\n",
      "step 94000 of 100000: train_loss 0.0116, val loss 0.0123\n",
      "step 94100 of 100000: train_loss 0.0112, val loss 0.0116\n",
      "step 94200 of 100000: train_loss 0.0117, val loss 0.0123\n",
      "step 94300 of 100000: train_loss 0.0116, val loss 0.0119\n",
      "step 94400 of 100000: train_loss 0.0123, val loss 0.0127\n",
      "step 94500 of 100000: train_loss 0.0121, val loss 0.0126\n",
      "step 94600 of 100000: train_loss 0.0120, val loss 0.0123\n",
      "step 94700 of 100000: train_loss 0.0123, val loss 0.0129\n",
      "step 94800 of 100000: train_loss 0.0117, val loss 0.0125\n",
      "step 94900 of 100000: train_loss 0.0114, val loss 0.0120\n",
      "step 95000 of 100000: train_loss 0.0114, val loss 0.0117\n",
      "step 95100 of 100000: train_loss 0.0119, val loss 0.0124\n",
      "step 95200 of 100000: train_loss 0.0114, val loss 0.0120\n",
      "step 95300 of 100000: train_loss 0.0120, val loss 0.0125\n",
      "step 95400 of 100000: train_loss 0.0113, val loss 0.0121\n",
      "step 95500 of 100000: train_loss 0.0114, val loss 0.0116\n",
      "step 95600 of 100000: train_loss 0.0113, val loss 0.0122\n",
      "step 95700 of 100000: train_loss 0.0114, val loss 0.0121\n",
      "step 95800 of 100000: train_loss 0.0117, val loss 0.0120\n",
      "step 95900 of 100000: train_loss 0.0114, val loss 0.0118\n",
      "step 96000 of 100000: train_loss 0.0118, val loss 0.0120\n",
      "step 96100 of 100000: train_loss 0.0117, val loss 0.0120\n",
      "step 96200 of 100000: train_loss 0.0114, val loss 0.0121\n",
      "step 96300 of 100000: train_loss 0.0133, val loss 0.0137\n",
      "step 96400 of 100000: train_loss 0.0116, val loss 0.0122\n",
      "step 96500 of 100000: train_loss 0.0111, val loss 0.0117\n",
      "step 96600 of 100000: train_loss 0.0117, val loss 0.0120\n",
      "step 96700 of 100000: train_loss 0.0115, val loss 0.0118\n",
      "step 96800 of 100000: train_loss 0.0108, val loss 0.0118\n",
      "step 96900 of 100000: train_loss 0.0115, val loss 0.0123\n",
      "step 97000 of 100000: train_loss 0.0115, val loss 0.0118\n",
      "step 97100 of 100000: train_loss 0.0115, val loss 0.0117\n",
      "step 97200 of 100000: train_loss 0.0120, val loss 0.0129\n",
      "step 97300 of 100000: train_loss 0.0150, val loss 0.0157\n",
      "step 97400 of 100000: train_loss 0.0115, val loss 0.0123\n",
      "step 97500 of 100000: train_loss 0.0111, val loss 0.0118\n",
      "step 97600 of 100000: train_loss 0.0111, val loss 0.0115\n",
      "step 97700 of 100000: train_loss 0.0112, val loss 0.0118\n",
      "step 97800 of 100000: train_loss 0.0128, val loss 0.0132\n",
      "step 97900 of 100000: train_loss 0.0113, val loss 0.0116\n",
      "step 98000 of 100000: train_loss 0.0110, val loss 0.0117\n",
      "step 98100 of 100000: train_loss 0.0114, val loss 0.0119\n",
      "step 98200 of 100000: train_loss 0.0119, val loss 0.0124\n",
      "step 98300 of 100000: train_loss 0.0113, val loss 0.0118\n",
      "step 98400 of 100000: train_loss 0.0114, val loss 0.0119\n",
      "step 98500 of 100000: train_loss 0.0125, val loss 0.0132\n",
      "step 98600 of 100000: train_loss 0.0113, val loss 0.0118\n",
      "step 98700 of 100000: train_loss 0.0116, val loss 0.0121\n",
      "step 98800 of 100000: train_loss 0.0108, val loss 0.0113\n",
      "step 98900 of 100000: train_loss 0.0111, val loss 0.0117\n",
      "step 99000 of 100000: train_loss 0.0114, val loss 0.0116\n",
      "step 99100 of 100000: train_loss 0.0112, val loss 0.0121\n",
      "step 99200 of 100000: train_loss 0.0114, val loss 0.0121\n",
      "step 99300 of 100000: train_loss 0.0128, val loss 0.0134\n",
      "step 99400 of 100000: train_loss 0.0110, val loss 0.0113\n",
      "step 99500 of 100000: train_loss 0.0124, val loss 0.0130\n",
      "step 99600 of 100000: train_loss 0.0112, val loss 0.0115\n",
      "step 99700 of 100000: train_loss 0.0114, val loss 0.0120\n",
      "step 99800 of 100000: train_loss 0.0110, val loss 0.0120\n",
      "step 99900 of 100000: train_loss 0.0117, val loss 0.0118\n"
     ]
    }
   ],
   "source": [
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "120aa247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [1. 1. 1.] est ATE: [0.98952744 1.02514063 1.02447   ] error: [0.01047256 0.02514063 0.02447   ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff7300828e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "inf = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model(train_data.to(device))\n",
    "\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d1bea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([1.300e+01, 1.170e+02, 9.140e+02, 2.787e+03, 3.690e+03, 1.917e+03,\n",
       "        4.950e+02, 6.500e+01, 1.000e+00, 1.000e+00]),\n",
       " array([-4.75669575, -3.42283726, -2.08897877, -0.75512028,  0.57873821,\n",
       "         1.9125967 ,  3.24645519,  4.58031368,  5.91417217,  7.24803066,\n",
       "         8.58188915]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.hist(preds0[:,3, 0], alpha=0.5)\n",
    "plt.hist(preds1[:,3, 0], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc2c102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view attention maps\n",
    "maps = []\n",
    "for j in range(n_layers):\n",
    "    heads = model.blocks[j].mha.heads\n",
    "    for i in range(num_heads):\n",
    "        maps.append(heads[i].att_wei.mean(0).cpu().detach().numpy())\n",
    "\n",
    "maps = np.stack(maps).mean(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(maps, cmap='hot', interpolation='nearest')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=1)\n",
    "# Setting the axis tick labels\n",
    "ax.set_xticks(np.arange(len(list(DAGnx.nodes))))\n",
    "ax.set_yticks(np.arange(len(list(DAGnx.nodes))))\n",
    "\n",
    "ax.set_xticklabels(list(DAGnx.nodes))\n",
    "ax.set_yticklabels(list(DAGnx.nodes))\n",
    "\n",
    "# Rotating the tick labels inorder to set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('attention_maps.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb57135",
   "metadata": {},
   "source": [
    "## Mediation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67aa10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'M', 'Y'] [0.25 0.25 0.25]\n",
      "(10000, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "d=3\n",
    "_, _, _, _, _, Y0, Y1 = generate_data_mediation(N=1000000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  # multi-dim ATE based off a large sample\n",
    "all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data_mediation(N=sample_size, d=d)\n",
    "print(var_names, ATE)\n",
    "print(all_data.shape)\n",
    "\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b83dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = all_data.shape[2]\n",
    "\n",
    "model = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx,\n",
    "                causal_ordering=causal_ordering,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types_sorted=var_types,\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14e8f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 100000: train_loss 2.5349, val loss 2.4913\n",
      "step 100 of 100000: train_loss 2.2544, val loss 2.2282\n",
      "step 200 of 100000: train_loss 2.0964, val loss 2.0861\n",
      "step 300 of 100000: train_loss 2.0538, val loss 2.0514\n",
      "step 400 of 100000: train_loss 2.0400, val loss 2.0190\n",
      "step 500 of 100000: train_loss 2.0294, val loss 2.0311\n",
      "step 600 of 100000: train_loss 2.0253, val loss 2.0141\n",
      "step 700 of 100000: train_loss 2.0548, val loss 2.0068\n",
      "step 800 of 100000: train_loss 2.0317, val loss 2.0219\n",
      "step 900 of 100000: train_loss 2.0319, val loss 2.0376\n",
      "step 1000 of 100000: train_loss 2.0153, val loss 2.0021\n",
      "step 1100 of 100000: train_loss 2.0224, val loss 2.0200\n",
      "step 1200 of 100000: train_loss 2.0125, val loss 2.0134\n",
      "step 1300 of 100000: train_loss 2.0322, val loss 2.0134\n",
      "step 1400 of 100000: train_loss 2.0064, val loss 2.0309\n",
      "step 1500 of 100000: train_loss 2.0065, val loss 2.0054\n",
      "step 1600 of 100000: train_loss 2.0270, val loss 2.0112\n",
      "step 1700 of 100000: train_loss 2.0252, val loss 2.0016\n",
      "step 1800 of 100000: train_loss 2.0313, val loss 2.0161\n",
      "step 1900 of 100000: train_loss 2.0147, val loss 2.0240\n",
      "step 2000 of 100000: train_loss 2.0302, val loss 2.0076\n",
      "step 2100 of 100000: train_loss 2.0377, val loss 2.0244\n",
      "step 2200 of 100000: train_loss 2.0274, val loss 2.0228\n",
      "step 2300 of 100000: train_loss 2.0185, val loss 2.0000\n",
      "step 2400 of 100000: train_loss 2.0306, val loss 2.0171\n",
      "step 2500 of 100000: train_loss 2.0264, val loss 2.0053\n",
      "step 2600 of 100000: train_loss 1.9966, val loss 1.9988\n",
      "step 2700 of 100000: train_loss 2.0222, val loss 2.0164\n",
      "step 2800 of 100000: train_loss 2.0244, val loss 2.0025\n",
      "step 2900 of 100000: train_loss 2.0243, val loss 2.0039\n",
      "step 3000 of 100000: train_loss 2.0199, val loss 2.0106\n",
      "step 3100 of 100000: train_loss 2.0074, val loss 1.9899\n",
      "step 3200 of 100000: train_loss 2.0306, val loss 1.9995\n",
      "step 3300 of 100000: train_loss 2.0167, val loss 1.9899\n",
      "step 3400 of 100000: train_loss 2.0293, val loss 2.0018\n",
      "step 3500 of 100000: train_loss 2.0261, val loss 2.0220\n",
      "step 3600 of 100000: train_loss 2.0026, val loss 2.0047\n",
      "step 3700 of 100000: train_loss 2.0291, val loss 1.9923\n",
      "step 3800 of 100000: train_loss 2.0098, val loss 2.0104\n",
      "step 3900 of 100000: train_loss 2.0208, val loss 2.0074\n",
      "step 4000 of 100000: train_loss 2.0221, val loss 1.9988\n",
      "step 4100 of 100000: train_loss 2.0274, val loss 2.0398\n",
      "step 4200 of 100000: train_loss 2.0107, val loss 2.0028\n",
      "step 4300 of 100000: train_loss 2.0458, val loss 1.9980\n",
      "step 4400 of 100000: train_loss 2.0297, val loss 2.0029\n",
      "step 4500 of 100000: train_loss 2.0362, val loss 2.0081\n",
      "step 4600 of 100000: train_loss 2.0202, val loss 2.0112\n",
      "step 4700 of 100000: train_loss 2.0211, val loss 2.0029\n",
      "step 4800 of 100000: train_loss 2.0198, val loss 2.0210\n",
      "step 4900 of 100000: train_loss 2.0395, val loss 2.0151\n",
      "step 5000 of 100000: train_loss 2.0032, val loss 1.9776\n",
      "step 5100 of 100000: train_loss 2.0277, val loss 2.0072\n",
      "step 5200 of 100000: train_loss 2.0183, val loss 1.9946\n",
      "step 5300 of 100000: train_loss 2.0065, val loss 2.0082\n",
      "step 5400 of 100000: train_loss 2.0235, val loss 2.0094\n",
      "step 5500 of 100000: train_loss 2.0286, val loss 2.0287\n",
      "step 5600 of 100000: train_loss 2.0372, val loss 1.9802\n",
      "step 5700 of 100000: train_loss 2.0016, val loss 2.0019\n",
      "step 5800 of 100000: train_loss 2.0160, val loss 2.0097\n",
      "step 5900 of 100000: train_loss 2.0187, val loss 2.0075\n",
      "step 6000 of 100000: train_loss 2.0271, val loss 1.9981\n",
      "step 6100 of 100000: train_loss 2.0147, val loss 2.0169\n",
      "step 6200 of 100000: train_loss 2.0158, val loss 2.0085\n",
      "step 6300 of 100000: train_loss 2.0055, val loss 2.0050\n",
      "step 6400 of 100000: train_loss 2.0047, val loss 2.0078\n",
      "step 6500 of 100000: train_loss 2.0194, val loss 1.9954\n",
      "step 6600 of 100000: train_loss 2.0103, val loss 2.0136\n",
      "step 6700 of 100000: train_loss 2.0212, val loss 2.0064\n",
      "step 6800 of 100000: train_loss 2.0225, val loss 2.0202\n",
      "step 6900 of 100000: train_loss 2.0193, val loss 1.9848\n",
      "step 7000 of 100000: train_loss 2.0133, val loss 1.9992\n",
      "step 7100 of 100000: train_loss 2.0194, val loss 2.0225\n",
      "step 7200 of 100000: train_loss 2.0345, val loss 2.0053\n",
      "step 7300 of 100000: train_loss 2.0186, val loss 2.0052\n",
      "step 7400 of 100000: train_loss 2.0488, val loss 1.9997\n",
      "step 7500 of 100000: train_loss 2.0132, val loss 2.0034\n",
      "step 7600 of 100000: train_loss 2.0274, val loss 2.0125\n",
      "step 7700 of 100000: train_loss 2.0225, val loss 2.0035\n",
      "step 7800 of 100000: train_loss 2.0279, val loss 2.0026\n",
      "step 7900 of 100000: train_loss 2.0166, val loss 2.0015\n",
      "step 8000 of 100000: train_loss 2.0226, val loss 2.0092\n",
      "step 8100 of 100000: train_loss 2.0277, val loss 2.0099\n",
      "step 8200 of 100000: train_loss 2.0021, val loss 2.0028\n",
      "step 8300 of 100000: train_loss 2.0248, val loss 2.0077\n",
      "step 8400 of 100000: train_loss 2.0279, val loss 2.0135\n",
      "step 8500 of 100000: train_loss 2.0264, val loss 2.0058\n",
      "step 8600 of 100000: train_loss 2.0251, val loss 2.0136\n",
      "step 8700 of 100000: train_loss 2.0024, val loss 2.0139\n",
      "step 8800 of 100000: train_loss 2.0136, val loss 2.0003\n",
      "step 8900 of 100000: train_loss 2.0217, val loss 2.0166\n",
      "step 9000 of 100000: train_loss 2.0128, val loss 2.0046\n",
      "step 9100 of 100000: train_loss 2.0232, val loss 1.9983\n",
      "step 9200 of 100000: train_loss 2.0100, val loss 2.0161\n",
      "step 9300 of 100000: train_loss 2.0085, val loss 2.0002\n",
      "step 9400 of 100000: train_loss 2.0266, val loss 1.9782\n",
      "step 9500 of 100000: train_loss 2.0409, val loss 2.0087\n",
      "step 9600 of 100000: train_loss 2.0213, val loss 1.9961\n",
      "step 9700 of 100000: train_loss 2.0342, val loss 2.0059\n",
      "step 9800 of 100000: train_loss 2.0050, val loss 2.0121\n",
      "step 9900 of 100000: train_loss 2.0377, val loss 2.0117\n",
      "step 10000 of 100000: train_loss 2.0274, val loss 1.9854\n",
      "step 10100 of 100000: train_loss 2.0311, val loss 2.0123\n",
      "step 10200 of 100000: train_loss 2.0086, val loss 1.9948\n",
      "step 10300 of 100000: train_loss 2.0069, val loss 2.0205\n",
      "step 10400 of 100000: train_loss 2.0398, val loss 1.9898\n",
      "step 10500 of 100000: train_loss 1.9987, val loss 1.9932\n",
      "step 10600 of 100000: train_loss 2.0272, val loss 2.0318\n",
      "step 10700 of 100000: train_loss 2.0342, val loss 2.0095\n",
      "step 10800 of 100000: train_loss 2.0172, val loss 1.9935\n",
      "step 10900 of 100000: train_loss 2.0302, val loss 1.9681\n",
      "step 11000 of 100000: train_loss 2.0219, val loss 2.0112\n",
      "step 11100 of 100000: train_loss 2.0089, val loss 2.0162\n",
      "step 11200 of 100000: train_loss 2.0092, val loss 2.0084\n",
      "step 11300 of 100000: train_loss 2.0177, val loss 1.9965\n",
      "step 11400 of 100000: train_loss 2.0237, val loss 1.9952\n",
      "step 11500 of 100000: train_loss 2.0225, val loss 2.0037\n",
      "step 11600 of 100000: train_loss 1.9905, val loss 2.0019\n",
      "step 11700 of 100000: train_loss 2.0114, val loss 2.0049\n",
      "step 11800 of 100000: train_loss 2.0306, val loss 1.9954\n",
      "step 11900 of 100000: train_loss 2.0078, val loss 1.9970\n",
      "step 12000 of 100000: train_loss 2.0256, val loss 2.0049\n",
      "step 12100 of 100000: train_loss 2.0134, val loss 1.9967\n",
      "step 12200 of 100000: train_loss 2.0247, val loss 1.9997\n",
      "step 12300 of 100000: train_loss 2.0132, val loss 2.0398\n",
      "step 12400 of 100000: train_loss 2.0036, val loss 1.9976\n",
      "step 12500 of 100000: train_loss 2.0235, val loss 2.0094\n",
      "step 12600 of 100000: train_loss 2.0103, val loss 1.9996\n",
      "step 12700 of 100000: train_loss 2.0123, val loss 1.9859\n",
      "step 12800 of 100000: train_loss 2.0281, val loss 1.9753\n",
      "step 12900 of 100000: train_loss 2.0237, val loss 1.9972\n",
      "step 13000 of 100000: train_loss 2.0051, val loss 2.0181\n",
      "step 13100 of 100000: train_loss 2.0084, val loss 2.0095\n",
      "step 13200 of 100000: train_loss 2.0251, val loss 2.0067\n",
      "step 13300 of 100000: train_loss 2.0067, val loss 2.0096\n",
      "step 13400 of 100000: train_loss 2.0186, val loss 1.9852\n",
      "step 13500 of 100000: train_loss 2.0273, val loss 2.0012\n",
      "step 13600 of 100000: train_loss 2.0178, val loss 2.0086\n",
      "step 13700 of 100000: train_loss 2.0524, val loss 2.0106\n",
      "step 13800 of 100000: train_loss 1.9972, val loss 2.0051\n",
      "step 13900 of 100000: train_loss 2.0040, val loss 2.0124\n",
      "step 14000 of 100000: train_loss 2.0105, val loss 2.0139\n",
      "step 14100 of 100000: train_loss 2.0228, val loss 1.9863\n",
      "step 14200 of 100000: train_loss 2.0109, val loss 2.0048\n",
      "step 14300 of 100000: train_loss 2.0286, val loss 2.0064\n",
      "step 14400 of 100000: train_loss 2.0100, val loss 1.9984\n",
      "step 14500 of 100000: train_loss 2.0118, val loss 2.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14600 of 100000: train_loss 2.0208, val loss 2.0133\n",
      "step 14700 of 100000: train_loss 2.0081, val loss 2.0136\n",
      "step 14800 of 100000: train_loss 2.0161, val loss 2.0166\n",
      "step 14900 of 100000: train_loss 2.0155, val loss 1.9956\n",
      "step 15000 of 100000: train_loss 2.0125, val loss 2.0126\n",
      "step 15100 of 100000: train_loss 2.0233, val loss 2.0154\n",
      "step 15200 of 100000: train_loss 2.0146, val loss 1.9905\n",
      "step 15300 of 100000: train_loss 2.0097, val loss 2.0055\n",
      "step 15400 of 100000: train_loss 2.0236, val loss 2.0288\n",
      "step 15500 of 100000: train_loss 2.0236, val loss 1.9940\n",
      "step 15600 of 100000: train_loss 2.0004, val loss 2.0122\n",
      "step 15700 of 100000: train_loss 2.0463, val loss 2.0075\n",
      "step 15800 of 100000: train_loss 2.0173, val loss 1.9942\n",
      "step 15900 of 100000: train_loss 2.0056, val loss 2.0173\n",
      "step 16000 of 100000: train_loss 2.0072, val loss 2.0022\n",
      "step 16100 of 100000: train_loss 1.9933, val loss 1.9943\n",
      "step 16200 of 100000: train_loss 2.0218, val loss 2.0131\n",
      "step 16300 of 100000: train_loss 2.0008, val loss 2.0071\n",
      "step 16400 of 100000: train_loss 2.0321, val loss 2.0079\n",
      "step 16500 of 100000: train_loss 2.0186, val loss 2.0182\n",
      "step 16600 of 100000: train_loss 2.0142, val loss 1.9983\n",
      "step 16700 of 100000: train_loss 2.0283, val loss 2.0093\n",
      "step 16800 of 100000: train_loss 2.0089, val loss 2.0092\n",
      "step 16900 of 100000: train_loss 2.0291, val loss 2.0048\n",
      "step 17000 of 100000: train_loss 2.0135, val loss 2.0088\n",
      "step 17100 of 100000: train_loss 2.0128, val loss 2.0293\n",
      "step 17200 of 100000: train_loss 2.0218, val loss 2.0145\n",
      "step 17300 of 100000: train_loss 2.0215, val loss 2.0123\n",
      "step 17400 of 100000: train_loss 2.0061, val loss 2.0008\n",
      "step 17500 of 100000: train_loss 2.0380, val loss 2.0335\n",
      "step 17600 of 100000: train_loss 2.0132, val loss 1.9861\n",
      "step 17700 of 100000: train_loss 2.0146, val loss 2.0182\n",
      "step 17800 of 100000: train_loss 2.0314, val loss 2.0053\n",
      "step 17900 of 100000: train_loss 2.0140, val loss 2.0090\n",
      "step 18000 of 100000: train_loss 2.0265, val loss 2.0088\n",
      "step 18100 of 100000: train_loss 2.0131, val loss 2.0059\n",
      "step 18200 of 100000: train_loss 2.0149, val loss 2.0113\n",
      "step 18300 of 100000: train_loss 2.0292, val loss 2.0011\n",
      "step 18400 of 100000: train_loss 2.0142, val loss 2.0120\n",
      "step 18500 of 100000: train_loss 2.0106, val loss 1.9979\n",
      "step 18600 of 100000: train_loss 2.0149, val loss 1.9935\n",
      "step 18700 of 100000: train_loss 2.0107, val loss 2.0001\n",
      "step 18800 of 100000: train_loss 2.0112, val loss 1.9929\n",
      "step 18900 of 100000: train_loss 2.0244, val loss 1.9860\n",
      "step 19000 of 100000: train_loss 1.9939, val loss 2.0126\n",
      "step 19100 of 100000: train_loss 2.0045, val loss 2.0082\n",
      "step 19200 of 100000: train_loss 2.0103, val loss 2.0077\n",
      "step 19300 of 100000: train_loss 2.0229, val loss 2.0160\n",
      "step 19400 of 100000: train_loss 2.0186, val loss 2.0106\n",
      "step 19500 of 100000: train_loss 2.0173, val loss 1.9792\n",
      "step 19600 of 100000: train_loss 2.0305, val loss 1.9890\n",
      "step 19700 of 100000: train_loss 2.0082, val loss 2.0023\n",
      "step 19800 of 100000: train_loss 2.0135, val loss 2.0102\n",
      "step 19900 of 100000: train_loss 2.0490, val loss 2.0055\n",
      "step 20000 of 100000: train_loss 2.0143, val loss 1.9749\n",
      "step 20100 of 100000: train_loss 2.0088, val loss 1.9878\n",
      "step 20200 of 100000: train_loss 2.0200, val loss 2.0079\n",
      "step 20300 of 100000: train_loss 2.0186, val loss 2.0107\n",
      "step 20400 of 100000: train_loss 2.0258, val loss 2.0136\n",
      "step 20500 of 100000: train_loss 2.0196, val loss 2.0230\n",
      "step 20600 of 100000: train_loss 2.0383, val loss 2.0194\n",
      "step 20700 of 100000: train_loss 2.0284, val loss 2.0244\n",
      "step 20800 of 100000: train_loss 2.0420, val loss 2.0073\n",
      "step 20900 of 100000: train_loss 2.0301, val loss 2.0015\n",
      "step 21000 of 100000: train_loss 2.0219, val loss 2.0042\n",
      "step 21100 of 100000: train_loss 2.0141, val loss 2.0040\n",
      "step 21200 of 100000: train_loss 2.0199, val loss 2.0474\n",
      "step 21300 of 100000: train_loss 2.0202, val loss 1.9935\n",
      "step 21400 of 100000: train_loss 2.0285, val loss 2.0483\n",
      "step 21500 of 100000: train_loss 2.0193, val loss 2.0222\n",
      "step 21600 of 100000: train_loss 2.0276, val loss 2.0194\n",
      "step 21700 of 100000: train_loss 2.0383, val loss 2.0190\n",
      "step 21800 of 100000: train_loss 2.0258, val loss 2.0304\n",
      "step 21900 of 100000: train_loss 2.0153, val loss 2.0121\n",
      "step 22000 of 100000: train_loss 2.0161, val loss 2.0228\n",
      "step 22100 of 100000: train_loss 2.0120, val loss 2.0131\n",
      "step 22200 of 100000: train_loss 2.0000, val loss 1.9894\n",
      "step 22300 of 100000: train_loss 2.0192, val loss 2.0344\n",
      "step 22400 of 100000: train_loss 2.0191, val loss 2.0178\n",
      "step 22500 of 100000: train_loss 2.0101, val loss 1.9894\n",
      "step 22600 of 100000: train_loss 2.0143, val loss 1.9862\n",
      "step 22700 of 100000: train_loss 2.0142, val loss 2.0190\n",
      "step 22800 of 100000: train_loss 2.0150, val loss 2.0111\n",
      "step 22900 of 100000: train_loss 2.0053, val loss 1.9954\n",
      "step 23000 of 100000: train_loss 2.0119, val loss 2.0026\n",
      "step 23100 of 100000: train_loss 2.0189, val loss 2.0324\n",
      "step 23200 of 100000: train_loss 2.0264, val loss 2.0023\n",
      "step 23300 of 100000: train_loss 2.0224, val loss 1.9898\n",
      "step 23400 of 100000: train_loss 1.9913, val loss 2.0014\n",
      "step 23500 of 100000: train_loss 2.0215, val loss 1.9982\n",
      "step 23600 of 100000: train_loss 2.0051, val loss 1.9975\n",
      "step 23700 of 100000: train_loss 2.0201, val loss 2.0045\n",
      "step 23800 of 100000: train_loss 2.0287, val loss 2.0127\n",
      "step 23900 of 100000: train_loss 2.0162, val loss 2.0059\n",
      "step 24000 of 100000: train_loss 2.0209, val loss 2.0103\n",
      "step 24100 of 100000: train_loss 2.0147, val loss 2.0044\n",
      "step 24200 of 100000: train_loss 2.0286, val loss 1.9992\n",
      "step 24300 of 100000: train_loss 2.0318, val loss 2.0157\n",
      "step 24400 of 100000: train_loss 2.0292, val loss 1.9951\n",
      "step 24500 of 100000: train_loss 2.0012, val loss 2.0051\n",
      "step 24600 of 100000: train_loss 2.0215, val loss 2.0206\n",
      "step 24700 of 100000: train_loss 2.0265, val loss 2.0074\n",
      "step 24800 of 100000: train_loss 2.0133, val loss 1.9977\n",
      "step 24900 of 100000: train_loss 2.0122, val loss 2.0061\n",
      "step 25000 of 100000: train_loss 2.0227, val loss 2.0010\n",
      "step 25100 of 100000: train_loss 2.0159, val loss 2.0255\n",
      "step 25200 of 100000: train_loss 1.9955, val loss 2.0047\n",
      "step 25300 of 100000: train_loss 2.0190, val loss 2.0075\n",
      "step 25400 of 100000: train_loss 2.0108, val loss 2.0019\n",
      "step 25500 of 100000: train_loss 2.0156, val loss 1.9889\n",
      "step 25600 of 100000: train_loss 2.0285, val loss 2.0135\n",
      "step 25700 of 100000: train_loss 2.0255, val loss 1.9875\n",
      "step 25800 of 100000: train_loss 2.0201, val loss 1.9797\n",
      "step 25900 of 100000: train_loss 2.0386, val loss 2.0037\n",
      "step 26000 of 100000: train_loss 2.0075, val loss 2.0156\n",
      "step 26100 of 100000: train_loss 2.0293, val loss 2.0056\n",
      "step 26200 of 100000: train_loss 2.0245, val loss 2.0207\n",
      "step 26300 of 100000: train_loss 2.0223, val loss 2.0007\n",
      "step 26400 of 100000: train_loss 2.0226, val loss 2.0043\n",
      "step 26500 of 100000: train_loss 2.0134, val loss 2.0107\n",
      "step 26600 of 100000: train_loss 2.0393, val loss 1.9974\n",
      "step 26700 of 100000: train_loss 2.0524, val loss 1.9944\n",
      "step 26800 of 100000: train_loss 2.0214, val loss 2.0112\n",
      "step 26900 of 100000: train_loss 2.0171, val loss 2.0106\n",
      "step 27000 of 100000: train_loss 2.0345, val loss 2.0099\n",
      "step 27100 of 100000: train_loss 2.0276, val loss 1.9953\n",
      "step 27200 of 100000: train_loss 2.0332, val loss 2.0007\n",
      "step 27300 of 100000: train_loss 2.0116, val loss 1.9939\n",
      "step 27400 of 100000: train_loss 2.0353, val loss 2.0043\n",
      "step 27500 of 100000: train_loss 2.0110, val loss 2.0098\n",
      "step 27600 of 100000: train_loss 2.0134, val loss 2.0059\n",
      "step 27700 of 100000: train_loss 2.0354, val loss 1.9945\n",
      "step 27800 of 100000: train_loss 2.0125, val loss 1.9907\n",
      "step 27900 of 100000: train_loss 2.0069, val loss 2.0237\n",
      "step 28000 of 100000: train_loss 1.9927, val loss 2.0233\n",
      "step 28100 of 100000: train_loss 2.0123, val loss 1.9950\n",
      "step 28200 of 100000: train_loss 2.0095, val loss 1.9916\n",
      "step 28300 of 100000: train_loss 2.0362, val loss 2.0174\n",
      "step 28400 of 100000: train_loss 2.0019, val loss 1.9964\n",
      "step 28500 of 100000: train_loss 2.0155, val loss 2.0023\n",
      "step 28600 of 100000: train_loss 2.0161, val loss 2.0093\n",
      "step 28700 of 100000: train_loss 2.0031, val loss 2.0365\n",
      "step 28800 of 100000: train_loss 2.0221, val loss 1.9946\n",
      "step 28900 of 100000: train_loss 2.0200, val loss 1.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29000 of 100000: train_loss 2.0093, val loss 1.9925\n",
      "step 29100 of 100000: train_loss 2.0064, val loss 2.0138\n",
      "step 29200 of 100000: train_loss 2.0136, val loss 2.0162\n",
      "step 29300 of 100000: train_loss 2.0185, val loss 2.0090\n",
      "step 29400 of 100000: train_loss 2.0177, val loss 2.0023\n",
      "step 29500 of 100000: train_loss 2.0074, val loss 2.0031\n",
      "step 29600 of 100000: train_loss 2.0427, val loss 2.0172\n",
      "step 29700 of 100000: train_loss 2.0260, val loss 1.9842\n",
      "step 29800 of 100000: train_loss 2.0348, val loss 1.9975\n",
      "step 29900 of 100000: train_loss 2.0060, val loss 2.0225\n",
      "step 30000 of 100000: train_loss 2.0227, val loss 2.0028\n",
      "step 30100 of 100000: train_loss 2.0387, val loss 1.9922\n",
      "step 30200 of 100000: train_loss 2.0175, val loss 1.9904\n",
      "step 30300 of 100000: train_loss 2.0191, val loss 2.0154\n",
      "step 30400 of 100000: train_loss 2.0003, val loss 2.0100\n",
      "step 30500 of 100000: train_loss 2.0092, val loss 2.0282\n",
      "step 30600 of 100000: train_loss 2.0109, val loss 2.0042\n",
      "step 30700 of 100000: train_loss 1.9983, val loss 1.9999\n",
      "step 30800 of 100000: train_loss 2.0021, val loss 2.0079\n",
      "step 30900 of 100000: train_loss 2.0385, val loss 1.9937\n",
      "step 31000 of 100000: train_loss 2.0274, val loss 1.9961\n",
      "step 31100 of 100000: train_loss 2.0143, val loss 1.9942\n",
      "step 31200 of 100000: train_loss 2.0159, val loss 1.9840\n",
      "step 31300 of 100000: train_loss 1.9860, val loss 1.9899\n",
      "step 31400 of 100000: train_loss 2.0189, val loss 2.0042\n",
      "step 31500 of 100000: train_loss 2.0184, val loss 2.0161\n",
      "step 31600 of 100000: train_loss 2.0278, val loss 2.0084\n",
      "step 31700 of 100000: train_loss 2.0381, val loss 1.9830\n",
      "step 31800 of 100000: train_loss 2.0167, val loss 2.0148\n",
      "step 31900 of 100000: train_loss 2.0230, val loss 2.0247\n",
      "step 32000 of 100000: train_loss 1.9925, val loss 1.9922\n",
      "step 32100 of 100000: train_loss 2.0312, val loss 2.0176\n",
      "step 32200 of 100000: train_loss 2.0085, val loss 1.9947\n",
      "step 32300 of 100000: train_loss 1.9914, val loss 2.0089\n",
      "step 32400 of 100000: train_loss 2.0054, val loss 1.9973\n",
      "step 32500 of 100000: train_loss 2.0110, val loss 2.0006\n",
      "step 32600 of 100000: train_loss 2.0230, val loss 2.0193\n",
      "step 32700 of 100000: train_loss 2.0147, val loss 2.0123\n",
      "step 32800 of 100000: train_loss 2.0287, val loss 2.0281\n",
      "step 32900 of 100000: train_loss 2.0152, val loss 1.9934\n",
      "step 33000 of 100000: train_loss 1.9915, val loss 2.0067\n",
      "step 33100 of 100000: train_loss 1.9967, val loss 1.9922\n",
      "step 33200 of 100000: train_loss 2.0145, val loss 2.0127\n",
      "step 33300 of 100000: train_loss 2.0117, val loss 1.9982\n",
      "step 33400 of 100000: train_loss 2.0010, val loss 2.0145\n",
      "step 33500 of 100000: train_loss 2.0201, val loss 1.9904\n",
      "step 33600 of 100000: train_loss 2.0048, val loss 2.0050\n",
      "step 33700 of 100000: train_loss 2.0137, val loss 2.0090\n",
      "step 33800 of 100000: train_loss 2.0098, val loss 2.0185\n",
      "step 33900 of 100000: train_loss 2.0066, val loss 2.0052\n",
      "step 34000 of 100000: train_loss 2.0161, val loss 2.0177\n",
      "step 34100 of 100000: train_loss 2.0042, val loss 2.0014\n",
      "step 34200 of 100000: train_loss 2.0012, val loss 1.9805\n",
      "step 34300 of 100000: train_loss 2.0310, val loss 2.0026\n",
      "step 34400 of 100000: train_loss 2.0120, val loss 2.0099\n",
      "step 34500 of 100000: train_loss 2.0135, val loss 2.0266\n",
      "step 34600 of 100000: train_loss 2.0140, val loss 1.9858\n",
      "step 34700 of 100000: train_loss 2.0064, val loss 1.9931\n",
      "step 34800 of 100000: train_loss 2.0187, val loss 2.0212\n",
      "step 34900 of 100000: train_loss 2.0111, val loss 2.0127\n",
      "step 35000 of 100000: train_loss 1.9961, val loss 2.0242\n",
      "step 35100 of 100000: train_loss 2.0249, val loss 1.9907\n",
      "step 35200 of 100000: train_loss 2.0177, val loss 2.0074\n",
      "step 35300 of 100000: train_loss 2.0290, val loss 2.0080\n",
      "step 35400 of 100000: train_loss 2.0246, val loss 2.0031\n",
      "step 35500 of 100000: train_loss 2.0392, val loss 2.0142\n",
      "step 35600 of 100000: train_loss 2.0076, val loss 1.9962\n",
      "step 35700 of 100000: train_loss 2.0195, val loss 2.0300\n",
      "step 35800 of 100000: train_loss 2.0388, val loss 2.0216\n",
      "step 35900 of 100000: train_loss 2.0104, val loss 2.0154\n",
      "step 36000 of 100000: train_loss 2.0257, val loss 2.0164\n",
      "step 36100 of 100000: train_loss 2.0118, val loss 1.9982\n",
      "step 36200 of 100000: train_loss 2.0269, val loss 1.9877\n",
      "step 36300 of 100000: train_loss 2.0190, val loss 2.0033\n",
      "step 36400 of 100000: train_loss 2.0162, val loss 2.0060\n",
      "step 36500 of 100000: train_loss 2.0204, val loss 1.9862\n",
      "step 36600 of 100000: train_loss 2.0126, val loss 1.9956\n",
      "step 36700 of 100000: train_loss 1.9999, val loss 1.9940\n",
      "step 36800 of 100000: train_loss 2.0132, val loss 2.0027\n",
      "step 36900 of 100000: train_loss 2.0159, val loss 2.0006\n",
      "step 37000 of 100000: train_loss 2.0112, val loss 2.0041\n",
      "step 37100 of 100000: train_loss 2.0253, val loss 2.0056\n",
      "step 37200 of 100000: train_loss 2.0145, val loss 2.0166\n",
      "step 37300 of 100000: train_loss 2.0357, val loss 1.9987\n",
      "step 37400 of 100000: train_loss 2.0268, val loss 1.9993\n",
      "step 37500 of 100000: train_loss 2.0095, val loss 1.9993\n",
      "step 37600 of 100000: train_loss 2.0095, val loss 2.0157\n",
      "step 37700 of 100000: train_loss 2.0288, val loss 2.0039\n",
      "step 37800 of 100000: train_loss 1.9916, val loss 1.9940\n",
      "step 37900 of 100000: train_loss 2.0141, val loss 2.0074\n",
      "step 38000 of 100000: train_loss 2.0121, val loss 1.9917\n",
      "step 38100 of 100000: train_loss 2.0123, val loss 1.9879\n",
      "step 38200 of 100000: train_loss 2.0124, val loss 2.0028\n",
      "step 38300 of 100000: train_loss 2.0128, val loss 1.9966\n",
      "step 38400 of 100000: train_loss 2.0250, val loss 2.0048\n",
      "step 38500 of 100000: train_loss 2.0169, val loss 2.0165\n",
      "step 38600 of 100000: train_loss 2.0330, val loss 1.9938\n",
      "step 38700 of 100000: train_loss 2.0160, val loss 2.0036\n",
      "step 38800 of 100000: train_loss 2.0353, val loss 1.9964\n",
      "step 38900 of 100000: train_loss 2.0189, val loss 2.0119\n",
      "step 39000 of 100000: train_loss 2.0147, val loss 1.9970\n",
      "step 39100 of 100000: train_loss 2.0306, val loss 2.0092\n",
      "step 39200 of 100000: train_loss 2.0244, val loss 2.0025\n",
      "step 39300 of 100000: train_loss 2.0195, val loss 2.0060\n",
      "step 39400 of 100000: train_loss 2.0268, val loss 1.9862\n",
      "step 39500 of 100000: train_loss 2.0283, val loss 2.0090\n",
      "step 39600 of 100000: train_loss 2.0229, val loss 1.9842\n",
      "step 39700 of 100000: train_loss 2.0115, val loss 2.0173\n",
      "step 39800 of 100000: train_loss 2.0204, val loss 2.0096\n",
      "step 39900 of 100000: train_loss 2.0359, val loss 2.0163\n",
      "step 40000 of 100000: train_loss 1.9979, val loss 2.0045\n",
      "step 40100 of 100000: train_loss 2.0181, val loss 2.0151\n",
      "step 40200 of 100000: train_loss 2.0112, val loss 2.0067\n",
      "step 40300 of 100000: train_loss 2.0160, val loss 2.0089\n",
      "step 40400 of 100000: train_loss 2.0113, val loss 2.0200\n",
      "step 40500 of 100000: train_loss 2.0392, val loss 2.0052\n",
      "step 40600 of 100000: train_loss 2.0090, val loss 2.0086\n",
      "step 40700 of 100000: train_loss 2.0370, val loss 2.0054\n",
      "step 40800 of 100000: train_loss 2.0212, val loss 2.0126\n",
      "step 40900 of 100000: train_loss 2.0139, val loss 2.0073\n",
      "step 41000 of 100000: train_loss 2.0066, val loss 2.0055\n",
      "step 41100 of 100000: train_loss 2.0312, val loss 1.9945\n",
      "step 41200 of 100000: train_loss 2.0227, val loss 2.0106\n",
      "step 41300 of 100000: train_loss 2.0170, val loss 2.0173\n",
      "step 41400 of 100000: train_loss 2.0285, val loss 1.9854\n",
      "step 41500 of 100000: train_loss 2.0251, val loss 1.9960\n",
      "step 41600 of 100000: train_loss 2.0303, val loss 1.9870\n",
      "step 41700 of 100000: train_loss 1.9981, val loss 1.9985\n",
      "step 41800 of 100000: train_loss 2.0114, val loss 2.0319\n",
      "step 41900 of 100000: train_loss 2.0248, val loss 2.0042\n",
      "step 42000 of 100000: train_loss 2.0198, val loss 1.9874\n",
      "step 42100 of 100000: train_loss 2.0419, val loss 2.0153\n",
      "step 42200 of 100000: train_loss 2.0171, val loss 2.0052\n",
      "step 42300 of 100000: train_loss 2.0149, val loss 1.9988\n",
      "step 42400 of 100000: train_loss 2.0093, val loss 1.9955\n",
      "step 42500 of 100000: train_loss 2.0272, val loss 2.0024\n",
      "step 42600 of 100000: train_loss 2.0044, val loss 2.0113\n",
      "step 42700 of 100000: train_loss 2.0422, val loss 1.9962\n",
      "step 42800 of 100000: train_loss 2.0146, val loss 2.0100\n",
      "step 42900 of 100000: train_loss 2.0046, val loss 1.9925\n",
      "step 43000 of 100000: train_loss 1.9937, val loss 2.0190\n",
      "step 43100 of 100000: train_loss 2.0199, val loss 1.9990\n",
      "step 43200 of 100000: train_loss 2.0382, val loss 2.0081\n",
      "step 43300 of 100000: train_loss 2.0305, val loss 2.0324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 43400 of 100000: train_loss 2.0130, val loss 2.0100\n",
      "step 43500 of 100000: train_loss 1.9999, val loss 2.0065\n",
      "step 43600 of 100000: train_loss 2.0133, val loss 2.0020\n",
      "step 43700 of 100000: train_loss 2.0255, val loss 2.0029\n",
      "step 43800 of 100000: train_loss 2.0180, val loss 2.0102\n",
      "step 43900 of 100000: train_loss 2.0207, val loss 2.0011\n",
      "step 44000 of 100000: train_loss 2.0223, val loss 2.0151\n",
      "step 44100 of 100000: train_loss 2.0108, val loss 2.0060\n",
      "step 44200 of 100000: train_loss 2.0096, val loss 2.0131\n",
      "step 44300 of 100000: train_loss 2.0168, val loss 2.0032\n",
      "step 44400 of 100000: train_loss 2.0026, val loss 2.0039\n",
      "step 44500 of 100000: train_loss 2.0129, val loss 2.0122\n",
      "step 44600 of 100000: train_loss 2.0392, val loss 1.9920\n",
      "step 44700 of 100000: train_loss 2.0100, val loss 1.9896\n",
      "step 44800 of 100000: train_loss 2.0274, val loss 1.9873\n",
      "step 44900 of 100000: train_loss 2.0329, val loss 2.0149\n",
      "step 45000 of 100000: train_loss 1.9935, val loss 2.0152\n",
      "step 45100 of 100000: train_loss 2.0157, val loss 2.0217\n",
      "step 45200 of 100000: train_loss 2.0147, val loss 1.9976\n",
      "step 45300 of 100000: train_loss 2.0098, val loss 2.0008\n",
      "step 45400 of 100000: train_loss 2.0178, val loss 2.0220\n",
      "step 45500 of 100000: train_loss 2.0256, val loss 2.0192\n",
      "step 45600 of 100000: train_loss 2.0009, val loss 1.9981\n",
      "step 45700 of 100000: train_loss 1.9950, val loss 1.9964\n",
      "step 45800 of 100000: train_loss 1.9912, val loss 1.9775\n",
      "step 45900 of 100000: train_loss 2.0119, val loss 1.9935\n",
      "step 46000 of 100000: train_loss 2.0311, val loss 2.0187\n",
      "step 46100 of 100000: train_loss 2.0135, val loss 2.0148\n",
      "step 46200 of 100000: train_loss 2.0219, val loss 1.9920\n",
      "step 46300 of 100000: train_loss 2.0175, val loss 1.9974\n",
      "step 46400 of 100000: train_loss 2.0360, val loss 1.9962\n",
      "step 46500 of 100000: train_loss 2.0209, val loss 1.9901\n",
      "step 46600 of 100000: train_loss 2.0353, val loss 2.0082\n",
      "step 46700 of 100000: train_loss 2.0223, val loss 1.9889\n",
      "step 46800 of 100000: train_loss 2.0113, val loss 1.9981\n",
      "step 46900 of 100000: train_loss 2.0179, val loss 1.9912\n",
      "step 47000 of 100000: train_loss 2.0374, val loss 2.0137\n",
      "step 47100 of 100000: train_loss 2.0019, val loss 1.9933\n",
      "step 47200 of 100000: train_loss 2.0174, val loss 2.0035\n",
      "step 47300 of 100000: train_loss 2.0035, val loss 2.0056\n",
      "step 47400 of 100000: train_loss 2.0376, val loss 2.0036\n",
      "step 47500 of 100000: train_loss 2.0143, val loss 1.9847\n",
      "step 47600 of 100000: train_loss 2.0118, val loss 1.9967\n",
      "step 47700 of 100000: train_loss 2.0293, val loss 1.9964\n",
      "step 47800 of 100000: train_loss 2.0136, val loss 1.9981\n",
      "step 47900 of 100000: train_loss 2.0215, val loss 2.0074\n",
      "step 48000 of 100000: train_loss 2.0026, val loss 2.0114\n",
      "step 48100 of 100000: train_loss 2.0064, val loss 2.0008\n",
      "step 48200 of 100000: train_loss 2.0144, val loss 2.0035\n",
      "step 48300 of 100000: train_loss 2.0156, val loss 2.0074\n",
      "step 48400 of 100000: train_loss 2.0395, val loss 2.0295\n",
      "step 48500 of 100000: train_loss 2.0151, val loss 1.9942\n",
      "step 48600 of 100000: train_loss 2.0174, val loss 2.0075\n",
      "step 48700 of 100000: train_loss 1.9914, val loss 2.0149\n",
      "step 48800 of 100000: train_loss 2.0228, val loss 2.0034\n",
      "step 48900 of 100000: train_loss 2.0389, val loss 2.0250\n",
      "step 49000 of 100000: train_loss 2.0078, val loss 2.0129\n",
      "step 49100 of 100000: train_loss 2.0058, val loss 1.9950\n",
      "step 49200 of 100000: train_loss 2.0115, val loss 2.0002\n",
      "step 49300 of 100000: train_loss 2.0479, val loss 1.9925\n",
      "step 49400 of 100000: train_loss 2.0198, val loss 2.0062\n",
      "step 49500 of 100000: train_loss 2.0166, val loss 2.0324\n",
      "step 49600 of 100000: train_loss 2.0045, val loss 2.0133\n",
      "step 49700 of 100000: train_loss 2.0122, val loss 2.0004\n",
      "step 49800 of 100000: train_loss 2.0157, val loss 2.0086\n",
      "step 49900 of 100000: train_loss 2.0201, val loss 1.9949\n",
      "step 50000 of 100000: train_loss 2.0006, val loss 1.9974\n",
      "step 50100 of 100000: train_loss 2.0132, val loss 1.9931\n",
      "step 50200 of 100000: train_loss 2.0241, val loss 2.0061\n",
      "step 50300 of 100000: train_loss 2.0206, val loss 2.0136\n",
      "step 50400 of 100000: train_loss 2.0270, val loss 1.9909\n",
      "step 50500 of 100000: train_loss 2.0022, val loss 2.0035\n",
      "step 50600 of 100000: train_loss 2.0276, val loss 2.0185\n",
      "step 50700 of 100000: train_loss 2.0218, val loss 2.0055\n",
      "step 50800 of 100000: train_loss 2.0336, val loss 2.0047\n",
      "step 50900 of 100000: train_loss 2.0074, val loss 2.0169\n",
      "step 51000 of 100000: train_loss 2.0051, val loss 2.0068\n",
      "step 51100 of 100000: train_loss 2.0130, val loss 1.9960\n",
      "step 51200 of 100000: train_loss 1.9969, val loss 2.0140\n",
      "step 51300 of 100000: train_loss 2.0016, val loss 2.0092\n",
      "step 51400 of 100000: train_loss 2.0306, val loss 2.0291\n",
      "step 51500 of 100000: train_loss 2.0161, val loss 1.9993\n",
      "step 51600 of 100000: train_loss 2.0144, val loss 2.0046\n",
      "step 51700 of 100000: train_loss 2.0122, val loss 2.0147\n",
      "step 51800 of 100000: train_loss 2.0019, val loss 2.0019\n",
      "step 51900 of 100000: train_loss 2.0232, val loss 2.0110\n",
      "step 52000 of 100000: train_loss 2.0262, val loss 1.9983\n",
      "step 52100 of 100000: train_loss 2.0138, val loss 2.0295\n",
      "step 52200 of 100000: train_loss 2.0057, val loss 2.0121\n",
      "step 52300 of 100000: train_loss 2.0277, val loss 2.0017\n",
      "step 52400 of 100000: train_loss 2.0221, val loss 2.0186\n",
      "step 52500 of 100000: train_loss 2.0132, val loss 1.9951\n",
      "step 52600 of 100000: train_loss 2.0407, val loss 2.0095\n",
      "step 52700 of 100000: train_loss 2.0298, val loss 2.0064\n",
      "step 52800 of 100000: train_loss 2.0188, val loss 2.0013\n",
      "step 52900 of 100000: train_loss 2.0385, val loss 2.0101\n",
      "step 53000 of 100000: train_loss 2.0111, val loss 1.9976\n",
      "step 53100 of 100000: train_loss 2.0184, val loss 2.0098\n",
      "step 53200 of 100000: train_loss 2.0212, val loss 2.0230\n",
      "step 53300 of 100000: train_loss 2.0089, val loss 1.9924\n",
      "step 53400 of 100000: train_loss 2.0208, val loss 2.0038\n",
      "step 53500 of 100000: train_loss 2.0126, val loss 2.0193\n",
      "step 53600 of 100000: train_loss 2.0085, val loss 1.9941\n",
      "step 53700 of 100000: train_loss 2.0213, val loss 1.9986\n",
      "step 53800 of 100000: train_loss 2.0266, val loss 2.0155\n",
      "step 53900 of 100000: train_loss 2.0249, val loss 1.9990\n",
      "step 54000 of 100000: train_loss 2.0224, val loss 2.0009\n",
      "step 54100 of 100000: train_loss 2.0023, val loss 2.0046\n",
      "step 54200 of 100000: train_loss 2.0207, val loss 1.9923\n",
      "step 54300 of 100000: train_loss 2.0174, val loss 2.0054\n",
      "step 54400 of 100000: train_loss 1.9941, val loss 2.0022\n",
      "step 54500 of 100000: train_loss 2.0182, val loss 2.0049\n",
      "step 54600 of 100000: train_loss 2.0170, val loss 2.0069\n",
      "step 54700 of 100000: train_loss 2.0198, val loss 2.0112\n",
      "step 54800 of 100000: train_loss 2.0206, val loss 1.9874\n",
      "step 54900 of 100000: train_loss 2.0113, val loss 2.0136\n",
      "step 55000 of 100000: train_loss 2.0336, val loss 2.0247\n",
      "step 55100 of 100000: train_loss 2.0250, val loss 2.0085\n",
      "step 55200 of 100000: train_loss 2.0186, val loss 2.0069\n",
      "step 55300 of 100000: train_loss 2.0217, val loss 1.9958\n",
      "step 55400 of 100000: train_loss 2.0306, val loss 1.9936\n",
      "step 55500 of 100000: train_loss 2.0111, val loss 2.0338\n",
      "step 55600 of 100000: train_loss 2.0225, val loss 1.9859\n",
      "step 55700 of 100000: train_loss 2.0114, val loss 2.0116\n",
      "step 55800 of 100000: train_loss 2.0170, val loss 2.0010\n",
      "step 55900 of 100000: train_loss 2.0257, val loss 1.9975\n",
      "step 56000 of 100000: train_loss 2.0243, val loss 2.0119\n",
      "step 56100 of 100000: train_loss 2.0211, val loss 2.0129\n",
      "step 56200 of 100000: train_loss 2.0449, val loss 2.0067\n",
      "step 56300 of 100000: train_loss 2.0132, val loss 2.0154\n",
      "step 56400 of 100000: train_loss 2.0072, val loss 2.0063\n",
      "step 56500 of 100000: train_loss 2.0090, val loss 1.9943\n",
      "step 56600 of 100000: train_loss 2.0244, val loss 2.0153\n",
      "step 56700 of 100000: train_loss 2.0235, val loss 1.9961\n",
      "step 56800 of 100000: train_loss 2.0085, val loss 2.0198\n",
      "step 56900 of 100000: train_loss 2.0291, val loss 2.0096\n",
      "step 57000 of 100000: train_loss 2.0237, val loss 2.0158\n",
      "step 57100 of 100000: train_loss 1.9972, val loss 1.9979\n",
      "step 57200 of 100000: train_loss 2.0132, val loss 1.9886\n",
      "step 57300 of 100000: train_loss 2.0196, val loss 2.0155\n",
      "step 57400 of 100000: train_loss 2.0243, val loss 1.9886\n",
      "step 57500 of 100000: train_loss 2.0146, val loss 1.9892\n",
      "step 57600 of 100000: train_loss 2.0314, val loss 2.0193\n",
      "step 57700 of 100000: train_loss 2.0343, val loss 2.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 57800 of 100000: train_loss 2.0135, val loss 1.9869\n",
      "step 57900 of 100000: train_loss 2.0424, val loss 1.9962\n",
      "step 58000 of 100000: train_loss 2.0076, val loss 2.0132\n",
      "step 58100 of 100000: train_loss 2.0059, val loss 2.0066\n",
      "step 58200 of 100000: train_loss 2.0235, val loss 1.9869\n",
      "step 58300 of 100000: train_loss 2.0002, val loss 2.0098\n",
      "step 58400 of 100000: train_loss 2.0154, val loss 2.0231\n",
      "step 58500 of 100000: train_loss 2.0082, val loss 1.9995\n",
      "step 58600 of 100000: train_loss 2.0227, val loss 1.9878\n",
      "step 58700 of 100000: train_loss 2.0229, val loss 2.0211\n",
      "step 58800 of 100000: train_loss 2.0181, val loss 2.0104\n",
      "step 58900 of 100000: train_loss 2.0113, val loss 2.0228\n",
      "step 59000 of 100000: train_loss 2.0226, val loss 2.0068\n",
      "step 59100 of 100000: train_loss 2.0331, val loss 2.0057\n",
      "step 59200 of 100000: train_loss 2.0296, val loss 1.9907\n",
      "step 59300 of 100000: train_loss 2.0298, val loss 2.0097\n",
      "step 59400 of 100000: train_loss 2.0251, val loss 2.0040\n",
      "step 59500 of 100000: train_loss 2.0278, val loss 1.9917\n",
      "step 59600 of 100000: train_loss 2.0457, val loss 1.9993\n",
      "step 59700 of 100000: train_loss 2.0192, val loss 2.0001\n",
      "step 59800 of 100000: train_loss 1.9984, val loss 2.0210\n",
      "step 59900 of 100000: train_loss 2.0294, val loss 2.0270\n",
      "step 60000 of 100000: train_loss 1.9922, val loss 2.0182\n",
      "step 60100 of 100000: train_loss 2.0382, val loss 2.0043\n",
      "step 60200 of 100000: train_loss 2.0226, val loss 2.0052\n",
      "step 60300 of 100000: train_loss 2.0174, val loss 1.9948\n",
      "step 60400 of 100000: train_loss 2.0149, val loss 1.9946\n",
      "step 60500 of 100000: train_loss 1.9976, val loss 2.0051\n",
      "step 60600 of 100000: train_loss 2.0039, val loss 2.0028\n",
      "step 60700 of 100000: train_loss 2.0236, val loss 2.0027\n",
      "step 60800 of 100000: train_loss 2.0211, val loss 2.0099\n",
      "step 60900 of 100000: train_loss 2.0121, val loss 2.0075\n",
      "step 61000 of 100000: train_loss 2.0316, val loss 2.0125\n",
      "step 61100 of 100000: train_loss 2.0238, val loss 1.9814\n",
      "step 61200 of 100000: train_loss 2.0140, val loss 1.9972\n",
      "step 61300 of 100000: train_loss 1.9985, val loss 2.0156\n",
      "step 61400 of 100000: train_loss 2.0102, val loss 2.0265\n",
      "step 61500 of 100000: train_loss 2.0213, val loss 2.0127\n",
      "step 61600 of 100000: train_loss 2.0379, val loss 1.9951\n",
      "step 61700 of 100000: train_loss 2.0180, val loss 2.0097\n",
      "step 61800 of 100000: train_loss 2.0444, val loss 2.0049\n",
      "step 61900 of 100000: train_loss 2.0072, val loss 2.0102\n",
      "step 62000 of 100000: train_loss 2.0258, val loss 2.0204\n",
      "step 62100 of 100000: train_loss 2.0186, val loss 2.0143\n",
      "step 62200 of 100000: train_loss 2.0149, val loss 1.9994\n",
      "step 62300 of 100000: train_loss 2.0224, val loss 2.0016\n",
      "step 62400 of 100000: train_loss 2.0467, val loss 1.9770\n",
      "step 62500 of 100000: train_loss 2.0059, val loss 1.9848\n",
      "step 62600 of 100000: train_loss 2.0171, val loss 2.0125\n",
      "step 62700 of 100000: train_loss 2.0274, val loss 1.9917\n",
      "step 62800 of 100000: train_loss 2.0211, val loss 2.0049\n",
      "step 62900 of 100000: train_loss 2.0446, val loss 2.0124\n",
      "step 63000 of 100000: train_loss 2.0362, val loss 2.0211\n",
      "step 63100 of 100000: train_loss 2.0125, val loss 2.0035\n",
      "step 63200 of 100000: train_loss 2.0022, val loss 2.0102\n",
      "step 63300 of 100000: train_loss 2.0234, val loss 1.9942\n",
      "step 63400 of 100000: train_loss 2.0419, val loss 1.9892\n",
      "step 63500 of 100000: train_loss 2.0188, val loss 2.0066\n",
      "step 63600 of 100000: train_loss 2.0121, val loss 2.0013\n",
      "step 63700 of 100000: train_loss 2.0073, val loss 2.0131\n",
      "step 63800 of 100000: train_loss 2.0118, val loss 2.0294\n",
      "step 63900 of 100000: train_loss 2.0066, val loss 2.0031\n",
      "step 64000 of 100000: train_loss 2.0240, val loss 1.9909\n",
      "step 64100 of 100000: train_loss 2.0112, val loss 2.0102\n",
      "step 64200 of 100000: train_loss 2.0093, val loss 1.9928\n",
      "step 64300 of 100000: train_loss 2.0009, val loss 2.0131\n",
      "step 64400 of 100000: train_loss 2.0335, val loss 2.0018\n",
      "step 64500 of 100000: train_loss 2.0184, val loss 2.0239\n",
      "step 64600 of 100000: train_loss 2.0143, val loss 2.0085\n",
      "step 64700 of 100000: train_loss 2.0099, val loss 1.9926\n",
      "step 64800 of 100000: train_loss 2.0288, val loss 2.0135\n",
      "step 64900 of 100000: train_loss 2.0144, val loss 2.0134\n",
      "step 65000 of 100000: train_loss 2.0147, val loss 2.0108\n",
      "step 65100 of 100000: train_loss 2.0163, val loss 1.9969\n",
      "step 65200 of 100000: train_loss 2.0110, val loss 2.0166\n",
      "step 65300 of 100000: train_loss 2.0333, val loss 2.0018\n",
      "step 65400 of 100000: train_loss 2.0303, val loss 2.0032\n",
      "step 65500 of 100000: train_loss 2.0196, val loss 1.9978\n",
      "step 65600 of 100000: train_loss 2.0233, val loss 2.0112\n",
      "step 65700 of 100000: train_loss 2.0250, val loss 2.0090\n",
      "step 65800 of 100000: train_loss 2.0135, val loss 2.0205\n",
      "step 65900 of 100000: train_loss 2.0343, val loss 1.9992\n",
      "step 66000 of 100000: train_loss 2.0248, val loss 2.0319\n",
      "step 66100 of 100000: train_loss 2.0137, val loss 1.9919\n",
      "step 66200 of 100000: train_loss 2.0272, val loss 2.0182\n",
      "step 66300 of 100000: train_loss 2.0193, val loss 1.9751\n",
      "step 66400 of 100000: train_loss 2.0042, val loss 1.9974\n",
      "step 66500 of 100000: train_loss 2.0110, val loss 1.9958\n",
      "step 66600 of 100000: train_loss 2.0173, val loss 2.0168\n",
      "step 66700 of 100000: train_loss 2.0188, val loss 1.9848\n",
      "step 66800 of 100000: train_loss 2.0192, val loss 2.0068\n",
      "step 66900 of 100000: train_loss 2.0004, val loss 2.0048\n",
      "step 67000 of 100000: train_loss 2.0140, val loss 2.0039\n",
      "step 67100 of 100000: train_loss 2.0411, val loss 1.9975\n",
      "step 67200 of 100000: train_loss 2.0040, val loss 2.0280\n",
      "step 67300 of 100000: train_loss 2.0328, val loss 2.0022\n",
      "step 67400 of 100000: train_loss 2.0311, val loss 2.0068\n",
      "step 67500 of 100000: train_loss 2.0121, val loss 1.9970\n",
      "step 67600 of 100000: train_loss 2.0130, val loss 1.9907\n",
      "step 67700 of 100000: train_loss 2.0207, val loss 2.0068\n",
      "step 67800 of 100000: train_loss 1.9973, val loss 2.0065\n",
      "step 67900 of 100000: train_loss 2.0175, val loss 1.9971\n",
      "step 68000 of 100000: train_loss 2.0136, val loss 1.9910\n",
      "step 68100 of 100000: train_loss 2.0163, val loss 1.9957\n",
      "step 68200 of 100000: train_loss 2.0046, val loss 1.9943\n",
      "step 68300 of 100000: train_loss 2.0264, val loss 2.0095\n",
      "step 68400 of 100000: train_loss 2.0183, val loss 2.0176\n",
      "step 68500 of 100000: train_loss 2.0204, val loss 2.0037\n",
      "step 68600 of 100000: train_loss 2.0166, val loss 2.0009\n",
      "step 68700 of 100000: train_loss 2.0160, val loss 1.9900\n",
      "step 68800 of 100000: train_loss 2.0079, val loss 2.0130\n",
      "step 68900 of 100000: train_loss 2.0225, val loss 1.9924\n",
      "step 69000 of 100000: train_loss 2.0372, val loss 2.0145\n",
      "step 69100 of 100000: train_loss 2.0210, val loss 2.0033\n",
      "step 69200 of 100000: train_loss 2.0262, val loss 1.9983\n",
      "step 69300 of 100000: train_loss 2.0292, val loss 2.0158\n",
      "step 69400 of 100000: train_loss 2.0194, val loss 2.0092\n",
      "step 69500 of 100000: train_loss 2.0056, val loss 2.0061\n",
      "step 69600 of 100000: train_loss 2.0213, val loss 2.0268\n",
      "step 69700 of 100000: train_loss 2.0341, val loss 2.0080\n",
      "step 69800 of 100000: train_loss 2.0140, val loss 2.0032\n",
      "step 69900 of 100000: train_loss 2.0147, val loss 2.0101\n",
      "step 70000 of 100000: train_loss 2.0242, val loss 2.0207\n",
      "step 70100 of 100000: train_loss 2.0142, val loss 2.0143\n",
      "step 70200 of 100000: train_loss 2.0144, val loss 2.0073\n",
      "step 70300 of 100000: train_loss 1.9944, val loss 2.0136\n",
      "step 70400 of 100000: train_loss 2.0140, val loss 2.0155\n",
      "step 70500 of 100000: train_loss 2.0140, val loss 2.0082\n",
      "step 70600 of 100000: train_loss 2.0175, val loss 2.0202\n",
      "step 70700 of 100000: train_loss 2.0220, val loss 1.9996\n",
      "step 70800 of 100000: train_loss 2.0448, val loss 1.9821\n",
      "step 70900 of 100000: train_loss 2.0175, val loss 2.0084\n",
      "step 71000 of 100000: train_loss 2.0123, val loss 2.0105\n",
      "step 71100 of 100000: train_loss 2.0284, val loss 2.0193\n",
      "step 71200 of 100000: train_loss 2.0015, val loss 2.0164\n",
      "step 71300 of 100000: train_loss 2.0170, val loss 2.0051\n",
      "step 71400 of 100000: train_loss 2.0076, val loss 2.0148\n",
      "step 71500 of 100000: train_loss 2.0115, val loss 1.9836\n",
      "step 71600 of 100000: train_loss 2.0162, val loss 2.0171\n",
      "step 71700 of 100000: train_loss 2.0196, val loss 1.9980\n",
      "step 71800 of 100000: train_loss 2.0091, val loss 1.9942\n",
      "step 71900 of 100000: train_loss 2.0406, val loss 2.0084\n",
      "step 72000 of 100000: train_loss 2.0059, val loss 2.0233\n",
      "step 72100 of 100000: train_loss 2.0228, val loss 2.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 72200 of 100000: train_loss 2.0262, val loss 2.0001\n",
      "step 72300 of 100000: train_loss 2.0273, val loss 1.9892\n",
      "step 72400 of 100000: train_loss 2.0100, val loss 1.9978\n",
      "step 72500 of 100000: train_loss 2.0322, val loss 2.0037\n",
      "step 72600 of 100000: train_loss 2.0289, val loss 1.9866\n",
      "step 72700 of 100000: train_loss 2.0085, val loss 2.0032\n",
      "step 72800 of 100000: train_loss 2.0114, val loss 1.9888\n",
      "step 72900 of 100000: train_loss 2.0216, val loss 2.0131\n",
      "step 73000 of 100000: train_loss 2.0364, val loss 2.0242\n",
      "step 73100 of 100000: train_loss 2.0032, val loss 1.9918\n",
      "step 73200 of 100000: train_loss 2.0399, val loss 2.0162\n",
      "step 73300 of 100000: train_loss 2.0117, val loss 2.0193\n",
      "step 73400 of 100000: train_loss 2.0199, val loss 2.0168\n",
      "step 73500 of 100000: train_loss 2.0210, val loss 2.0011\n",
      "step 73600 of 100000: train_loss 2.0019, val loss 2.0090\n",
      "step 73700 of 100000: train_loss 2.0031, val loss 2.0020\n",
      "step 73800 of 100000: train_loss 2.0356, val loss 1.9897\n",
      "step 73900 of 100000: train_loss 2.0122, val loss 2.0276\n",
      "step 74000 of 100000: train_loss 1.9994, val loss 2.0078\n",
      "step 74100 of 100000: train_loss 2.0136, val loss 2.0118\n",
      "step 74200 of 100000: train_loss 2.0164, val loss 2.0152\n",
      "step 74300 of 100000: train_loss 2.0137, val loss 2.0058\n",
      "step 74400 of 100000: train_loss 2.0070, val loss 2.0196\n",
      "step 74500 of 100000: train_loss 2.0189, val loss 2.0110\n",
      "step 74600 of 100000: train_loss 2.0293, val loss 2.0061\n",
      "step 74700 of 100000: train_loss 1.9959, val loss 1.9945\n",
      "step 74800 of 100000: train_loss 2.0269, val loss 2.0215\n",
      "step 74900 of 100000: train_loss 2.0188, val loss 2.0316\n",
      "step 75000 of 100000: train_loss 2.0035, val loss 1.9696\n",
      "step 75100 of 100000: train_loss 2.0196, val loss 1.9942\n",
      "step 75200 of 100000: train_loss 2.0236, val loss 1.9782\n",
      "step 75300 of 100000: train_loss 2.0385, val loss 1.9978\n",
      "step 75400 of 100000: train_loss 2.0148, val loss 2.0068\n",
      "step 75500 of 100000: train_loss 2.0163, val loss 2.0170\n",
      "step 75600 of 100000: train_loss 2.0345, val loss 2.0127\n",
      "step 75700 of 100000: train_loss 2.0144, val loss 2.0049\n",
      "step 75800 of 100000: train_loss 2.0180, val loss 2.0004\n",
      "step 75900 of 100000: train_loss 2.0070, val loss 2.0212\n",
      "step 76000 of 100000: train_loss 2.0199, val loss 2.0213\n",
      "step 76100 of 100000: train_loss 2.0025, val loss 1.9922\n",
      "step 76200 of 100000: train_loss 2.0084, val loss 2.0106\n",
      "step 76300 of 100000: train_loss 2.0180, val loss 2.0209\n",
      "step 76400 of 100000: train_loss 2.0160, val loss 2.0076\n",
      "step 76500 of 100000: train_loss 2.0320, val loss 2.0256\n",
      "step 76600 of 100000: train_loss 2.0248, val loss 2.0183\n",
      "step 76700 of 100000: train_loss 2.0051, val loss 1.9997\n",
      "step 76800 of 100000: train_loss 2.0047, val loss 1.9869\n",
      "step 76900 of 100000: train_loss 2.0152, val loss 2.0276\n",
      "step 77000 of 100000: train_loss 1.9988, val loss 2.0087\n",
      "step 77100 of 100000: train_loss 2.0022, val loss 1.9953\n",
      "step 77200 of 100000: train_loss 1.9892, val loss 1.9903\n",
      "step 77300 of 100000: train_loss 2.0011, val loss 1.9928\n",
      "step 77400 of 100000: train_loss 1.9926, val loss 2.0178\n",
      "step 77500 of 100000: train_loss 2.0164, val loss 2.0097\n",
      "step 77600 of 100000: train_loss 2.0154, val loss 2.0183\n",
      "step 77700 of 100000: train_loss 1.9876, val loss 2.0012\n",
      "step 77800 of 100000: train_loss 2.0201, val loss 2.0277\n",
      "step 77900 of 100000: train_loss 2.0064, val loss 1.9932\n",
      "step 78000 of 100000: train_loss 2.0342, val loss 2.0050\n",
      "step 78100 of 100000: train_loss 2.0082, val loss 2.0188\n",
      "step 78200 of 100000: train_loss 2.0272, val loss 2.0121\n",
      "step 78300 of 100000: train_loss 2.0251, val loss 2.0162\n",
      "step 78400 of 100000: train_loss 2.0291, val loss 2.0147\n",
      "step 78500 of 100000: train_loss 2.0223, val loss 2.0190\n",
      "step 78600 of 100000: train_loss 2.0339, val loss 1.9958\n",
      "step 78700 of 100000: train_loss 2.0172, val loss 1.9975\n",
      "step 78800 of 100000: train_loss 2.0110, val loss 1.9875\n",
      "step 78900 of 100000: train_loss 2.0184, val loss 2.0022\n",
      "step 79000 of 100000: train_loss 2.0252, val loss 1.9991\n",
      "step 79100 of 100000: train_loss 2.0138, val loss 1.9980\n",
      "step 79200 of 100000: train_loss 2.0229, val loss 2.0213\n",
      "step 79300 of 100000: train_loss 2.0357, val loss 2.0086\n",
      "step 79400 of 100000: train_loss 2.0224, val loss 1.9919\n",
      "step 79500 of 100000: train_loss 2.0066, val loss 2.0025\n",
      "step 79600 of 100000: train_loss 2.0311, val loss 2.0037\n",
      "step 79700 of 100000: train_loss 2.0074, val loss 2.0145\n",
      "step 79800 of 100000: train_loss 2.0127, val loss 2.0101\n",
      "step 79900 of 100000: train_loss 2.0175, val loss 2.0101\n",
      "step 80000 of 100000: train_loss 2.0335, val loss 2.0103\n",
      "step 80100 of 100000: train_loss 2.0177, val loss 1.9992\n",
      "step 80200 of 100000: train_loss 2.0104, val loss 1.9969\n",
      "step 80300 of 100000: train_loss 2.0147, val loss 2.0191\n",
      "step 80400 of 100000: train_loss 2.0059, val loss 1.9843\n",
      "step 80500 of 100000: train_loss 2.0490, val loss 1.9845\n",
      "step 80600 of 100000: train_loss 2.0118, val loss 2.0115\n",
      "step 80700 of 100000: train_loss 2.0242, val loss 2.0206\n",
      "step 80800 of 100000: train_loss 2.0088, val loss 2.0123\n",
      "step 80900 of 100000: train_loss 2.0109, val loss 2.0043\n",
      "step 81000 of 100000: train_loss 2.0055, val loss 2.0008\n",
      "step 81100 of 100000: train_loss 2.0190, val loss 1.9867\n",
      "step 81200 of 100000: train_loss 2.0053, val loss 1.9957\n",
      "step 81300 of 100000: train_loss 2.0264, val loss 1.9929\n",
      "step 81400 of 100000: train_loss 2.0043, val loss 1.9906\n",
      "step 81500 of 100000: train_loss 2.0401, val loss 2.0043\n",
      "step 81600 of 100000: train_loss 2.0198, val loss 1.9976\n",
      "step 81700 of 100000: train_loss 2.0001, val loss 2.0232\n",
      "step 81800 of 100000: train_loss 2.0302, val loss 2.0127\n",
      "step 81900 of 100000: train_loss 2.0285, val loss 2.0348\n",
      "step 82000 of 100000: train_loss 2.0321, val loss 1.9905\n",
      "step 82100 of 100000: train_loss 2.0137, val loss 2.0051\n",
      "step 82200 of 100000: train_loss 2.0044, val loss 2.0313\n",
      "step 82300 of 100000: train_loss 2.0211, val loss 2.0194\n",
      "step 82400 of 100000: train_loss 2.0278, val loss 2.0047\n",
      "step 82500 of 100000: train_loss 2.0239, val loss 2.0161\n",
      "step 82600 of 100000: train_loss 2.0358, val loss 2.0112\n",
      "step 82700 of 100000: train_loss 2.0193, val loss 1.9968\n",
      "step 82800 of 100000: train_loss 2.0130, val loss 2.0046\n",
      "step 82900 of 100000: train_loss 2.0202, val loss 2.0141\n",
      "step 83000 of 100000: train_loss 2.0214, val loss 2.0108\n",
      "step 83100 of 100000: train_loss 2.0260, val loss 2.0196\n",
      "step 83200 of 100000: train_loss 2.0055, val loss 2.0107\n",
      "step 83300 of 100000: train_loss 2.0232, val loss 2.0119\n",
      "step 83400 of 100000: train_loss 2.0264, val loss 2.0154\n",
      "step 83500 of 100000: train_loss 2.0194, val loss 1.9924\n",
      "step 83600 of 100000: train_loss 2.0201, val loss 2.0200\n",
      "step 83700 of 100000: train_loss 2.0196, val loss 2.0164\n",
      "step 83800 of 100000: train_loss 2.0418, val loss 1.9917\n",
      "step 83900 of 100000: train_loss 2.0145, val loss 2.0093\n",
      "step 84000 of 100000: train_loss 2.0126, val loss 2.0015\n",
      "step 84100 of 100000: train_loss 2.0146, val loss 2.0049\n",
      "step 84200 of 100000: train_loss 2.0216, val loss 2.0036\n",
      "step 84300 of 100000: train_loss 2.0397, val loss 2.0100\n",
      "step 84400 of 100000: train_loss 2.0038, val loss 1.9847\n",
      "step 84500 of 100000: train_loss 2.0203, val loss 2.0165\n",
      "step 84600 of 100000: train_loss 2.0193, val loss 2.0060\n",
      "step 84700 of 100000: train_loss 2.0213, val loss 2.0045\n",
      "step 84800 of 100000: train_loss 1.9946, val loss 2.0006\n",
      "step 84900 of 100000: train_loss 2.0438, val loss 2.0079\n",
      "step 85000 of 100000: train_loss 2.0279, val loss 1.9983\n",
      "step 85100 of 100000: train_loss 2.0132, val loss 2.0127\n",
      "step 85200 of 100000: train_loss 2.0457, val loss 2.0180\n",
      "step 85300 of 100000: train_loss 2.0193, val loss 2.0125\n",
      "step 85400 of 100000: train_loss 2.0065, val loss 2.0044\n",
      "step 85500 of 100000: train_loss 2.0313, val loss 2.0054\n",
      "step 85600 of 100000: train_loss 2.0157, val loss 2.0176\n",
      "step 85700 of 100000: train_loss 2.0191, val loss 2.0138\n",
      "step 85800 of 100000: train_loss 2.0152, val loss 2.0100\n",
      "step 85900 of 100000: train_loss 2.0302, val loss 1.9893\n",
      "step 86000 of 100000: train_loss 2.0164, val loss 1.9972\n",
      "step 86100 of 100000: train_loss 2.0206, val loss 2.0087\n",
      "step 86200 of 100000: train_loss 2.0018, val loss 2.0056\n",
      "step 86300 of 100000: train_loss 2.0036, val loss 2.0328\n",
      "step 86400 of 100000: train_loss 2.0081, val loss 2.0014\n",
      "step 86500 of 100000: train_loss 2.0193, val loss 1.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 86600 of 100000: train_loss 2.0287, val loss 1.9921\n",
      "step 86700 of 100000: train_loss 2.0180, val loss 1.9931\n",
      "step 86800 of 100000: train_loss 2.0078, val loss 2.0104\n",
      "step 86900 of 100000: train_loss 2.0165, val loss 1.9942\n",
      "step 87000 of 100000: train_loss 2.0046, val loss 2.0112\n",
      "step 87100 of 100000: train_loss 2.0109, val loss 1.9840\n",
      "step 87200 of 100000: train_loss 2.0237, val loss 1.9984\n",
      "step 87300 of 100000: train_loss 2.0186, val loss 2.0226\n",
      "step 87400 of 100000: train_loss 2.0151, val loss 2.0060\n",
      "step 87500 of 100000: train_loss 1.9923, val loss 1.9847\n",
      "step 87600 of 100000: train_loss 2.0231, val loss 2.0148\n",
      "step 87700 of 100000: train_loss 2.0079, val loss 2.0058\n",
      "step 87800 of 100000: train_loss 2.0042, val loss 2.0111\n",
      "step 87900 of 100000: train_loss 2.0031, val loss 2.0156\n",
      "step 88000 of 100000: train_loss 2.0293, val loss 2.0136\n",
      "step 88100 of 100000: train_loss 2.0203, val loss 2.0024\n",
      "step 88200 of 100000: train_loss 1.9946, val loss 2.0141\n",
      "step 88300 of 100000: train_loss 2.0210, val loss 1.9996\n",
      "step 88400 of 100000: train_loss 2.0192, val loss 1.9875\n",
      "step 88500 of 100000: train_loss 2.0185, val loss 2.0091\n",
      "step 88600 of 100000: train_loss 2.0269, val loss 2.0047\n",
      "step 88700 of 100000: train_loss 2.0163, val loss 1.9886\n",
      "step 88800 of 100000: train_loss 2.0217, val loss 2.0096\n",
      "step 88900 of 100000: train_loss 2.0224, val loss 1.9879\n",
      "step 89000 of 100000: train_loss 2.0339, val loss 2.0218\n",
      "step 89100 of 100000: train_loss 2.0023, val loss 2.0059\n",
      "step 89200 of 100000: train_loss 2.0140, val loss 2.0036\n",
      "step 89300 of 100000: train_loss 2.0033, val loss 1.9983\n",
      "step 89400 of 100000: train_loss 2.0215, val loss 1.9970\n",
      "step 89500 of 100000: train_loss 2.0114, val loss 1.9871\n",
      "step 89600 of 100000: train_loss 2.0087, val loss 2.0264\n",
      "step 89700 of 100000: train_loss 2.0249, val loss 2.0235\n",
      "step 89800 of 100000: train_loss 2.0154, val loss 1.9970\n",
      "step 89900 of 100000: train_loss 2.0151, val loss 2.0023\n",
      "step 90000 of 100000: train_loss 2.0343, val loss 2.0142\n",
      "step 90100 of 100000: train_loss 2.0189, val loss 2.0022\n",
      "step 90200 of 100000: train_loss 2.0113, val loss 2.0297\n",
      "step 90300 of 100000: train_loss 2.0274, val loss 2.0068\n",
      "step 90400 of 100000: train_loss 2.0227, val loss 2.0145\n",
      "step 90500 of 100000: train_loss 1.9894, val loss 1.9959\n",
      "step 90600 of 100000: train_loss 2.0155, val loss 2.0064\n",
      "step 90700 of 100000: train_loss 2.0036, val loss 2.0012\n",
      "step 90800 of 100000: train_loss 2.0160, val loss 2.0255\n",
      "step 90900 of 100000: train_loss 2.0241, val loss 1.9901\n",
      "step 91000 of 100000: train_loss 2.0249, val loss 2.0030\n",
      "step 91100 of 100000: train_loss 2.0187, val loss 2.0013\n",
      "step 91200 of 100000: train_loss 2.0186, val loss 2.0159\n",
      "step 91300 of 100000: train_loss 2.0303, val loss 1.9869\n",
      "step 91400 of 100000: train_loss 2.0047, val loss 2.0128\n",
      "step 91500 of 100000: train_loss 2.0055, val loss 1.9982\n",
      "step 91600 of 100000: train_loss 2.0152, val loss 2.0075\n",
      "step 91700 of 100000: train_loss 2.0224, val loss 1.9858\n",
      "step 91800 of 100000: train_loss 2.0334, val loss 2.0099\n",
      "step 91900 of 100000: train_loss 2.0292, val loss 2.0069\n",
      "step 92000 of 100000: train_loss 1.9850, val loss 2.0066\n",
      "step 92100 of 100000: train_loss 2.0290, val loss 2.0070\n",
      "step 92200 of 100000: train_loss 2.0132, val loss 1.9984\n",
      "step 92300 of 100000: train_loss 2.0131, val loss 2.0118\n",
      "step 92400 of 100000: train_loss 2.0183, val loss 2.0070\n",
      "step 92500 of 100000: train_loss 2.0090, val loss 1.9836\n",
      "step 92600 of 100000: train_loss 2.0254, val loss 2.0021\n",
      "step 92700 of 100000: train_loss 2.0227, val loss 2.0121\n",
      "step 92800 of 100000: train_loss 2.0371, val loss 2.0006\n",
      "step 92900 of 100000: train_loss 1.9998, val loss 2.0005\n",
      "step 93000 of 100000: train_loss 2.0094, val loss 1.9857\n",
      "step 93100 of 100000: train_loss 2.0314, val loss 1.9895\n",
      "step 93200 of 100000: train_loss 2.0250, val loss 1.9957\n",
      "step 93300 of 100000: train_loss 2.0230, val loss 2.0102\n",
      "step 93400 of 100000: train_loss 2.0280, val loss 2.0170\n",
      "step 93500 of 100000: train_loss 2.0182, val loss 1.9873\n",
      "step 93600 of 100000: train_loss 2.0044, val loss 2.0100\n",
      "step 93700 of 100000: train_loss 2.0120, val loss 2.0007\n",
      "step 93800 of 100000: train_loss 2.0185, val loss 2.0209\n",
      "step 93900 of 100000: train_loss 2.0011, val loss 2.0036\n",
      "step 94000 of 100000: train_loss 2.0012, val loss 2.0179\n",
      "step 94100 of 100000: train_loss 2.0296, val loss 2.0001\n",
      "step 94200 of 100000: train_loss 2.0394, val loss 2.0060\n",
      "step 94300 of 100000: train_loss 2.0417, val loss 1.9882\n",
      "step 94400 of 100000: train_loss 2.0167, val loss 1.9992\n",
      "step 94500 of 100000: train_loss 2.0172, val loss 2.0048\n",
      "step 94600 of 100000: train_loss 2.0239, val loss 2.0088\n",
      "step 94700 of 100000: train_loss 2.0354, val loss 1.9950\n",
      "step 94800 of 100000: train_loss 2.0096, val loss 1.9986\n",
      "step 94900 of 100000: train_loss 2.0230, val loss 1.9994\n",
      "step 95000 of 100000: train_loss 2.0114, val loss 2.0046\n",
      "step 95100 of 100000: train_loss 2.0320, val loss 1.9977\n",
      "step 95200 of 100000: train_loss 2.0249, val loss 2.0009\n",
      "step 95300 of 100000: train_loss 2.0171, val loss 2.0127\n",
      "step 95400 of 100000: train_loss 2.0318, val loss 2.0105\n",
      "step 95500 of 100000: train_loss 2.0084, val loss 1.9893\n",
      "step 95600 of 100000: train_loss 2.0219, val loss 2.0063\n",
      "step 95700 of 100000: train_loss 2.0323, val loss 2.0008\n",
      "step 95800 of 100000: train_loss 2.0280, val loss 2.0012\n",
      "step 95900 of 100000: train_loss 2.0385, val loss 2.0156\n",
      "step 96000 of 100000: train_loss 2.0106, val loss 2.0037\n",
      "step 96100 of 100000: train_loss 2.0129, val loss 2.0115\n",
      "step 96200 of 100000: train_loss 2.0203, val loss 1.9993\n",
      "step 96300 of 100000: train_loss 2.0046, val loss 2.0130\n",
      "step 96400 of 100000: train_loss 2.0295, val loss 2.0130\n",
      "step 96500 of 100000: train_loss 2.0147, val loss 2.0082\n",
      "step 96600 of 100000: train_loss 2.0363, val loss 1.9899\n",
      "step 96700 of 100000: train_loss 2.0354, val loss 1.9961\n",
      "step 96800 of 100000: train_loss 2.0276, val loss 1.9897\n",
      "step 96900 of 100000: train_loss 2.0142, val loss 2.0293\n",
      "step 97000 of 100000: train_loss 2.0166, val loss 2.0144\n",
      "step 97100 of 100000: train_loss 2.0255, val loss 1.9954\n",
      "step 97200 of 100000: train_loss 2.0296, val loss 2.0077\n",
      "step 97300 of 100000: train_loss 2.0090, val loss 2.0033\n",
      "step 97400 of 100000: train_loss 2.0112, val loss 2.0120\n",
      "step 97500 of 100000: train_loss 2.0352, val loss 2.0119\n",
      "step 97600 of 100000: train_loss 2.0061, val loss 2.0144\n",
      "step 97700 of 100000: train_loss 2.0119, val loss 1.9949\n",
      "step 97800 of 100000: train_loss 2.0123, val loss 2.0322\n",
      "step 97900 of 100000: train_loss 2.0008, val loss 2.0110\n",
      "step 98000 of 100000: train_loss 2.0179, val loss 1.9980\n",
      "step 98100 of 100000: train_loss 2.0281, val loss 2.0104\n",
      "step 98200 of 100000: train_loss 2.0339, val loss 2.0112\n",
      "step 98300 of 100000: train_loss 2.0124, val loss 1.9933\n",
      "step 98400 of 100000: train_loss 2.0133, val loss 1.9997\n",
      "step 98500 of 100000: train_loss 2.0329, val loss 1.9863\n",
      "step 98600 of 100000: train_loss 2.0183, val loss 2.0070\n",
      "step 98700 of 100000: train_loss 2.0106, val loss 2.0138\n",
      "step 98800 of 100000: train_loss 2.0363, val loss 1.9946\n",
      "step 98900 of 100000: train_loss 2.0184, val loss 2.0133\n",
      "step 99000 of 100000: train_loss 2.0202, val loss 2.0209\n",
      "step 99100 of 100000: train_loss 2.0195, val loss 1.9951\n",
      "step 99200 of 100000: train_loss 2.0356, val loss 2.0238\n",
      "step 99300 of 100000: train_loss 2.0216, val loss 2.0012\n",
      "step 99400 of 100000: train_loss 2.0210, val loss 2.0081\n",
      "step 99500 of 100000: train_loss 2.0175, val loss 1.9931\n",
      "step 99600 of 100000: train_loss 2.0211, val loss 1.9903\n",
      "step 99700 of 100000: train_loss 2.0150, val loss 2.0160\n",
      "step 99800 of 100000: train_loss 1.9963, val loss 2.0211\n",
      "step 99900 of 100000: train_loss 2.0222, val loss 2.0136\n"
     ]
    }
   ],
   "source": [
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "all_var_losses = {}\n",
    "for iter_ in range(0, max_iters):\n",
    "    # train and update the model\n",
    "    model.train()\n",
    "\n",
    "    xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "    xb_mod = torch.clone(xb.detach())\n",
    "    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "        for key in loss_dict.keys():\n",
    "            if key not in all_var_losses.keys():\n",
    "                all_var_losses[key] = []\n",
    "            all_var_losses[key].append(loss_dict[key])\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = {}\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                               batch_size=batch_size)\n",
    "                xb_mod = torch.clone(xb.detach())\n",
    "                X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                losses[k] = loss.item()\n",
    "            eval_loss[split] = losses.mean()\n",
    "        model.train()\n",
    "        print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ccc249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [0.25 0.25 0.25] est ATE: [0.2242581  0.20168029 0.23853552] error: [0.0257419  0.04831971 0.01146448]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f726c023970>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "inf = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "int_nodes_vals1 = {'X':np.array([1.0,])}\n",
    "effect_var = 'Y'\n",
    "effect_index = var_names.index(effect_var)\n",
    "\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "ATE_pred = (preds1[:,effect_index,:] - preds0[:,effect_index,:]).mean(0)\n",
    "eATE = np.abs(ATE_pred - ATE)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "\n",
    "preds = model(train_data.to(device))\n",
    "\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9fdf883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEYCAYAAADPkTRJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQTElEQVR4nO3df6zdd13H8eer3eZgTAcUDLZzVlNBJENZHRDBDXHYjZjGhMgAXRjGuoSREKNhf4nGLIFAVIiD2pBKZmAlhImFFBZi5IeZW9o6LHSkULuwXroIZWbKcG5d3/5xTsnpd9/vveeu59zvvec8H8k3vd/z/dzP+ezu9tX35/v5fr8nVYUk6Wzr+h6AJK1GhqMktTAcJamF4ShJLQxHSWphOEpSC8NR0pqXZHeS7yb5esfxJPlgkqNJDiV52VJ9Go6SZsFHgW2LHL8W2DLcdgAfXqpDw1HSmldVXwYeXqTJduD2GrgHuCTJCxbr87xJDrApSZm+5+aXr7ii7yGsefcdPNj3ENa000BVZZJ9btu2rU6ePDl2+4MHDx4GHht5aVdV7VrGW24Ejo/sLwxfe6jrG6YajuuAC6f5BnPgwIEDfQ9hzbsoE/17PXceW7rJsp08+T0OHLh37PbJ+Y9V1dZzeMu2X4JF752eajhKUrdTK/lmC8ClI/ubgBOLfYOzXkk9KAbhOO52zvYCNwxXrV8BPFJVnVNqsHKU1Isz4TgZSe4ArgY2JFkA3g2cD1BVO4F9wHXAUeCHwI1L9Wk4SurBZMOxqt60xPEC3r6cPg1HST2YbDhOg+EoqQeGoyR1MBwlqaGAJ/sexKIMR0k9cFotSS0MR0nqYDhKUoOVoyS1MBwlqYXhKEktiuk8DG1yDEdJPbBylKQWhqMktTAcJamF4ShJHQxHSWqwcpSkFoajJLXwkWWS1MLKUZI6GI6S1GDlKEktDEdJamE4SlILw1GSOhiOktRg5ShJLQxHSWpxGp8ELkmtrBwlqcFptSS1WP3huG6cRkkuTfJAkucM95893L9susOTNJvOhOO428obKxyr6jjwYeA9w5feA+yqqm9Pa2CSZt2Ty9hW3nKm1X8FHEzyTuBVwDumMiJJc2D1T6vHDseqeiLJnwCfB15XVY+3tUuyA9gBkIkMUdLsWf3hONa0esS1wEPAS7oaVNWuqtpaVVsNR0ntVv85x7ErxyS/BFwDvAL4lyR7quqhaQ1M0iybkcoxSRgsyLyzqh4E3ge8f5oDkzTLVn/lOO60+g+AB6vqC8P9DwEvSnLVdIYlafbNQDgOzyO+cWT/yaq6oqq+NL2hSZpdk60ck2xLciTJ0SS3tBz/iSSfSfLvSQ4nuXGpPr1DRlIPJnfOMcl64DYGayILwP4ke6vq/pFmbwfur6rfSvI84EiSj3VddQOGo6ReTHRB5krgaFUdA0iyB9gOjIZjARcP10+eBTy81AAMR0k9WHY4bkhyYGR/V1XtGn69ETg+cmwBeHnj+/8G2AucAC4G3lhVpxd7Q8NRUk+WFY4nq2prx7G2S6qrsf+bwFeBXwd+DvhCkq9U1X93veFyLwKXpAk487DbcbdFLQCXjuxvYlAhjroRuLMGjgIPAC9arFPDUVIPJrpavR/YkmRzkguA6xlMoUc9CLwWIMlPAi8Eji3WqdNqST2Y3IJMVZ1KcjNwF7Ae2F1Vh5PcNDy+E/gL4KNJvsZgGv6uqjq5WL+Go6SeTO5RZFW1D9jXeG3nyNcngNctp0/DUVIPVv+91YajpB4YjpLUwnCUpA6GoyQ1WDlKUgvDUZJaGI6S1K76+cjVcRmOkvqx6DNx+mc4Slp5xSRvkJkKw1HSyjMcJamD02pJarBylKQOVo6S1FBA5+f+rQ6Go6SVV1g5SlIrzzlKUoMLMpLUwWm1JDVYOUpSBytHSWqwcpSkFoajJHVwWi1JDVaOktTBcNS5uCjpewhr3qNVfQ9hTdu6devkO/X2QUnqYOUoSQ1WjpLUwcpRkhpcrZakDk6rJanBJ4FLUgsXZCSpg+ccJanBylGSOlg5SlKDl/JIUodVPq1e1/cAJM2hM5XjuNsSkmxLciTJ0SS3dLS5OslXkxxO8qWl+rRylLTyJjitTrIeuA24BlgA9ifZW1X3j7S5BPgQsK2qHkzy/KX6tXKU1I/Ty9gWdyVwtKqOVdXjwB5ge6PNm4E7q+pBgKr67lKdGo6SVt7yp9UbkhwY2XaM9LYROD6yvzB8bdTPA89O8sUkB5PcsNQQnVZL6sfyFmROVlXXU3fbngjdfMLxecAVwGuBZwD/muSeqvpm1xsajpJW3mQv5VkALh3Z3wScaGlzsqoeBR5N8mXgpUBnODqtltSPya1W7we2JNmc5ALgemBvo80/Aq9Ocl6SZwIvB76xWKdWjpJW3gRvH6yqU0luBu4C1gO7q+pwkpuGx3dW1TeSfB44NHznj1TV1xfr13CU1I8J3iFTVfuAfY3Xdjb23we8b9w+DUdJK88HT0hSCx92K0kdrBwlqcGn8khSC8NRkjo4rZakBitHSepg5ShJDVaOktTBcJSkBu+QkaQOVo6S1OA5R0nq4LRakhrWQOW45JPAk1SSvx/ZPy/J95J8drpDkzTTJvfpg1MxTuX4KPCSJM+oqv9l8Nmw35nusCTNtFmoHIc+B7x++PWbgDumMxxJc2NynyEzFeOG4x7g+iQXApcD905vSJJm3pnrHNf4tJqqOpTkZxhUjfsWazv8sO0d0P5hspI0a08C3wu8H7gaeG5Xo6raBewCWJ80P1hbkgZm6FKe3cAjVfW1JFdPZziS5sIaWJAZOxyragH4wBTHImlezMK91VX1rJbXvgh8cQrjkTQvZqVylKSJmaVptSRN1FqfVkvSxFk5SlIHw1GSGmZhtVqSpsLKUZIarBwlqYOVoyQ1uFotSR2cVktSg5WjJLUwHCWpRQFP9D2IxRmOkvrhOUdJanBaLUkdVnk4jvvpg5I0ORP+9MEk25IcSXI0yS2LtPuVJE8mecNSfVo5SurHhCrHJOuB24BrgAVgf5K9VXV/S7v3AneN06+Vo6SVd+ac47jb4q4EjlbVsap6HNgDbG9p9w7gU8B3xxmi4SipH8ubVm9IcmBk2zHS00bg+Mj+wvC1H0myEfhtYOe4w3NaLWnlLX+1+mRVbe04lo53GPXXwLuq6smkrflTGY6S+jG56xwXgEtH9jcBJxpttgJ7hsG4Abguyamq+nRXp4ajpJU32esc9wNbkmwGvgNcD7z5rLer2nzm6yQfBT67WDCC4SipLxMKx6o6leRmBqvQ64HdVXU4yU3D42OfZxxlOEpaeRN+EnhV7QP2NV5rDcWqeus4fRqOkvqxyu+QMRwlrTzvrZakDj6VR5KeapUXjoajpJW3BmbVhqOklbcGHgRuOErqxyo/5Wg4Slp5TqslqYXhKEkdnFZLPbtozEdUqd1jU+jTylGSOhiOktQw4edOTIXhKKkXVo6S1GDlKEkdrBwlqcHVaknq4LRakhqsHCWpheEoSR2cVktSg5WjJHWwcpSkhgIe73sQSzAcJa0475CRpA6ec5SkBhdkJKmD02pJarBylKQOVo6S1GDlKEkdDEdJavA6R0nqYOUoSQ2ec5SkDk6rJanBylGSWrggI0kdVnvluK7vAUiaP2em1eNuS0myLcmRJEeT3NJy/C1JDg23u5O8dKk+rRwlrbgCnphQX0nWA7cB1wALwP4ke6vq/pFmDwBXVdV/JbkW2AW8fLF+DUdJvZjgtPpK4GhVHQNIsgfYDvwoHKvq7pH29wCblurUabWkFXdmQWbcDdiQ5MDItmOku43A8ZH9heFrXX4f+NxSY7RylNSLZVaOJ6tqa8extLxWrQ2T1zAIx1ct9YaGo6QVN+FLeRaAS0f2NwEnmo2SXA58BLi2qr6/VKdOqyX1YoKr1fuBLUk2J7kAuB7YO9ogyU8DdwK/V1XfHGd8Vo6SVtwk75CpqlNJbgbuAtYDu6vqcJKbhsd3An8KPBf4UBKAU4tM0wFIVevU/KkNBz1+Bbi1qj43fO13gLdV1ba271mf1IVj9S5ptXoMeLKq7bze07YxqT9cRvt3w8GlwmzSxq4cq6qGSfzJJP/MIKFvBVqDUZK6zNy91VX19SSfAd4FXATcXlX/MZWRSZpZMxeOQ38O/BvwOPCUMnd4/dEOaF9flySYwQdPVNWjST4B/KCq/q/l+C4Gt+awPhnvhKakuTKrlSOcdeG6JC3fag8QL+WRtOJmuXKUpHMyk+FYVX824XFImiM+CVySOsxk5ShJ58JzjpLUYpJPAp8Ww1FSLzznKEkNTqslqYPhKEkNXsojSR2sHCWpwXOOktTCabUkdbBylKQGK0dJ6mDlKEkNLshIUgen1ZLUYOUoSR0MR0lqcLVakjpYOUpSgw+7laQWLshIUgfPOUpSg5WjJLUwHCWpg9NqSWqwcpSkDlaOktRg5ShJHQxHSWrw3mpJ6rDaK8d1fQ9A0vw5c85x3G0pSbYlOZLkaJJbWo4nyQeHxw8ledlSfRqOknpxehnbYpKsB24DrgVeDLwpyYsbza4Ftgy3HcCHlxqf4ShpxU24crwSOFpVx6rqcWAPsL3RZjtwew3cA1yS5AWLdWo4SurFpCpHYCNwfGR/YfjactucZaoLMqfh5A/h29N8j3O0ATjZ9yDWOH+G5261/wwvm3SHp+GuRwf/3eO6MMmBkf1dVbVr+HVa2ldjf5w2Z5lqOFbV86bZ/7lKcqCqtvY9jrXMn+G5m8efYVVtm2B3C8ClI/ubgBNPo81ZnFZLWuv2A1uSbE5yAXA9sLfRZi9ww3DV+hXAI1X10GKdep2jpDWtqk4luRm4C1gP7K6qw0luGh7fCewDrgOOAj8Eblyq31QtOu2eaUl2jJy30NPgz/Dc+TNcneY6HCWpi+ccpR4l2dT3GNTOcJR6kuT5wN8l2ZDEv4urzFz+D0nymiSv7Hsca1mSZ/c9hhlwPvDjwHlVtdofUjN35jIcgauAGwD8F3v5krwO+MLwTz1NVfUd4G7g1eDv4mozr5fy3A28FcB/sZ+WFwIvAf44yTOr6tM9j2fNSPJrDO7zLWA3g8pxCwx+F5OkXCVdFeYmHJO8FvgF4D7gW8BlSX6qqk6MtFlnWI7lDuBngQcZXFh7flV9sucxrRX/yeAf52sYzF6uAjYn+aequreqyoBcHeYmHIELgJcx+KW8iEFQvi3JoaraO/yFNBg7JLkcoKoOAQ8DjwO/yODRTzcnOV1Vn+pxiGtCVR0BjgCfAkjyceD1wPYk66vqboNxdZjL6xyTbAZuBS5mEJQPAy8A/hK401/OsyV5LvA9Bven/hGDh4ncB3yAwW1ZlwBvBvZU1R09DXPNOFMZjvy5BXgL8BzgY1V1b89DFHO2IHPmhHdVPcDgfsyTVfXrwDuAvwXuMxifqqq+D/wGg5v1Lwe2AbczuA3reVX1CeAfGFQ/F/c20DXizO/YyJ/fYvAMwoeAYz0OTSPmsnIESHIZcGtV/W7fY1krhudtdzM4PfEGBtXiAoP7VH8MoKr+p7cBrnHDc7dP9D0ODcxzOF7C4MT4jU5jxpfkOuC9wCur6gdJNg8rcWmmzNOCTNMjwMc5++nAWkJV7UsCsD/Jr54JRldYNWvmtnIESHJeVZ3qexxrUZLtwLuBrQxOn83vL5Jm0lyHo85NkmdV1Q/6Hoc0DYajJLWYq0t5JGlchqMktTAcJamF4ShJLQxHSWphOEpSi/8H3kltgDWaQC4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view attention maps\n",
    "maps = []\n",
    "for j in range(n_layers):\n",
    "    heads = model.blocks[j].mha.heads\n",
    "    for i in range(num_heads):\n",
    "        maps.append(heads[i].att_wei.mean(0).cpu().detach().numpy())\n",
    "\n",
    "maps = np.stack(maps).mean(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(maps, cmap='hot', interpolation='nearest')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=1)\n",
    "# Setting the axis tick labels\n",
    "ax.set_xticks(np.arange(len(list(DAGnx.nodes))))\n",
    "ax.set_yticks(np.arange(len(list(DAGnx.nodes))))\n",
    "\n",
    "ax.set_xticklabels(list(DAGnx.nodes))\n",
    "ax.set_yticklabels(list(DAGnx.nodes))\n",
    "\n",
    "# Rotating the tick labels inorder to set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('attention_maps.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc7a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
