{
 "cells": [
  {
   "cell_type": "code",
   "id": "1f6504d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:48:43.105643Z",
     "start_time": "2024-08-21T14:48:41.631380Z"
    }
   },
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import CaT\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_full_ordering, reorder_dag\n",
    "import utils\n",
    "\n",
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 1000000\n",
    "batch_size = 100\n",
    "max_iters = 100000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.3\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 5e-3\n",
    "ff_n_embed = 6\n",
    "num_heads = 2\n",
    "n_layers = 1\n",
    "head_size = 6\n",
    "d = 1\n",
    "\n",
    "def generate_data(N, d=3):\n",
    "    DAGnx = nx.DiGraph()\n",
    "    \n",
    "    Ux = np.random.randn(N,d)\n",
    "    X =  Ux\n",
    "    \n",
    "    Ub = np.random.randn(N,d)\n",
    "    B =  Ub\n",
    "    \n",
    "    Uc = np.random.randn(N,d)\n",
    "    C =  Uc\n",
    "    \n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y = 0.3 * X + 0.6 * B + 1.2 * C +  Uy\n",
    "\n",
    "    Y0 = 0.3 * 0 + 0.6 * B + 1.2 * C +  Uy\n",
    "    Y1 =  0.3 * 1 + 0.6 * B + 1.2 * C + Uy\n",
    "\n",
    "    all_data_dict = {'X': X, 'B': B, 'C': C, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont', 'B': 'cont', 'C': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'Y'), ('B', 'Y'), ('C', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "    ordered_var_types = dict(sorted(var_types.items(), key=lambda item: causal_ordering[item[0]]))\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, ordered_var_types, Y0, Y1\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "51aaf293",
   "metadata": {},
   "source": [
    "## Exogenous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae1124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "id": "c8056409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:52:50.484401Z",
     "start_time": "2024-08-21T14:52:50.481073Z"
    }
   },
   "source": [
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1982159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'effect_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 81\u001B[0m\n\u001B[1;32m     78\u001B[0m D0 \u001B[38;5;241m=\u001B[39m ci\u001B[38;5;241m.\u001B[39mforward(data\u001B[38;5;241m=\u001B[39mall_data , intervention_nodes_vals\u001B[38;5;241m=\u001B[39mintervention_nodes_vals_0)\n\u001B[1;32m     79\u001B[0m D1 \u001B[38;5;241m=\u001B[39m ci\u001B[38;5;241m.\u001B[39mforward(data\u001B[38;5;241m=\u001B[39mall_data , intervention_nodes_vals\u001B[38;5;241m=\u001B[39mintervention_nodes_vals_1)\n\u001B[0;32m---> 81\u001B[0m est_ATE \u001B[38;5;241m=\u001B[39m (D1[:,\u001B[43meffect_index\u001B[49m] \u001B[38;5;241m-\u001B[39m D0[:,effect_index])\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mATE:\u001B[39m\u001B[38;5;124m'\u001B[39m, ATE, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mest ATE:\u001B[39m\u001B[38;5;124m'\u001B[39m, ATE_pred, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror:\u001B[39m\u001B[38;5;124m'\u001B[39m, eATE)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'effect_index' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "_, _, _, _, _, Y0, Y1 = generate_data(N=1000000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  # multi-dim ATE based off a large sample\n",
    "\n",
    "print(ATE)\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data(N=sample_size, d=d)\n",
    "    indices = np.arange(0, len(all_data))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "    train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "    train_data = all_data[train_inds]\n",
    "    val_data = all_data[val_inds]\n",
    "    train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "    input_dim = all_data.shape[2]\n",
    "    \n",
    "    model = CaT(input_dim=input_dim,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    head_size=head_size,\n",
    "                    num_heads=num_heads,\n",
    "                    ff_n_embed=ff_n_embed,\n",
    "                    dag=DAGnx,\n",
    "                    causal_ordering=causal_ordering,\n",
    "                    n_layers=n_layers,\n",
    "                    device=device,\n",
    "                    var_types=var_types,\n",
    "                    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    all_var_losses = {}\n",
    "    for iter_ in range(0, max_iters):\n",
    "        # train and update the model\n",
    "        model.train()\n",
    "    \n",
    "        xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "        xb_mod = torch.clone(xb.detach())\n",
    "        X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "    \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "        if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "            for key in loss_dict.keys():\n",
    "                if key not in all_var_losses.keys():\n",
    "                    all_var_losses[key] = []\n",
    "                all_var_losses[key].append(loss_dict[key])\n",
    "    \n",
    "            model.eval()\n",
    "            eval_loss = {}\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "    \n",
    "                    xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                                   batch_size=batch_size)\n",
    "                    xb_mod = torch.clone(xb.detach())\n",
    "                    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                    losses[k] = loss.item()\n",
    "                eval_loss[split] = losses.mean()\n",
    "            model.train()\n",
    "            # print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    intervention_nodes_vals_0 = {'X': 0}\n",
    "    intervention_nodes_vals_1 = {'X': 1}\n",
    "    ci = inference.CausalInference(model=model, device=device)\n",
    "    D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "    D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "    \n",
    "    effect_var = 'Y'\n",
    "    effect_index = utils.find_element_in_list(var_names, target_string=effect_var)\n",
    "    \n",
    "    est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean()\n",
    "    print('ATE:', ATE, 'est ATE:', est_ATE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120aa247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "intervention_nodes_vals_0 = {'X': 0}\n",
    "intervention_nodes_vals_1 = {'X': 1}\n",
    "ci = inference.CausalInference(model=model, device=device)\n",
    "D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "\n",
    "est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean(0)\n",
    "print('ATE:', ATE, 'est ATE:', ATE_pred, 'error:', eATE)\n",
    "preds = model(train_data.to(device))\n",
    "\n",
    "plt.scatter(train_data[:,effect_index,-1].detach().cpu().numpy(), preds[:, effect_index, -1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(D0[:,3, 0], alpha=0.5)\n",
    "plt.hist(D1[:,3, 0], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view attention maps\n",
    "maps = []\n",
    "for j in range(n_layers):\n",
    "    heads = model.blocks[j].mha.heads\n",
    "    for i in range(num_heads):\n",
    "        maps.append(heads[i].Sprime.mean(0).cpu().detach().numpy())\n",
    "\n",
    "maps = np.stack(maps).mean(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(maps, cmap='hot', interpolation='nearest')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=1)\n",
    "# Setting the axis tick labels\n",
    "ax.set_xticks(np.arange(len(list(DAGnx.nodes))))\n",
    "ax.set_yticks(np.arange(len(list(DAGnx.nodes))))\n",
    "\n",
    "ax.set_xticklabels(list(DAGnx.nodes))\n",
    "ax.set_yticklabels(list(DAGnx.nodes))\n",
    "\n",
    "# Rotating the tick labels inorder to set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('attention_maps.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data(N=sample_size, d=d)\n",
    "indices = np.arange(0, len(all_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "train_data = all_data[train_inds]\n",
    "val_data = all_data[val_inds]\n",
    "train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "input_dim = all_data.shape[2]\n",
    "\n",
    "model = CaT(input_dim=input_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_n_embed=ff_n_embed,\n",
    "                dag=DAGnx,\n",
    "                causal_ordering=causal_ordering,\n",
    "                n_layers=n_layers,\n",
    "                device=device,\n",
    "                var_types=var_types,\n",
    "                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf84be6-04d4-42e1-a245-dc87522ef6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=3 and 4 variables\n",
    "x = torch.tensor(all_data[0:1] * 0., dtype=torch.float).to(device)\n",
    "x[0][0] = 1  # X1\n",
    "x[0][1] = 4  # X2\n",
    "x[0][2] = 2  # X3\n",
    "x[0][3] = 1  # X4\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f8f2c-5a73-420b-9716-62eff5255c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = model.blocks[0].mha.heads[0].key(x)\n",
    "Q = model.blocks[0].mha.heads[0].query(x)\n",
    "V = model.blocks[0].mha.heads[0].value(x)\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82965d9b-baf8-49cc-8fd7-4f10cb1c1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QK = torch.matmul(Q, K.transpose(1, 2)) / (head_size ** 0.5)\n",
    "QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e80e6-7ab7-4ce1-95a7-58b8d0a311a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869e8a7-b1a9-487f-a82c-f8199fce0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.tensor(model.blocks[0].mha.heads[0].dag_mod.T, dtype=torch.float, requires_grad=False) @ QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df8ebc-c5e4-4400-a132-8f49a6b65d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = S.masked_fill(S == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aaf405-af3c-4606-99c6-bc102bb24f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.nn.functional.softmax(S, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780f152-772e-43cb-a430-8af79b388fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = torch.any(torch.isnan(S), dim=-1)  # check if any rows are <all> -inf, these need to be masked to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393eb88-b88f-4d73-98ee-d7675d71e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask = nan_rows.unsqueeze(-1).expand_as(S).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9777fd6-eff4-4b9f-87ff-9a0d93225b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.where(nan_mask, torch.zeros_like(S), S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5755ab-2207-4e13-b4cb-70ff0a3cc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D =torch.tensor(model.blocks[0].mha.heads[0].dag_mod, dtype=torch.float, requires_grad=False)\n",
    "D.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb34714-22d1-484d-967a-296bbe67a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add = D.T @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18ce36-8d7c-4159-90c1-c7af540d6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "O1 = S.transpose(1, 2) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73184ef0-de90-4af5-b7a3-d9ef444dd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = O1  + add\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15085c49-72a2-4c93-bdc4-c66434d63f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c87ac6-2c64-4fab-b53f-c839b7dc11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(model.blocks[0].mha.heads[0].dag_mod.T, dtype=torch.float, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b662c1-eff4-4b67-82ef-e3463d16dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd93ee-2e36-4a1e-a476-4a209d311b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af54dfb-c441-4744-b170-ec1db6bd5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "V[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21f975-64f9-432a-830b-d3005129be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "inf = inference.CausalInference(model=model, device=device)\n",
    "\n",
    "int_nodes_vals0 = {'X':np.array([0.0,])}\n",
    "preds0 = inf.forward(all_data, int_nodes_vals0)\n",
    "preds0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960dffa-5800-4b5d-b737-349a08173c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_nodes_vals1 = {'X':np.array([-4.0,])}\n",
    "preds1 = inf.forward(all_data, int_nodes_vals1)\n",
    "preds1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0f23f-1246-4adc-b21d-410958751baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "id": "edc16638-bc8d-4022-bea4-04dbe56b95f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:52:55.010907Z",
     "start_time": "2024-08-21T14:52:54.846604Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T15:19:22.231497Z",
     "start_time": "2024-08-21T15:19:22.229727Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ecef317cd3ac3a08",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:53:33.053017Z",
     "start_time": "2024-08-21T14:53:33.051271Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "93b7d36a815b298e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T15:06:54.199577Z",
     "start_time": "2024-08-21T15:06:54.197862Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "6cd8b92943c4a950",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T15:18:25.171669Z",
     "start_time": "2024-08-21T15:18:25.169839Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c2651d26c9c6a2a5",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e5ae836b4838452"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
