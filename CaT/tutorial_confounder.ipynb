{
 "cells": [
  {
   "cell_type": "code",
   "id": "1f6504d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:26:57.832194Z",
     "start_time": "2024-09-05T10:26:57.823316Z"
    }
   },
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from test_model import CaT\n",
    "import inference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_full_ordering, reorder_dag\n",
    "import utils\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "shuffling = 0\n",
    "seed = 1\n",
    "standardize = 0\n",
    "sample_size = 100000\n",
    "batch_size = 300\n",
    "max_iters = 20000\n",
    "eval_interval = 1000\n",
    "eval_iters = 100\n",
    "validation_fraction = 0.1\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda'\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 2e-4\n",
    "ff_n_embed = 4\n",
    "num_heads = 3\n",
    "n_layers = 1\n",
    "head_size = 4\n",
    "embed_dim = 5\n",
    "d = 1\n",
    "\n",
    "\n",
    "def generate_data(N, d=1):\n",
    "    DAGnx = nx.DiGraph()\n",
    "\n",
    "    \n",
    "    Uc = np.random.randn(N,d)\n",
    "    C =  Uc\n",
    "\n",
    "        \n",
    "    Ux = np.random.randn(N,d)\n",
    "    X =  1 * C + Ux\n",
    "    \n",
    "    Uy = np.random.randn(N,d)\n",
    "    Y = 0.8 * X  + 1.5 * C + Uy\n",
    "\n",
    "    Y0 = 0.8 * 0 + 1.5 * C + Uy\n",
    "    Y1 = 0.8 * 1  + 1.5 * C +  Uy\n",
    "\n",
    "    all_data_dict = {'X': X,  'C': C, 'Y': Y}\n",
    "\n",
    "    # types can be 'cat' (categorical) 'cont' (continuous) or 'bin' (binary)\n",
    "    var_types = {'X': 'cont',  'C': 'cont', 'Y': 'cont'}\n",
    "\n",
    "    DAGnx.add_edges_from([('X', 'Y'), ('C', 'X'), ('C', 'Y')])\n",
    "    DAGnx = reorder_dag(dag=DAGnx)  # topologically sorted dag\n",
    "    var_names = list(DAGnx.nodes())  # topologically ordered list of variables\n",
    "    all_data = np.stack([all_data_dict[key] for key in var_names], axis=1)\n",
    "    causal_ordering = get_full_ordering(DAGnx)\n",
    "    ordered_var_types = dict(sorted(var_types.items(), key=lambda item: causal_ordering[item[0]]))\n",
    "\n",
    "    return all_data, DAGnx, var_names, causal_ordering, ordered_var_types, Y0, Y1\n",
    "\n",
    "\n",
    "def get_batch(train_data, val_data, split, device, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[ix]\n",
    "    return x.to(device)\n",
    "\n",
    "def input_output_sensitivity_matrix(input, model, epsilon=100.0, full=True):\n",
    "    model.eval()\n",
    "    input = input.detach().clone() \n",
    "    \n",
    "    if full:\n",
    "        original_output, _, _ = model(X=input, targets=input, shuffling=False)\n",
    "    else:\n",
    "        original_output = model(X=input)\n",
    "    \n",
    "    sensitivity_matrix = torch.zeros((input.size(1), original_output.size(1)))\n",
    "\n",
    "    for i in range(input.size(1)): \n",
    "        perturbed_input = input.clone()\n",
    "        perturbed_input[0, i] += epsilon  \n",
    "        \n",
    "        if full:\n",
    "            perturbed_output, _, _ = model(X=perturbed_input, targets=perturbed_input, shuffling=False)\n",
    "        else:\n",
    "            perturbed_output = model(X=perturbed_input)\n",
    "            \n",
    "        output_difference = perturbed_output - original_output\n",
    "\n",
    "        sensitivity_matrix[i, :] =  torch.abs(output_difference.view(-1))\n",
    "    \n",
    "    return sensitivity_matrix"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "51aaf293",
   "metadata": {},
   "source": "## Confounding Example"
  },
  {
   "cell_type": "code",
   "id": "7733a0d107a2c1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:26:58.399549Z",
     "start_time": "2024-09-05T10:26:58.388596Z"
    }
   },
   "source": [
    "\n",
    "all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data(N=100000, d=d)\n",
    "ATE = (Y1 - Y0).mean(0)  # multi-dim ATE based off a large sample\n",
    "\n",
    "print('ATE:', ATE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [0.8]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:26:58.617597Z",
     "start_time": "2024-09-05T10:26:58.610526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = all_data[:, :2, 0] \n",
    "y = all_data[:, 2:3, 0]\n",
    "reg = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "print('Intercept:', reg.intercept_)\n",
    "print('Coefficients:', reg.coef_[0])"
   ],
   "id": "61cc3f05789612a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-0.00200594]\n",
      "Coefficients: [1.49799063 0.79865823]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The coefficients from the linear regression are highly biased as a result of the confounder... Now let's try with CaT!",
   "id": "d8b02eddabd0518a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-05T10:26:59.508491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "estimates = []\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    all_data, DAGnx, var_names, causal_ordering, var_types, Y0, Y1 = generate_data(N=sample_size, d=d)\n",
    "    \n",
    "    indices = np.arange(0, len(all_data))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    val_inds = indices[:int(validation_fraction*len(indices))]\n",
    "    train_inds = indices[int(validation_fraction*len(indices)):]\n",
    "    \n",
    "    train_data = all_data[train_inds]\n",
    "    val_data = all_data[val_inds]\n",
    "    \n",
    "    train_data, val_data = torch.from_numpy(train_data).float(),  torch.from_numpy(val_data).float()\n",
    "    input_dim = all_data.shape[2]\n",
    "    \n",
    "    model = CaT(input_dim=input_dim,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    head_size=head_size,\n",
    "                    num_heads=num_heads,\n",
    "                    ff_n_embed=ff_n_embed,\n",
    "                    embed_dim= embed_dim,\n",
    "                    dag=DAGnx,\n",
    "                    causal_ordering=causal_ordering,\n",
    "                    n_layers=n_layers,\n",
    "                    device=device,\n",
    "                    var_types=var_types, activation_function='Swish'\n",
    "                    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_iters:\n",
    "            return float(epoch) / float(max(1, warmup_iters))\n",
    "        return 1.0\n",
    "    \n",
    "    warmup_iters = max_iters//5  # Number of iterations for warmup\n",
    "    scheduler_warmup = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "    scheduler_cyclic = CosineAnnealingLR(optimizer, T_max=max_iters - warmup_iters)\n",
    "\n",
    "    \n",
    "    all_var_losses = {}\n",
    "    inter_model_ests = []\n",
    "    sub_epoch = []\n",
    "    mses = [] \n",
    "    for iter_ in range(0, max_iters):\n",
    "        # train and update the model\n",
    "        model.train()\n",
    "    \n",
    "        xb = get_batch(train_data=train_data, val_data=val_data, split='train', device=device, batch_size=batch_size)\n",
    "        xb_mod = torch.clone(xb.detach())\n",
    "        X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=shuffling)\n",
    "    \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter_ < warmup_iters:\n",
    "            scheduler_warmup.step()\n",
    "        else:\n",
    "            scheduler_cyclic.step()\n",
    "    \n",
    "    \n",
    "        if iter_ % eval_interval == 0:  # evaluate the loss (no gradients)\n",
    "            for key in loss_dict.keys():\n",
    "                if key not in all_var_losses.keys():\n",
    "                    all_var_losses[key] = []\n",
    "                all_var_losses[key].append(loss_dict[key])\n",
    "    \n",
    "            model.eval()\n",
    "            eval_loss = {}\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "    \n",
    "                    xb = get_batch(train_data=train_data, val_data=val_data, split=split, device=device,\n",
    "                                   batch_size=batch_size)\n",
    "                    xb_mod = torch.clone(xb.detach())\n",
    "                    X, loss, loss_dict = model(X=xb, targets=xb_mod, shuffling=False)\n",
    "                    losses[k] = loss.item()\n",
    "                eval_loss[split] = losses.mean()\n",
    "            mses.append(eval_loss['train'])\n",
    "                \n",
    "            intervention_nodes_vals_0 = {'X':  0}\n",
    "            intervention_nodes_vals_1 = {'X':  1}\n",
    "            ci = inference.CausalInference(model=model, device=device)\n",
    "            D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "            D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "            \n",
    "            effect_var = 'Y'\n",
    "            effect_index = utils.find_element_in_list(var_names, target_string=effect_var)\n",
    "            \n",
    "            est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean()\n",
    "            inter_model_ests.append(est_ATE)\n",
    "            sub_epoch.append(iter_)\n",
    "            model.train()\n",
    "            print(f\"step {iter_} of {max_iters}: train_loss {eval_loss['train']:.4f}, val loss {eval_loss['val']:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sub_epoch, inter_model_ests, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Est')\n",
    "    plt.title('Ests')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sub_epoch[2:], mses[2:], marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('MSEs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    sensitivity_matrix = input_output_sensitivity_matrix(xb[0:1], model, full=True).detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(sensitivity_matrix, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    for i in range(sensitivity_matrix.shape[0]):\n",
    "        for j in range(sensitivity_matrix.shape[1]):\n",
    "            plt.text(j, i, f'{sensitivity_matrix[i, j]:.4f}', ha='center', va='center', color='white')\n",
    "    \n",
    "    plt.xlabel('Output Dimensions')\n",
    "    plt.ylabel('Input Dimensions')\n",
    "    plt.title('Sensitivity Matrix Heatmap')\n",
    "    plt.xticks(ticks=np.arange(len(var_names)), labels=var_names)\n",
    "    plt.yticks(ticks=np.arange(len(var_names)), labels=var_names)\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    intervention_nodes_vals_0 = {'X':  0}\n",
    "    intervention_nodes_vals_1 = {'X':  1}\n",
    "    ci = inference.CausalInference(model=model, device=device)\n",
    "    D0 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_0)\n",
    "    D1 = ci.forward(data=all_data , intervention_nodes_vals=intervention_nodes_vals_1)\n",
    "    \n",
    "    \n",
    "    effect_var = 'Y'\n",
    "    effect_index = utils.find_element_in_list(var_names, target_string=effect_var)\n",
    "    \n",
    "    est_ATE = (D1[:,effect_index] - D0[:,effect_index]).mean()\n",
    "    print('ATE:', ATE, 'est ATE:', est_ATE)\n",
    "    estimates.append(est_ATE)\n",
    "    \n",
    "print(estimates)\n"
   ],
   "id": "c1982159",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 20000: train_loss 9.2144, val loss 9.0425\n",
      "step 1000 of 20000: train_loss 6.9265, val loss 6.9731\n",
      "step 2000 of 20000: train_loss 3.5515, val loss 3.5516\n",
      "step 3000 of 20000: train_loss 2.1203, val loss 2.1278\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "120aa247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:16:46.650711Z",
     "start_time": "2024-09-05T10:16:46.648554Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28d1bea2",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3dc2c102",
   "metadata": {},
   "source": "\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
